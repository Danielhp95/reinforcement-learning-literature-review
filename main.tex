\documentclass{article}
\usepackage[a4paper,margin=1in,footskip=0.25in]{geometry} % sets margins
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amssymb} % For maths symbols like real number R.
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[ruled, linesnumbered]{algorithm2e}

\DeclareMathOperator*{\argmax}{argmax}
\usepackage[round]{natbib}

\graphicspath{{images/}{../images/}}

\usepackage{subfiles}

% \usepackage{blindtext}

\title{\textbf{Literature review: Reinforcement Learning}}
\author{Daniel Hernandez}
\date{ }

\begin{document}

\maketitle

%\tableofcontents

% \subfile{sections/abstract}
% 
%\section{Introduction}
%\subfile{sections/introduction}
%
    \subsection{Markov Decision Processes}\label{section:markov-decision-processes}
    \subfile{sections/markov-decision-process}

    %\subsection{Categorization of RL algorithms}
    %\subfile{sections/reinforcement-learning-algorithms}
    
    %\subsection{Value Iteration algorithm}
    %\subfile{value-iteration}
    %
    \subsection{Q-learning}\label{section:q-learning}
    \subfile{sections/q-learning}
 
\section{Policy gradient methods}\label{section:policy-gradient}
\subfile{sections/policy-gradient-methods}
%
% \section{Actor critic methods}
% \subfile{sections/actor-critic-methods}
% 
% \section{Multi-agent reinforcement learning}
% \subfile{sections/multiagent}

\section{Learning Environments}
\subfile{sections/learning-environments}

\section{Appendix}
\subfile{sections/likelihood-ratio-policy-gradient-derivation}

%List of TODOS
%Define Exploration vs exploitation: The first is finding parts of the state space you haven't been and learning about them
%\begin{itemize}
%\item 
%\end{itemize}

% This was introduced by  \cite{Watkins1992}. This algorithm is very good \citep{Watkins1992}.

\bibliographystyle{apalike}
\bibliography{main}

\end{document}


