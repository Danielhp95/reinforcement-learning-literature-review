\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amssymb} % For maths symbols like real number R.
%\usepackage{amsmath}
\usepackage{mathtools}

\usepackage[round]{natbib}

\graphicspath{{images/}{../images/}}

\usepackage{subfiles}

% \usepackage{blindtext}

\title{\textbf{Literature review: Reinforcement Learning}}
\author{Daniel Hernandez}
\date{ }

\begin{document}

\maketitle

%\tableofcontents

% \subfile{sections/abstract}
% 
\section{Introduction}
\subfile{sections/introduction}

    \subsection{Markov Decision Processes}
    \subfile{sections/markov-decision-process}
    
    % \subsection{Value Iteration algorithm}
    % \subfile{value-iteration}
    %
    % \subsection{Q-learning}
    % \subfile{sections/q-learning}
% 
% \section{Policy gradient methods}
% \subfile{sections/policy-gradient-methods}
% 
% \section{Multi-agent reinforcement learning}
% \subfile{sections/multiagent}

%\section{Appendix}
%\subfile{sections/likelihood-ratio-policy-gradient-derivation}

%List of TODOS
%Define Exploration vs exploitation: The first is finding parts of the state space you haven't been and learning about them
%\begin{itemize}
%\item 
%\end{itemize}

% This was introduced by  \cite{Watkins1992}. This algorithm is very good \citep{Watkins1992}.

\bibliographystyle{apalike}
\bibliography{main}

\end{document}


