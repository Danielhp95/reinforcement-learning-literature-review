@article{Schulman2015a,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {1506.02438},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
eprint = {1506.02438},
file = {:home/sarios/papers/gae-paper.pdf:pdf},
pages = {1--14},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {http://arxiv.org/abs/1506.02438},
year = {2015}
}
@article{Schulman2015,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
doi = {10.1063/1.4927398},
eprint = {1502.05477},
file = {:home/sarios/papers/trust-region-policy-optimization.pdf:pdf},
isbn = {0375-9687},
issn = {2158-3226},
title = {{Trust Region Policy Optimization}},
url = {http://arxiv.org/abs/1502.05477},
year = {2015}
}
@article{Samuel1959,
abstract = {A new signature table technique is described together with an improved book learning procedure which is thought to be much superior to the linear polynomial method described earlier. Full use is made of the so called “alpha-beta” pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.},
author = {Samuel, A.L.},
doi = {10.1016/0066-4138(69)90004-4},
file = {:home/sarios/papers/samuel-checkers.pdf:pdf},
isbn = {0066-4138},
issn = {00664138},
journal = {Ibm Journal},
number = {3},
pages = {210},
title = {{Some studies in machine learning using the game of checkers}},
url = {http://pages.cs.wisc.edu/{~}dyer/cs540/handouts/samuel-checkers.pdf},
volume = {3},
year = {1959}
}
@article{Laurent2011,
abstract = {In multi-agent systems, the presence of learning agents can cause the environment to be non-Markovian from an agent's perspective thus violating the property that traditional single-agent learning methods rely upon. This paper formalizes some known intuition about concurrently learning agents by providing formal conditions that make the environment non-Markovian from an independent (non-communicative) learner's perspective. New concepts are introduced like the divergent learning paths and the observability of the effects of others' actions. To illustrate the formal concepts, a case study is also presented. These findings are significant because they both help to understand failures and successes of existing learning algorithms as well as being suggestive for future work},
author = {Laurent, Guillaume J. and Matignon, La{\"{e}}titia and Fort-Piat, N. Le},
doi = {10.3233/KES-2010-0206},
file = {:home/sarios/papers/multi-agent-non-markovian.pdf:pdf},
issn = {13272314},
journal = {International Journal of Knowledge-Based and Intelligent Engineering Systems},
keywords = {Multi-agent system,machine learning,reinforcement learning},
number = {1},
pages = {55--64},
title = {{The world of independent learners is not markovian}},
volume = {15},
year = {2011}
}
@article{Treutwein1995,
author = {Treutwein, Bernhard},
file = {:home/sarios/papers/Treutwein-adaptive-psychophysics-procedures.pdf:pdf},
title = {{Adaptive Psychophysical Procedures}},
year = {1995}
}
@article{Tesauro1992,
abstract = {This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(lambda) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(lambda) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating.},
author = {Tesauro, Gerald},
doi = {10.1007/BF00992697},
file = {:home/sarios/papers/practical-issues-in-temporal-difference-learning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {backgammon,cormectionist methods,feature,games,neural networks,temporal difference learning},
number = {3-4},
pages = {257--277},
title = {{Practical Issues in Temporal Difference Learning}},
url = {papers3://publication/uuid/4B128386-0184-4728-AC88-0F3385D1036F},
volume = {8},
year = {1992}
}
@article{Silver2017b,
abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
archivePrefix = {arXiv},
arxivId = {1712.01815},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
eprint = {1712.01815},
file = {:home/sarios/papers/alphago-zero.pdf:pdf},
pages = {1--19},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
url = {http://arxiv.org/abs/1712.01815},
year = {2017}
}
@article{Gomez2008,
abstract = {Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difficult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artificial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difficult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be significantly more efficient and powerful than the other methods on these tasks.},
author = {Gomez, Faustino and Ch, Tino@idsia and Schmidhuber, Urgen and Ch, Juergen@idsia and Miikkulainen, Risto},
doi = {10.1145/1390681.1390712},
file = {:home/sarios/papers/Evolution/Accelerated Neural Evolution through Cooperatively Coevolved Synapses.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {coevolution,exper-imental comparison,genetic algorithms,non-linear control,recurrent neural networks},
pages = {937--965},
title = {{Accelerated Neural Evolution through Cooperatively Coevolved Synapses}},
volume = {9},
year = {2008}
}
@article{Dosovitskiy2015,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Brox, Thomas},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1512.03385},
file = {:home/sarios/papers/Evolution/An Empirical Exploration of Recurrent Network Architectures.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1538--1546},
pmid = {18267787},
title = {{Learning to generate chairs with convolutional neural networks}},
volume = {07-12-June},
year = {2015}
}
@article{Mattiussi2007,
abstract = {This paper describes a new kind of genetic representation called analog genetic encoding (AGE). The representation is aimed at the evolutionary synthesis and reverse engineering of circuits and networks such as analog electronic circuits, neural networks, and genetic regulatory networks. AGE permits the simultaneous evolution of the topology and sizing of the networks. The establishment of the links between the devices that form the network is based on an implicit definition of the interaction between different parts of the genome. This reduces the amount of information that must be carried by the genome, relatively to a direct encoding of the links. The application of AGE is illustrated with examples of analog electronic circuit and neural network synthesis. The performance of the representation and the quality of the results obtained with AGE are compared with those produced by genetic programming.},
author = {Mattiussi, Claudio and Floreano, Dario},
doi = {10.1109/TEVC.2006.886801},
file = {:home/sarios/papers/Evolution/Analog Genetic Encoding for the Evolution of Circuits and Networks.pdf:pdf},
isbn = {1089-778X},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Analog circuit synthesis,Analog genetic encoding (AGE),Analog network synthesis,Evolutionary computation,Genetic representation,Neural network synthesis},
number = {5},
pages = {596--607},
title = {{Analog genetic encoding for the evolution of circuits and networks}},
volume = {11},
year = {2007}
}
@article{Fogarty1989,
author = {Fogarty, T.C.},
doi = {10.1109/icsmc.1989.71308},
file = {:home/sarios/papers/Evolution/An incremental genetic algorithm for real-time optimisation.pdf:pdf},
issn = {08843627},
journal = {Conference Proceedings, IEEE International Conference on Systems, Man and Cybernetics},
pages = {321--326},
title = {{An Incremental Genetic Algorithm for real-time optimisation}},
year = {1989}
}
@article{Alba2001,
author = {Alba, Enrique and Troya, Jos{\'{e}} M},
doi = {10.1016/S0167-739X(99)00129-6},
file = {:home/sarios/papers/Evolution/Analyzing synchronous and asynchronous parallel distributed genetic algorithms.pdf:pdf},
journal = {Future Generation Comp. Syst.},
keywords = {asynchronous parallel gas,cellular gas,numeric performance,selection pressure,speedup},
number = {4},
pages = {451--465},
title = {{Analyzing synchronous and asynchronous parallel distributed genetic algorithms}},
volume = {17},
year = {2001}
}
@article{Togelius2006,
abstract = {Evolutionary car racing (ECR) is extended to the case of two cars racing on the same track. A sensor representation is devised, and various methods of evolving car controllers for competitive racing are explored. ECR can be combined with co-evolution in a wide variety of ways, and one aspect which is explored here is the relative-absolute fitness continuum. Systematical behavioural differences are found along this continuum; further, a tendency to specialization and the reactive nature of the controller architecture are found to limit evolutionary progress.},
author = {Togelius, Julian and Lucas, Simon M.},
doi = {10.1007/11844297_62},
file = {:home/sarios/papers/Evolution/Arms races and car races.pdf:pdf},
isbn = {3540389903},
issn = {03029743},
keywords = {Artificial Intelligence,Machine Learning,Neural Nets},
pages = {1--10},
title = {{Arms races and car races}},
url = {http://cogprints.org/5220/},
year = {2006}
}
@article{Spector2012,
abstract = {Many potential target problems for genetic programming$\backslash$nare modal in the sense that qualitatively different$\backslash$nmodes of response are required for inputs from$\backslash$ndifferent regions of the problem's domain. This paper$\backslash$npresents a new approach to solving modal problems with$\backslash$ngenetic programming, using a simple and novel parent$\backslash$nselection method called lexicase selection. It then$\backslash$nshows how the differential performance of genetic$\backslash$nprogramming with and without lexicase selection can be$\backslash$nused to provide a measure of problem modality, and it$\backslash$nargues that defining such a measure in this way is not$\backslash$nas methodologically problematic as it may initially$\backslash$nappear. The modality measure is illustrated through the$\backslash$nanalysis of genetic programming runs on a simple modal$\backslash$nsymbolic regression problem. This is a preliminary$\backslash$nreport that is intended in part to stimulate discussion$\backslash$non the significance of modal problems, methods for$\backslash$nsolving them, and methods for measuring the modality of$\backslash$nproblems. Although the core concepts in this paper are$\backslash$npresented in the context of genetic programming, they$\backslash$nare also relevant to applications of other forms of$\backslash$nevolutionary computation to modal problems.},
author = {Spector, Lee},
doi = {10.1145/2330784.2330846},
file = {:home/sarios/papers/Evolution/Assessment of Problem Modality by Differential Performance of Lexicase Selection in Genetic Programming- A Preliminary Report.pdf:pdf},
isbn = {9781450311786},
journal = {Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference companion - GECCO Companion '12},
keywords = {all or part of,genetic programming,lexicase,modal problems,modality,or hard copies of,permission to make digital,problem metrics,selection,this work for},
pages = {401},
title = {{Assessment of problem modality by differential performance of lexicase selection in genetic programming}},
url = {http://dl.acm.org/citation.cfm?doid=2330784.2330846},
year = {2012}
}
@article{Alvernaz2017,
abstract = {Neuroevolution has proven effective at many reinforcement learning tasks, but does not seem to scale well to high-dimensional controller representations, which are needed for tasks where the input is raw pixel data. We propose a novel method where we train an autoencoder to create a comparatively low-dimensional representation of the environment observation, and then use CMA-ES to train neural network controllers acting on this input data. As the behavior of the agent changes the nature of the input data, the autoencoder training progresses throughout evolution. We test this method in the VizDoom environment built on the classic FPS Doom, where it performs well on a health-pack gathering task.},
archivePrefix = {arXiv},
arxivId = {1707.03902},
author = {Alvernaz, Samuel and Togelius, Julian},
doi = {10.1109/CIG.2017.8080408},
eprint = {1707.03902},
file = {:home/sarios/papers/Evolution/Autoencoder-augmented Neuroevolution for Visual Doom Playing.pdf:pdf},
isbn = {9781538632338},
journal = {2017 IEEE Conference on Computational Intelligence and Games, CIG 2017},
pages = {1--8},
title = {{Autoencoder-augmented neuroevolution for visual doom playing}},
year = {2017}
}
@article{Coleman2014,
abstract = {Algorithms for evolving agents that learn during their lifetime have typically been evaluated on only a handful of environments. Designing such environments is labour intensive, potentially biased, and provides only a small sample size that may prevent accurate general conclusions from being drawn. In this paper we introduce a method for automatically generating MDP environments which allows the difficulty to be scaled in several ways. We present a case study in which environments are generated that vary along three key dimensions of difficulty: the number of environment configurations, the number of available actions, and the length of each trial. The study reveals interesting differences between three neural network models -- Fixed-Weight, Plastic-Weight, and Modulated Plasticity -- that would not have been obvious without sweeping across these different dimensions. Our paper thus introduces a new way of conducting reinforcement learning science: instead of manually designing a few environments, researchers will be able to automatically generate a range of environments across key dimensions of variation. This will allow scientists to more rigorously assess the general learning capabilities of an algorithm, and may ultimately improve the rate at which we discover how to create AI with general purpose learning.},
author = {Coleman, Oliver J. and Blair, Alan D. and Clune, Jeff},
doi = {10.1145/2576768.2598257},
file = {:home/sarios/papers/Evolution/Automated Generation of Environments to Test the General Learning Capabilities of AI Agents.pdf:pdf},
isbn = {9781450326629},
journal = {Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14},
keywords = {all or part of,evolutionary robotics,generative encodings,learning,neural net-,neuromodulation,or,or hard copies of,permission to make digital,this work for personal,works},
pages = {161--168},
title = {{Automated generation of environments to test the general learning capabilities of AI agents}},
url = {http://dl.acm.org/citation.cfm?doid=2576768.2598257},
year = {2014}
}
@article{Dufourq2017,
abstract = {Regression or classification? This is perhaps the most basic question faced when tackling a new supervised learning problem. We present an Evolutionary Deep Learning (EDL) algorithm that automatically solves this by identifying the question type with high accuracy, along with a proposed deep architecture. Typically, a significant amount of human insight and preparation is required prior to executing machine learning algorithms. For example, when creating deep neural networks, the number of parameters must be selected in advance and furthermore, a lot of these choices are made based upon pre-existing knowledge of the data such as the use of a categorical cross entropy loss function. Humans are able to study a dataset and decide whether it represents a classification or a regression problem, and consequently make decisions which will be applied to the execution of the neural network. We propose the Automated Problem Identification (API) algorithm, which uses an evolutionary algorithm interface to TensorFlow to manipulate a deep neural network to decide if a dataset represents a classification or a regression problem. We test API on 16 different classification, regression and sentiment analysis datasets with up to 10,000 features and up to 17,000 unique target values. API achieves an average accuracy of {\$}96.3\backslash{\%}{\$} in identifying the problem type without hardcoding any insights about the general characteristics of regression or classification problems. For example, API successfully identifies classification problems even with 1000 target values. Furthermore, the algorithm recommends which loss function to use and also recommends a neural network architecture. Our work is therefore a step towards fully automated machine learning.},
archivePrefix = {arXiv},
arxivId = {1707.00703},
author = {Dufourq, Emmanuel and Bassett, Bruce A.},
doi = {10.1145/3129416.3129429},
eprint = {1707.00703},
file = {:home/sarios/papers/Evolution/Automated Problem Identification- Regression vs Classification via Evolutionary Deep Networks.pdf:pdf},
isbn = {9781450352505},
title = {{Automated Problem Identification: Regression vs Classification via Evolutionary Deep Networks}},
url = {http://arxiv.org/abs/1707.00703},
year = {2017}
}
@article{Chrabaszcz2018,
abstract = {Evolution Strategies (ES) have recently been demonstrated to be a viable alternative to reinforcement learning (RL) algorithms on a set of challenging deep RL problems, including Atari games and MuJoCo humanoid locomotion benchmarks. While the ES algorithms in that work belonged to the specialized class of natural evolution strategies (which resemble approximate gradient RL algorithms, such as REINFORCE), we demonstrate that even a very basic canonical ES algorithm can achieve the same or even better performance. This success of a basic ES algorithm suggests that the state-of-the-art can be advanced further by integrating the many advances made in the field of ES in the last decades. We also demonstrate qualitatively that ES algorithms have very different performance characteristics than traditional RL algorithms: on some games, they learn to exploit the environment and perform much better while on others they can get stuck in suboptimal local minima. Combining their strengths with those of traditional RL algorithms is therefore likely to lead to new advances in the state of the art.},
archivePrefix = {arXiv},
arxivId = {1802.08842},
author = {Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
eprint = {1802.08842},
file = {:home/sarios/papers/Evolution/Back to Basics- Benchmarking Canonical Evolution Strategies for Playing Atari.pdf:pdf},
title = {{Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari}},
url = {http://arxiv.org/abs/1802.08842},
year = {2018}
}
@article{Shalizi2010,
author = {Shalizi, Cosma},
file = {:home/sarios/papers/Evolution/Bayesian Updating, Evolutionary Dynamics and Relative Entropy.pdf:pdf},
number = {September},
title = {{Bayesian Updating , Evolutionary Dynamics and Relative Entropy What ' s the Problem ?}},
year = {2010}
}
@article{Doncieux2013,
abstract = {Recent results in evolutionary robotics show that explicitly encouraging the behavioral diversity of candidate solutions drastically improves the convergence of many experiments. The performance of this technique depends, however, on the choice of a behavioral similarity measure (BSM). Here we propose that the experimenter does not actually need to choose: provided that several similarity measures are conceivable, using them all could lead to better results than choosing a single one. Values computed by several BSM can be averaged, which is computationally expensive because it requires the computation of all the BSM at each generation, or randomly switched at a user-chosen frequency, which is a cheaper alternative. We compare these two approaches in two experimental setups - a ball collecting task and hexapod locomotion - with five different BSMs. Results show that (1) using several BSM in a single run increases the performance while avoiding the need to choose the most appropriate BSM and (2) switching between BSMs leads to better results than taking the mean behavioral diversity, while requiring less computational power.},
author = {Doncieux, Stephane and Mouret, Jean Baptiste},
doi = {10.1109/CEC.2013.6557731},
file = {:home/sarios/papers/Evolution/Behavioral diversity with multiple behavioral distances.pdf:pdf},
isbn = {9781479904549},
issn = {1089-778X},
journal = {2013 IEEE Congress on Evolutionary Computation, CEC 2013},
pages = {1427--1434},
title = {{Behavioral diversity with multiple behavioral distances}},
year = {2013}
}
@article{Schmidt2008,
abstract = {We present an algorithm that coevolves fitness predictors, optimized for the solution population, which reduce fitness evaluation cost and frequency, while maintaining evolutionary progress. Fitness predictors differ from fitness models in that they may or may not represent the objective fitness, opening opportunities to adapt selection pressures and diversify solutions. The use of coevolution addresses three fundamental challenges faced in past fitness approximation research: 1) the model learning investment; 2) the level of approximation of the model; and 3) the loss of accuracy. We discuss applications of this approach and demonstrate its impact on the symbolic regression problem. We show that coevolved predictors scale favorably with problem complexity on a series of randomly generated test problems. Finally, we present additional empirical results that demonstrate that fitness prediction can also reduce solution bloat and find solutions more reliably.},
author = {Schmidt, Michael D. and Lipson, Hod},
doi = {10.1109/TEVC.2008.919006},
file = {:home/sarios/papers/Evolution/Coevolution of Fitness Predictors.pdf:pdf},
isbn = {1089-778X},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Bloat Reduction,Coevolution,Fitness modeling,Symbolic regression},
number = {6},
pages = {736--749},
title = {{Coevolution of fitness predictors}},
volume = {12},
year = {2008}
}
@article{Adhikari2018,
abstract = {We investigate competitive co-evolution of unit micromanagement in real-time strategy games. Although good long-term macro-strategy and good short-term unit micromanagement both impact real-time strategy games performance, this paper focuses on generating quality micro. Better micro, for example, can help players win skirmishes and battles even when outnumbered. Prior work has shown that we can evolve micro to beat a given opponent. We remove the need for a good opponent to evolve against by using competitive co-evolution to evolve high-quality micro for both sides from scratch. We first co-evolve micro to control a group of ranged units versus a group of melee units. We then move to co-evolve micro for a group of ranged and melee units versus a group of ranged and melee units. Results show that competitive co-evolution produces good quality micro and when combined with the well-known techniques of fitness sharing, shared sampling, and a hall of fame takes less time to produce better quality micro than simple co-evolution. We believe these results indicate the viability of co-evolutionary approaches for generating good unit micro-management.},
archivePrefix = {arXiv},
arxivId = {1803.10314},
author = {Adhikari, Navin K and Louis, Sushil J. and Liu, Siming and Spurgeon, Walker},
eprint = {1803.10314},
file = {:home/sarios/papers/Evolution/Co-evolving Real-Time Strategy Game Micro.pdf:pdf},
title = {{Co-evolving Real-Time Strategy Game Micro}},
url = {http://arxiv.org/abs/1803.10314},
year = {2018}
}
@article{Soltoggio2017,
abstract = {Biological plastic neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifetime learning. The interplay of these elements leads to the emergence of adaptive behavior and intelligence. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed plastic neural networks with a large variety of dynamics, architectures, and plasticity rules: these artificial systems are composed of inputs, outputs, and plastic components that change in response to experiences in an environment. These systems may autonomously discover novel adaptive algorithms, and lead to hypotheses on the emergence of biological adaptation. EPANNs have seen considerable progress over the last two decades. Current scientific and technological advances in artificial neural networks are now setting the conditions for radically new approaches and results. In particular, the limitations of hand-designed networks could be overcome by more flexible and innovative solutions. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main methods and results are reviewed. Finally, new opportunities and developments are presented.},
archivePrefix = {arXiv},
arxivId = {1703.10371},
author = {Soltoggio, Andrea and Stanley, Kenneth O. and Risi, Sebastian},
doi = {10.1016/j.neunet.2018.07.013},
eprint = {1703.10371},
file = {:home/sarios/papers/Evolution/Born to Learn- the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks.pdf:pdf},
pages = {1--24},
title = {{Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks}},
url = {http://arxiv.org/abs/1703.10371{\%}0Ahttp://dx.doi.org/10.1016/j.neunet.2018.07.013},
year = {2017}
}
@article{Spirals1992,
author = {Spirals, Co-evolving Intertwined},
file = {:home/sarios/papers/Evolution/Co-evolving Intertwined Spirals.pdf:pdf},
title = {{Hugues Juill Jordan B . Pollack 2 Massively Parallel GP 3 The Spiral Problem and the Competi- tive Evolution Paradigm}},
year = {1992}
}
@article{Hillis1990,
abstract = {This paper shows an example of how simulated evolution can be applied to a practical optimization problem, and more specifically, how the addition of co-evolving parasites can improve the procedure by preventing the system from sticking at local maxima. Firstly an optimization procedure based on simulated evolution and its implementation on a parallel computer are described. Then an application of this system to the problem of generating minimal sorting networks is described. Finally it is shown how the introduction of a species of co-evolving parasites improves the efficiency and effectiveness of the procedure. {\textcopyright} 1990.},
author = {Hillis, W. Daniel},
doi = {10.1016/0167-2789(90)90076-2},
file = {:home/sarios/papers/Evolution/Co-evolving parasites improve simulated evolution as an optimization procedure.pdf:pdf},
isbn = {0262560577},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {1-3},
pages = {228--234},
pmid = {21338505},
title = {{Co-evolving parasites improve simulated evolution as an optimization procedure}},
volume = {42},
year = {1990}
}
@article{Watson2001,
abstract = {One of the central difficulties of coevolutionary methods arises from 'intransitive superiority' - in a two-player game, for example, the fact that A beats B, and B beats C, does not exclude the possibility that C beats A. Such cyclic superiority in a coevolutionary substrate is hypothesized to cause cycles in the dynamics of the population such that it 'chases its own tail' - traveling through some part of strategy space more than once despite apparent improvement with each step. It is often difficult to know whether an application domain contains such difficulties and to verify this hypothesis in the failure of a given coevolutionary set-up. In this paper we wish to elucidate some of the issues and concepts in an abstract domain where the dynamics of coevolution can be studied simply and directly. We define three simple 'number games' that illustrate intransitive superiority and resultant oscillatory dynamics, as well as some other relevant concepts. These include the distinction between a player's perceived performance and performance with respect to an external metric, and the significance of strategies with a multi-dimensional nature. These features alone can also cause oscillatory behavior and coevolutionary failure.},
author = {Watson, RA and Pollack, JB},
doi = {10.1.1.35.88},
file = {:home/sarios/papers/Evolution/Coevolutionary Dynamics in a Minimal Substrate.pdf:pdf},
journal = {Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2001)},
keywords = {coevolution,coevolutionary failure,dimensions,domains a,for many machine learning,from the fact that,intransitive superiority,multiple,performance is simply not,suitable objective metric of},
number = {1992},
pages = {702 -- 709},
title = {{Coevolutionary Dynamics in a Minimal Substrate}},
url = {http://eprints.ecs.soton.ac.uk/12011},
year = {2001}
}
@article{Hornby1999,
abstract = {We compare two types of coevolutionary tournaments,$\backslash$ntrue and diffuse, in contests using a general-purpose,$\backslash$nphysics-based simulator. Previous work in coevolving$\backslash$nagents has used true coevolution and found that$\backslash$npopulations tend to enter mediocre states. One$\backslash$nhypothesis for alleviating these problems is to use$\backslash$ndiffuse coevolution. Our results show that agents$\backslash$nevaluated with diffuse tournaments are more generalized$\backslash$nthan those evaluated with true tournaments.},
author = {Hornby, Gregory S and Mirtich, Brian},
doi = {10.1.1.45.9059},
file = {:home/sarios/papers/Evolution/Comparing Diffuse and True Coevolution in a Physics-Based World.pdf:pdf},
isbn = {1-55860-611-4},
journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
keywords = { adaptive behavior and agents, co-evolution, neural networks, pursuer-evader,artificial life},
pages = {1305--1312},
title = {{Diffuse versus True Coevolution in a Physics-based World}},
url = {http://www.cs.bham.ac.uk/{~}wbl/biblio/gecco1999/AA-025.ps},
volume = {2},
year = {1999}
}
@article{Fernando2016,
abstract = {Data represented as strings abounds in biology, linguistics, document mining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as "Lexis", that produces an optimized hierarchical representation of a given set of "target" strings. The resulting hierarchy, "Lexis-DAG", shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-Hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the "core" of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.05561v1},
author = {Fernando, Chrisantha and Banarse, Dylan and Reynolds, Malcolm and Besse, Frederic and Pfau, David and Jaderberg, Max and Lanctot, Marc and Wierstra, Daan},
doi = {10.1145/2908812.2908890},
eprint = {arXiv:1602.05561v1},
file = {:home/sarios/papers/Evolution/Convolution by Evolution.pdf:pdf},
isbn = {9781450342063},
journal = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference - GECCO '16},
keywords = {compositional pattern producing networks,cppns,de-,mnist,noising autoencoder},
pages = {109--116},
title = {{Convolution by Evolution}},
url = {http://dl.acm.org/citation.cfm?doid=2908812.2908890},
year = {2016}
}
@article{Miikkulainen2011,
abstract = {Two major goals in machine learning are the discovery and improvement of solutions to complex problems. In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals. We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures. NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head. Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification. When compared to the evolution of networks with fixed structure, complexifying evolution discovers significantly more sophisticated strategies. The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize.},
archivePrefix = {arXiv},
arxivId = {1107.0037},
author = {Miikkulainen, R. and Stanley, K. O.},
doi = {10.1613/jair.1338},
eprint = {1107.0037},
file = {:home/sarios/papers/Evolution/Competitive Coevolution through Evolutionary Complexification.pdf:pdf},
journal = {Journal Of Artificial Intelligence Research},
keywords = {Copyright {\textcopyright} 2004 AI Access, Changes and compilatio},
pages = {63--100},
title = {{Competitive Coevolution through Evolutionary Complexification}},
url = {http://arxiv.org/abs/1107.0037{\%}0Ahttp://dx.doi.org/10.1613/jair.1338},
volume = {21},
year = {2011}
}
@article{Gravina2017,
author = {Gravina, Daniele and Liapis, Antonios and Yannakakis, Georgios N.},
doi = {10.1145/3071178.3071179},
file = {:home/sarios/papers/Evolution/Coupling Novelty and Surprise for Evolutionary Divergence.pdf:pdf},
isbn = {9781450349208},
journal = {Proceedings of the Genetic and Evolutionary Computation Conference on - GECCO '17},
keywords = {acm reference format,based evolution,deception,divergent search,map navigation,neat,novelty search,surprise search,tness-},
pages = {107--114},
title = {{Coupling novelty and surprise for evolutionary divergence}},
url = {http://dl.acm.org/citation.cfm?doid=3071178.3071179},
year = {2017}
}
@article{Stanton2016,
abstract = {Natural animals are renowned for their ability to acquire a diverse and general skill set over the course of their lifetime. However, research in artificial intelligence has yet to produce agents that acquire all or even most of the available skills in non-trivial environments. One candidate algorithm for encouraging the production of such individuals is Novelty Search, which pressures organisms to exhibit different behaviors from other individuals. However, we hypothesized that Novelty Search would produce sub-populations of specialists, in which each individual possesses a subset of skills, but no one organism acquires all or most of the skills. In this paper, we propose a new algorithm called Curiosity Search, which is designed to produce individuals that acquire as many skills as possible during their life-time. We show that in a multiple-skill maze environment, Curiosity Search does produce individuals that explore their entire domain, while a traditional implementation of Novelty Search produces specialists. However, we reveal that when modified to encourage intra-life},
author = {Stanton, Christopher and Clune, Jeff},
doi = {10.1371/journal.pone.0162235},
file = {:home/sarios/papers/Evolution/Curiosity Search- Producing Generalists by Encouraging Individuals to Continually Explore and Acquire Skills throughout Their Lifetime.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {1--20},
pmid = {27589267},
title = {{Curiosity search: Producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime}},
volume = {11},
year = {2016}
}
@article{Such2017,
abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.$\backslash$ DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in {\$}{\{}\backslashraise.17ex\backslashhbox{\{}{\$}$\backslash$scriptstyle$\backslash$sim{\$}{\}}{\}}{\$}4 hours on one desktop or {\$}{\{}\backslashraise.17ex\backslashhbox{\{}{\$}$\backslash$scriptstyle$\backslash$sim{\$}{\}}{\}}{\$}1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.},
archivePrefix = {arXiv},
arxivId = {1712.06567},
author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
doi = {1712.06567},
eprint = {1712.06567},
file = {:home/sarios/papers/Evolution/Deep Neuroevolution- Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning.pdf:pdf},
title = {{Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}},
url = {http://arxiv.org/abs/1712.06567},
year = {2017}
}
@article{Gaier2018,
abstract = {Surrogate-assistance approaches have long been used in computationally expensive domains to improve the data-efficiency of optimization algorithms. Neuroevolution, however, has so far resisted the application of these techniques because it requires the surrogate model to make fitness predictions based on variable topologies, instead of a vector of parameters. Our main insight is that we can sidestep this problem by using kernel-based surrogate models, which require only the definition of a distance measure between individuals. Our second insight is that the well-established Neuroevolution of Augmenting Topologies (NEAT) algorithm provides a computationally efficient distance measure between dissimilar networks in the form of "compatibility distance", initially designed to maintain topological diversity. Combining these two ideas, we introduce a surrogate-assisted neuroevolution algorithm that combines NEAT and a surrogate model built using a compatibility distance kernel. We demonstrate the data-efficiency of this new algorithm on the low dimensional cart-pole swing-up problem, as well as the higher dimensional half-cheetah running task. In both tasks the surrogate-assisted variant achieves the same or better results with several times fewer function evaluations as the original NEAT.},
archivePrefix = {arXiv},
arxivId = {1804.05364},
author = {Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
doi = {10.1145/3205455.3205510},
eprint = {1804.05364},
file = {:home/sarios/papers/Evolution/Data-efficient Neuroevolution with Kernel-Based Surrogate Models.pdf:pdf},
isbn = {9781450356183},
keywords = {2018,acm reference format,adam gaier,alexander asteroth,and jean-baptiste mouret,data-,neat,neuroevolution,surrogate modeling},
title = {{Data-efficient Neuroevolution with Kernel-Based Surrogate Models}},
url = {http://arxiv.org/abs/1804.05364{\%}0Ahttp://dx.doi.org/10.1145/3205455.3205510},
year = {2018}
}
@article{Poulsen2017,
abstract = {This paper investigates the potential of combining deep learning and neuroevolution to create a bot for a simple first person shooter (FPS) game capable of aiming and shooting based on high-dimensional raw pixel input. The deep learning component is responsible for visual recognition and translating raw pixels to compact feature representations, while the evolving network takes those features as inputs to infer actions. Two types of feature representations are evaluated in terms of (1) how precise they allow the deep network to recognize the position of the enemy, (2) their effect on evolution, and (3) how well they allow the deep network and evolved network to interface with each other. Overall, the results suggest that combining deep learning and neuroevolution in a hybrid approach is a promising research direction that could make complex visual domains directly accessible to networks trained through evolution.},
author = {Poulsen, Andreas Precht and Thorhauge, Mark and Funch, Mikkel Hvilshj and Risi, Sebastian},
doi = {10.1109/CIG.2017.8080444},
file = {:home/sarios/papers/Evolution/DLNE- A Hybridization of Deep Learning and Neuroevolution for Visual Control.pdf:pdf},
isbn = {9781538632338},
journal = {2017 IEEE Conference on Computational Intelligence and Games, CIG 2017},
pages = {256--263},
title = {{DLNE: A hybridization of deep learning and neuroevolution for visual control}},
year = {2017}
}
@article{Juill1992,
author = {Juill, Hugues and Pollack, Jordan B},
file = {:home/sarios/papers/Evolution/Dynamics of co-evolutionary learning.pdf:pdf},
title = {{Dynamics of Co-evolutionary Learning}},
year = {1992}
}
@article{Lehman2013,
abstract = {Diversity maintenance techniques in evolutionary computation are designed to mitigate the problem of deceptive local optima by encouraging exploration. However, as problems become more difficult, the heuristic of fitness may become increasingly uninformative. Thus, simply encouraging genotypic diversity may fail to much increase the likelihood of evolving a solution. In such cases, diversity needs to be directed towards potentially useful structures. A representative example of such a search process is novelty search, which builds diversity by rewarding behavioral novelty. In this paper the effectiveness of fitness, novelty, and diversity maintenance objectives are compared in two evolutionary robotics domains. In a biped locomotion domain, genotypic diversity maintenance helps evolve biped control policies that travel farther before falling. However, the best method is to optimize a fitness objective and a behavioral novelty objective together. In the more deceptive maze navigation domain, diversity maintenance is ineffective while a novelty objective still increases performance. The conclusion is that while genotypic diversity maintenance works in well-posed domains, a method more directed by phenotypic information, like novelty search, is necessary for highly deceptive ones.},
author = {Lehman, Joel and Stanley, Kenneth O. and Miikkulainen, Risto},
doi = {10.1145/2463372.2463393},
file = {:home/sarios/papers/Evolution/Effective Diversity Maintenance in Deceptive Domains.pdf:pdf},
isbn = {9781450319638},
journal = {Proceeding of the fifteenth annual conference on Genetic and evolutionary computation conference - GECCO '13},
keywords = {deception,diversity maintenance,neat,novelty search},
number = {Gecco},
pages = {215},
title = {{Effective diversity maintenance in deceptive domains}},
url = {http://dl.acm.org/citation.cfm?doid=2463372.2463393},
year = {2013}
}
@article{Ficici2000,
abstract = {A strong assumption made in evolutionary game theory (EGT) [7] is that the evolving population is infinitely large. Recent simulations by Fogel, et al, [3, 4, 5] show that finite populations produce behavior that, at best, deviate with statistical significance from the evolutionary stable strategy (ESS) predicted by EGT. They conclude that evolutionary game theory loses its predictive power with finite populations. In this paper, we revisit the question of how finite populations affect EGT dynamics. By paying particular attention to the operation of the selection mechanisms used by Fogel, et al, we are able to account for the divergence between ESS predictions (based on infinite populations) and results observed in our own finite-population simulations. We then show that Baker's SUS [1] selection method corrects the divergence to a great extent. We thus conclude that the dynamics of EGT, and particularly ESSs, can indeed apply to finite-population systems. 1 Intro...},
author = {Ficici, Sevan G and Pollack, Jordan B},
doi = {10.1.1.34.9136},
file = {:home/sarios/papers/Evolution/Effects of Finite Populations on Evolutionary Stable Strategies.pdf:pdf},
journal = {Proceedings of the 2000 Genetic and Evolutionary Computation Conference, Las Vegas},
pages = {927--934},
title = {{Effects of Finite Populations on Evolutionary Stable Strategies}},
volume = {879},
year = {2000}
}
@article{Shahrzad2018,
abstract = {An important benefit of multi-objective search is that it maintains a diverse population of candidates, which helps in deceptive problems in particular. Not all diversity is useful, however: candidates that optimize only one objective while ignoring others are rarely helpful. This paper proposes a solution: The original objectives are replaced by their linear combinations, thus focusing the search on the most useful tradeoffs between objectives. To compensate for the loss of diversity, this transformation is accompanied by a selection mechanism that favors novelty. In the highly deceptive problem of discovering minimal sorting networks, this approach finds better solutions, and finds them faster and more consistently than standard methods. It is therefore a promising approach to solving deceptive problems through multi-objective optimization.},
archivePrefix = {arXiv},
arxivId = {1803.03744},
author = {Shahrzad, Hormoz and Fink, Daniel and Miikkulainen, Risto},
eprint = {1803.03744},
file = {:home/sarios/papers/Evolution/Enhanced Optimization with Composite Objectives and Novelty Selection.pdf:pdf},
keywords = {composite objective functions,diversity,multi-objective optimization,novelty search,novelty selection},
title = {{Enhanced Optimization with Composite Objectives and Novelty Selection}},
url = {http://arxiv.org/abs/1803.03744},
year = {2018}
}
@article{Lehman2017,
abstract = {An evolution strategy (ES) variant based on a simplification of a natural evolution strategy recently attracted attention because it performs surprisingly well in challenging deep reinforcement learning domains. It searches for neural network parameters by generating perturbations to the current set of parameters, checking their performance, and moving in the aggregate direction of higher reward. Because it resembles a traditional finite-difference approximation of the reward gradient, it can naturally be confused with one. However, this ES optimizes for a different gradient than just reward: It optimizes for the average reward of the entire population, thereby seeking parameters that are robust to perturbation. This difference can channel ES into distinct areas of the search space relative to gradient descent, and also consequently to networks with distinct properties. This unique robustness-seeking property, and its consequences for optimization, are demonstrated in several domains. They include humanoid locomotion, where networks from policy gradient-based reinforcement learning are significantly less robust to parameter perturbation than ES-based policies solving the same task. While the implications of such robustness and robustness-seeking remain open to further study, this work's main contribution is to highlight such differences and their potential importance.},
archivePrefix = {arXiv},
arxivId = {1712.06568},
author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
doi = {10.1145/3205455},
eprint = {1712.06568},
file = {:home/sarios/papers/Evolution/ES Is More Than Just a Traditional Finite-Difference Approximator.pdf:pdf},
isbn = {9781450356183},
title = {{ES Is More Than Just a Traditional Finite-Difference Approximator}},
url = {http://arxiv.org/abs/1712.06568},
year = {2017}
}
@article{Andrychowicz2017,
abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {1707.01495},
file = {:home/sarios/papers/hindsight-experience-replay.pdf:pdf},
issn = {10495258},
number = {Nips},
title = {{Hindsight Experience Replay}},
url = {http://arxiv.org/abs/1707.01495},
year = {2017}
}
@article{Jaderberg,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.01281v1},
author = {Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
doi = {arXiv:1807.01281},
eprint = {arXiv:1807.01281v1},
file = {:home/sarios/papers/for-the-win.pdf:pdf},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}}
}
@article{Schaul2015,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
archivePrefix = {arXiv},
arxivId = {1511.05952},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
doi = {10.1038/nature14236},
eprint = {1511.05952},
file = {:home/sarios/papers/prioritized-experience-replay.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {0028-0836},
pages = {1--21},
pmid = {25719670},
title = {{Prioritized Experience Replay}},
url = {http://arxiv.org/abs/1511.05952},
year = {2015}
}
@article{Sutton2010,
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other arti cial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a sin- gle predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the sys- tem's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, re- ward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever ac- tions are taken by the system as a whole. Gradient-based temporal-di erence learning methods are used to learn ef- ciently and reliably with function approximation in this o -policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real- time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from o - policy experience. Horde is a signi cant incremental step towards a real-time architecture for ecient learning of gen-},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam},
doi = {10.1037/a0023964},
eprint = {NIHMS150003},
file = {:home/sarios/papers/horde.pdf:pdf},
isbn = {0-9826571-6-1, 978-0-9826571-6-4},
issn = {0982657161},
journal = {In Practice},
keywords = {artificial intelligence,difference learning,inforcement learning,knowledge representation,off policy learning,re,real time,robotics,temporal,value function approximation},
number = {1972},
pages = {761--768},
pmid = {21604833},
title = {{Horde : A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction Categories and Subject Descriptors}},
url = {http://webdocs.cs.ualberta.ca/{~}sutton/papers/horde-aamas-11.pdf},
volume = {2},
year = {2011}
}
@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G},
doi = {10.1.1.4.6238-1},
file = {:home/sarios/papers/recent-advancements-in-hierarchical-rl.pdf:pdf},
isbn = {0924-6703},
issn = {0924-6703},
journal = {Most},
number = {5},
pages = {1--28},
title = {{Recent Advances in Hierarchical Reinforcement Learning Markov and Semi-Markov Decision Processes}},
volume = {13},
year = {2003}
}
@article{Piunovskiy2012,
abstract = {In this paper, we show that a discounted continuous-time Markov decision process in Borel spaces with randomized history-dependent policies, arbitrarily unbounded transition rates and a non-negative reward rate is equivalent to a discrete-time Markov decision process. Based on a completely new proof, which does not involve Kolmogorov's forward equation, it is shown that the value function for both models is given by the minimal non-negative solution to the same Bellman equation. A verifiable necessary and sufficient condition for the finiteness of this value function is given, which induces a new condition for the non-explosion of the underlying controlled process. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
author = {Piunovskiy, Alexey and Zhang, Yi},
doi = {10.1007/s10957-012-0015-8},
file = {:home/sarios/papers/The-Transformation-Method-for-Continuous-Time-mdps.pdf:pdf},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Continuous-time Markov decision process,Discrete-time Markov decision process,History-dependent policies,Transformation method,Unbounded transition rates},
number = {2},
pages = {691--712},
title = {{The Transformation Method for Continuous-Time Markov Decision Processes}},
volume = {154},
year = {2012}
}
@article{Owen1982,
abstract = {This advanced text introduces the principles of noncooperative game theory in a direct and uncomplicated style that will acquaint students with the broad spectrum of the field while highlighting and explaining what they need to know at any given point.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Owen, G and Owen, G},
doi = {10.4135/9781412984317},
eprint = {arXiv:1011.1669v3},
isbn = {9780803920507},
issn = {0-262-06141-4},
journal = {Collection},
keywords = {DSA 2009 BS-2 E-Democracy},
pmid = {3364812506127435976},
title = {{Game Theory}},
year = {1982}
}
@article{Bansal2017,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archivePrefix = {arXiv},
arxivId = {1710.03748},
author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
eprint = {1710.03748},
file = {:home/sarios/papers/emergent-complexity-behaviour-via-multi-agent-competition.pdf:pdf},
keywords = {self-play},
mendeley-tags = {self-play},
pages = {1--12},
title = {{Emergent Complexity via Multi-Agent Competition}},
url = {http://arxiv.org/abs/1710.03748},
volume = {2},
year = {2017}
}
@article{Hansen2004,
author = {Hansen, Eric and Bernstein, Daniel and Zilberstein, Shlomo},
file = {:home/sarios/papers/dynamic-programming-for-partially-observable-stochastic-games.pdf:pdf},
journal = {Nineteenth Conference on Artificial Conference (AAAI)},
pages = {709--715},
title = {{Dynamic Programming for Partially Observable Stochastic Games}},
year = {2004}
}
@article{Oliehoek2014,
abstract = {Bayesian methods for reinforcement learning are promising because they allow model uncertainty to be considered ex-plicitly and offer a principled way of dealing with the explo-ration/exploitation tradeoff. However, for multiagent sys-tems there have been few such approaches, and none of them apply to problems with state uncertainty. In this paper we fill this gap by proposing two frameworks for Bayesian RL for multiagent systems with state uncertainty. This includes a multiagent POMDP model where a team of agents oper-ates in a centralized fashion, but has uncertainty about the model of the environment. We also consider a best response model in which each agent also has uncertainty over the policies of the other agents. In each case, we seek to learn the appropriate models while acting in an online fashion. We transform the resulting problem into a planning problem and prove bounds on the solution quality in different situations. We demonstrate our methods using sample-based planning in several domains with varying levels of uncertainty about the model and the other agents' policies. Experimental re-sults show that overall, the approach is able to significantly decrease uncertainty and increase value when compared to initial models and policies.},
author = {Oliehoek, Frans A and Amato, Christopher},
file = {:home/sarios/papers/bayesian-rl-for-multiagent-systems-with-state-uncertainty.pdf:pdf},
journal = {AAMAS Workshop on Multiagent Sequential Decision Making Under Uncertainty, MSDM 2014},
number = {May},
title = {{Best Response Bayesian Reinforcement Learning for Multiagent Systems with State Uncertainty}},
year = {2014}
}
@article{Boutilier1996,
abstract = {There has been a growing interest in AI in the design ofmultiagent systems, especially inmultiagent cooperative planning. In this paper, we investigate the extent towhichmethods fromsingle-agent planning and learning can be applied in multiagent settings. We survey a number of different techniques fromdecision-theoretic planning and reinforcement learning and describe a number of interesting issues that arise with regard to coordinating the policies of individual agents. To this end, we describe multiagent Markov decision processes as a general model in which to frame this discussion. These are special n-person cooperative games in which agents share the same utility function. We discuss coordination mechanisms based on imposed conventions (or social laws) as well as learning methods for coordination. Our focus is on the decomposition of sequential decision processes so that coordination can be learned (or imposed) locally, at the level of individual states. We also discuss the use of structured problem representations and their role in the generalization of learned conventions and in approximation.},
author = {Boutilier, Craig},
file = {:home/sarios/papers/planning-learning-and-coordination-in-multiagent-decision-processes.pdf:pdf},
isbn = {1-55860-417-9},
issn = {1-55860-417-9},
journal = {Proceedings of the 6th conference on Theoretical aspects of rationality and knowledge},
keywords = {Environments,conventions,coordination,emergent,mmdp,social law},
mendeley-tags = {Environments},
pages = {195--210},
title = {{Planning, learning and coordination in multiagent decision processes}},
url = {http://dl.acm.org/citation.cfm?id=1029710},
year = {1996}
}
@article{Sharma2017,
abstract = {We present a neural architecture that takes as input a 2D or 3D shape and outputs a program that generates the shape. The instructions in our program are based on constructive solid geometry principles, i.e., a set of boolean operations on shape primitives defined recursively. Bottom-up techniques for this shape parsing task rely on primitive detection and are inherently slow since the search space over possible primitive combinations is large. In contrast, our model uses a recurrent neural network that parses the input shape in a top-down manner, which is significantly faster and yields a compact and easy-to-interpret sequence of modeling instructions. Our model is also more effective as a shape detector compared to existing state-of-the-art detection techniques. We finally demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques.},
archivePrefix = {arXiv},
arxivId = {1712.08290},
author = {Sharma, Gopal and Goyal, Rishabh and Liu, Difan and Kalogerakis, Evangelos and Maji, Subhransu},
doi = {10.1109/CVPR.2018.00578},
eprint = {1712.08290},
file = {:home/sarios/papers/csgnet-constructive-solid-geometry.pdf:pdf},
title = {{CSGNet: Neural Shape Parser for Constructive Solid Geometry}},
url = {http://arxiv.org/abs/1712.08290},
year = {2017}
}
@misc{Bellman1957a,
abstract = {Bellman R. A markovian decision process. Journal of Mathematics and Mechanics, 6, 1957.},
author = {Bellman, Richard},
booktitle = {Journal Of Mathematics And Mechanics},
doi = {10.1007/BF02935461},
file = {:home/sarios/papers/a-markovian-decision-process.pdf:pdf},
isbn = {0095-9057},
issn = {01650114},
pages = {679--684},
title = {{A Markovian decision process}},
volume = {6},
year = {1957}
}
@article{Juliani,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.02627v1},
author = {Juliani, Arthur and Henry, Hunter and Lange, Danny},
eprint = {arXiv:1809.02627v1},
file = {:home/sarios/papers/unity-ml-agents.pdf:pdf},
pages = {1--18},
title = {{Unity : A General Platform for Intelligent Agents}}
}
@article{Beattie2016,
abstract = {DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.},
archivePrefix = {arXiv},
arxivId = {1612.03801},
author = {Beattie, Charles and Leibo, Joel Z. and Teplyashin, Denis and Ward, Tom and Wainwright, Marcus and K{\"{u}}ttler, Heinrich and Lefrancq, Andrew and Green, Simon and Vald{\'{e}}s, V{\'{i}}ctor and Sadik, Amir and Schrittwieser, Julian and Anderson, Keith and York, Sarah and Cant, Max and Cain, Adam and Bolton, Adrian and Gaffney, Stephen and King, Helen and Hassabis, Demis and Legg, Shane and Petersen, Stig},
eprint = {1612.03801},
file = {:home/sarios/papers/deepmind-lab.pdf:pdf},
pages = {1--11},
title = {{DeepMind Lab}},
url = {http://arxiv.org/abs/1612.03801},
year = {2016}
}
@article{Espeholt2018,
abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
archivePrefix = {arXiv},
arxivId = {1802.01561},
author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
eprint = {1802.01561},
file = {:home/sarios/papers/impala.pdf:pdf},
keywords = {off policy},
mendeley-tags = {off policy},
title = {{IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures}},
url = {http://arxiv.org/abs/1802.01561},
year = {2018}
}
@article{Johnson2016,
abstract = {We present Project Malmo – an AI experimenta-tion platform built on top of the popular computer game Minecraft, and designed to support funda-mental research in artificial intelligence. As the AI research community pushes for artificial gen-eral intelligence (AGI), experimentation platforms are needed that support the development of flexible agents that learn to solve diverse tasks in complex environments. Minecraft is an ideal foundation for such a platform, as it exposes agents to complex 3D worlds, coupled with infinitely varied game-play. Project Malmo provides a sophisticated abstraction layer on top of Minecraft that supports a wide range of experimentation scenarios, ranging from navi-gation and survival to collaboration and problem solving tasks. In this demo we present the Malmo platform and its capabilities. The platform is pub-licly released as open source software at IJCAI, to support openness and collaboration in AI research.},
author = {Johnson, Matthew and Hofmann, Katja and Hutton, Tim and Bignell, David},
file = {:home/sarios/papers/malmo.pdf:pdf},
isbn = {978-1-57735-770-4},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Demonstrations},
pages = {4246--4247},
title = {{The malmo platform for artificial intelligence experimentation}},
volume = {2016-Janua},
year = {2016}
}
@article{Claes2017,
abstract = {Warehouse commissioning is a complex task in which a team of robots needs to gather and deliver items as fast and effi-ciently as possible while adhering to the constraint capacity of the robots. Typical centralised control approaches can quickly become infeasible when dealing with many robots. Instead, we tackle this spatial task allocation problem via distributed planning on each robot in the system. State of the art distributed planning approaches suffer from a number of limiting assumptions and ad-hoc approximations. This paper demonstrates how to use Monte Carlo Tree Search (MCTS) to overcome these limitations and provide scalabil-ity in a more principled manner. Our simulation-based eval-uation demonstrates that this translates to higher task per-formance, especially when tasks get more complex. More-over, this higher performance does not come at the cost of scalability: in fact, the proposed approach scales better than the previous best approach, demonstrating excellent perfor-mance on an 8-robot team servicing a warehouse comprised of over 200 locations.},
author = {Claes, Daniel and Oliehoek, Frans and Baier, Hendrik and Tuyls, Karl},
file = {:home/sarios/papers/decentralized-online-planning-for-multi-robot-warehouse.pdf:pdf},
isbn = {9781510855076},
issn = {15582914},
journal = {International Conference on Autonomous Agents and Multiagent Systems},
keywords = {Decentralised Online Planning,MMDP,Multi-Robot Sys-tems,SPATAPs,Warehouse Commissioning},
number = {Aamas},
pages = {492--500},
title = {{Decentralised Online Planning for Multi-Robot Warehouse Commissioning}},
url = {http://www.fransoliehoek.net/docs/Claes17AAMAS.pdf},
year = {2017}
}
@article{Graves2016,
abstract = {Modern computers separate computation and memory. Computation is performed by a processor, which can use an addressable memory to bring operands in and out of play. This confers two important benefits: the use of extensible storage to write new information and the ability to treat the contents of memory as variables. Variables are critical to algorithm generality: to perform the same procedure on one datum or another, an algorithm merely has to change the address it reads from. In contrast to computers, the computational and memory resources of artificial neural networks are mixed together in the network weights and neuron activity. This is a major liability: as the memory demands of a task increase, these networks cannot allocate new storage dynam-ically, nor easily learn algorithms that act independently of the values realized by the task variables. Although recent breakthroughs demonstrate that neural networks are remarkably adept at sensory processing 1 , sequence learning 2,3 and reinforcement learning 4 , cognitive scientists and neuroscientists have argued that neural networks are limited in their ability to represent variables and data structures 5–9 , and to store data over long timescales without interference 10,11 . We aim to combine the advantages of neu-ral and computational processing by providing a neural network with read–write access to external memory. The access is narrowly focused, minimizing interference among memoranda and enabling long-term storage 12,13 . The whole system is differentiable, and can therefore be trained end-to-end with gradient descent, allowing the network to learn how to operate and organize the memory in a goal-directed manner.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and Colmenarejo, Sergio G{\'{o}}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/nature20101},
eprint = {arXiv:1410.5401v2},
file = {:home/sarios/papers/differentiable-neural-computers.pdf:pdf},
isbn = {0896-6273},
issn = {14764687},
journal = {Nature},
number = {7626},
pages = {471--476},
pmid = {26774160},
publisher = {Nature Publishing Group},
title = {{Hybrid computing using a neural network with dynamic external memory}},
url = {http://dx.doi.org/10.1038/nature20101},
volume = {538},
year = {2016}
}
@article{Koza1996,
abstract = {"A Bradford book."},
author = {Koza, John R. and {Genetic Programming Conference (1st : 1996 : Stanford University)}, Darrell and Pyeatt, Larry},
file = {:home/sarios/papers/Evolution/A Comparison between Cellular Encoding and Direct Encoding for Genetic Neural Networks.pdf:pdf},
isbn = {0262611279},
journal = {Proceedings of the 1st annual conference on genetic programming},
pages = {568},
title = {{A comparison between cellular encoding and direct encoding for genetic neural networks}},
year = {1996}
}
@article{Potter1994,
abstract = {A general model for the coevolution of cooperating species is presented. This model is instantiated and tested in the domain of function optimization, and compared with a traditional GA-based function optimizer. The results are encouraging in two respects. They suggest ways in which the performance of GA and other EA-based optimizers can be improved, and they suggest a new approach to evolving complex structures such as neural networks and rule sets.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Potter, Mitchell A. and Jong, Kenneth A.},
doi = {10.1007/3-540-58484-6_269},
eprint = {9780201398298},
file = {:home/sarios/papers/Evolution/A Cooperative Coevolutionary Approach to Function Optimization.pdf:pdf},
isbn = {3-540-58484-6},
issn = {0302-9743},
pages = {249--257},
pmid = {4520227},
title = {{A cooperative coevolutionary approach to function optimization}},
url = {http://link.springer.com/10.1007/3-540-58484-6{\_}269},
year = {1994}
}
@article{Deb2002,
abstract = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN3) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed},
author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, T.},
doi = {10.1109/4235.996017},
file = {:home/sarios/papers/Evolution/A Fast and Elitist Multiobjective Genetic Algorithm- NSGA-II.pdf:pdf},
isbn = {1089-778X VO - 6},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Constraint handling,Elitism,Genetic algorithms,Multicriterion decision making,Multiobjective optimization,Pareto-optimal solutions},
number = {2},
pages = {182--197},
title = {{A fast and elitist multiobjective genetic algorithm: NSGA-II}},
volume = {6},
year = {2002}
}
@article{Bryant2018,
abstract = {A multi-agent architecture called the Adaptive Team of Agents (ATA) is introduced, wherein homogeneous agents adopt specific roles in a team dynamically in order to address all the sub-tasks necessary to meet the team's goals. Artificial neural networks are then trained by neuroevolution to produce an example of such a team, trained to solve the problem posed by a simple strategy game. The evolutionary algorithm is found to induce the necessary in situ adaptivity of behavior into the agents, even when controlled by stateless feed-forward networks.},
author = {Bryant, Bobby D. and Miikkulainen, Risto},
doi = {10.1007/978-3-319-64816-3_5},
file = {:home/sarios/papers/Evolution/A Neuroevolutionary Approach to Adaptive Multi-agent Teams.pdf:pdf},
isbn = {9783319648163},
issn = {21984190},
journal = {Studies in Systems, Decision and Control},
pages = {87--115},
title = {{A neuroevolutionary approach to adaptive multi-agent teams}},
volume = {117},
year = {2018}
}
@article{Miller1998,
author = {Miller, Julian F and Harding, Simon L},
file = {:home/sarios/papers/Evolution/GECCO 2012 Tutorial- Cartesian Genetic Programming.pdf:pdf},
isbn = {9781450311786},
pages = {1093--1116},
title = {{Abstract Contents Classic CGP Modular CGP Self-modifying CGP Cyclic CGP Applications Resources Bibliography Genetic Programming The automatic evolution of computer programs Origins of Cartesian Genetic Programming ( CGP ) Grew out of work in the evolution}},
year = {1998}
}
@article{Miller1975,
abstract = {Genetic algorithms utilize populations of individual hypotheses that converge over time to a single optimum, even within a multimodal domain. This paper examines methods that enable genetic algorithms to identify multiple optima within multimodal domains by maintaining population members within the niches defined by the multiple optima. A new mechanism, dynamic niche sharing, is developed that is able to efficiently identify and search multiple niches (peaks) in a multimodal domain. Dynamic niche sharing is shown to perform better than two other methods for multiple optima identification, standard sharing and deterministic crowding},
author = {Miller, B.L. and Shaw, M.J.},
doi = {10.1109/ICEC.1996.542701},
file = {:home/sarios/papers/Evolution/Genetic Algorithms with Dynamic Niche Sharing for Multimodal Function Optimization.pdf:pdf},
isbn = {0-7803-2902-3},
journal = {Proceedings of IEEE International Conference on Evolutionary Computation},
pages = {786--791},
title = {{Genetic algorithms with dynamic niche sharing for multimodal function optimization}},
url = {http://ieeexplore.ieee.org/document/542701/},
year = {1975}
}
@article{Miller1995,
abstract = {Tournament selection is a useful and robust selection mechanism commonly used by genetic algorithms. The selection pressure of tournament selection directly varies with the tournament size --- the more competitors, the higher the resulting selection pressure. This article develops a model, based on order statistics, that can be used to quantitatively predict the resulting selection pressure of a tournament of a given size. This model is used to predict the convergence rates of genetic algorithms utilizing tournament selection. While tournament selection is often used in conjunction with noisy (imperfect) fitness functions, little is understood about how the noise affects the resulting selection pressure. The model is extended to quantitatively predict the selection pressure for tournament selection utilizing noisy fitness functions. Given the tournament size and noise level of a noisy fitness function, the extended model is used to predict the resulting selection pressure of tournament...},
author = {Miller, Brad L and Goldberg, David E},
doi = {10.1.1.30.6625},
file = {:home/sarios/papers/Evolution/Genetic Algorithms, Tournament Selection, and the Effects of Noise.pdf:pdf},
isbn = {0891-2513},
issn = {0891-2513},
journal = {Complex Systems},
number = {3},
pages = {193--212},
title = {{Genetic Algorithms, Tournament Selection, and the Effects of Noise}},
volume = {9},
year = {1995}
}
@article{Watson2011,
abstract = {We examine the behavior of sexual and asexual populations in modular multipeaked fitness landscapes and show that sexuals can systematically reach different, higher fitness adaptive peaks than asexuals. Whereas asexuals must move against selection to escape local optima, sexuals reach higher fitness peaks reliably because they create specific genetic variants that "skip over" fitness valleys, moving from peak to peak in the fitness landscape. This occurs because recombination can supply combinations of mutations in functional composites or "modules," that may include individually deleterious mutations. Thus when a beneficial module is substituted for another less-fit module by sexual recombination it provides a genetic variant that would require either several specific simultaneous mutations in an asexual population or a sequence of individual mutations some of which would be selected against. This effect requires modular genomes, such that subsets of strongly epistatic mutations are tightly physically linked. We argue that such a structure is provided simply by virtue of the fact that genomes contain many genes each containing many strongly epistatic nucleotides. We briefly discuss the connections with "building blocks" in the evolutionary computation literature. We conclude that there are conditions in which sexuals can systematically evolve high-fitness genotypes that are essentially unevolvable for asexuals.},
author = {Watson, Richard A. and Weinreich, Daniel M. and Wakeley, John},
doi = {10.1111/j.1558-5646.2010.01144.x},
file = {:home/sarios/papers/Evolution/Genome structure and the benefit of sex.pdf:pdf},
isbn = {1558-5646 (Electronic)$\backslash$r0014-3820 (Linking)},
issn = {00143820},
journal = {Evolution},
keywords = {Epistasis,Models/simulations,Molecular evolution,Population genetics,Sex},
number = {2},
pages = {523--536},
pmid = {21029076},
title = {{Genome structure and the benefit of sex}},
volume = {65},
year = {2011}
}
@article{Gangwani2017,
abstract = {Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency.},
archivePrefix = {arXiv},
arxivId = {1711.01012},
author = {Gangwani, Tanmay and Peng, Jian},
eprint = {1711.01012},
file = {:home/sarios/papers/Evolution/Genetic Policy Optimization.pdf:pdf},
pages = {1--13},
title = {{Policy Optimization by Genetic Distillation}},
url = {http://arxiv.org/abs/1711.01012},
year = {2017}
}
@article{Ryan1998,
abstract = {We describe a Genetic Algorithm that can evolve complete programs. Using a variabe lenght linear genome to govern how a Backus Naur Form grammar definition is mapped to a program, expressions and programs of arbitrary complexity may be evolved. Other automatic programming methods are described, before our system, Grammatical Evolution, is applied to a symbolic regression problem.},
author = {Ryan, Conor and Collins, JJ and O'Neil, Michael},
doi = {10.10007/BFb0055930},
file = {:home/sarios/papers/Evolution/Grammatical Evolution - Evolving Programs for an Arbitrary Language.pdf:pdf},
journal = {Genetic Programming},
pages = {83--96},
title = {{Grammatical Evolution: Envolving Programs for an Arbitrary Language}},
year = {1998}
}
@article{Maheswaranathan2018,
abstract = {Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications, or when using synthetic gradients). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a guiding subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and we use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and a synthetic gradient problem, demonstrating improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient. We provide a demo of Guided ES at https://github.com/brain-research/guided-evolutionary-strategies},
archivePrefix = {arXiv},
arxivId = {1806.10230},
author = {Maheswaranathan, Niru and Metz, Luke and Tucker, George and Sohl-Dickstein, Jascha},
doi = {arXiv:1806.10230v2},
eprint = {1806.10230},
file = {:home/sarios/papers/Evolution/Guided evolutionary strategies- escaping the curse of dimensionality in random search.pdf:pdf},
title = {{Guided evolutionary strategies: escaping the curse of dimensionality in random search}},
url = {http://arxiv.org/abs/1806.10230},
year = {2018}
}
@article{Cully2018,
abstract = {Enabling artificial agents to automatically learn complex, versatile and high-performing behaviors is a long-lasting challenge. This paper presents a step in this direction with hierarchical behavioral repertoires that stack several behavioral repertoires to generate sophisticated behaviors. Each repertoire of this architecture uses the lower repertoires to create complex behaviors as sequences of simpler ones, while only the lowest repertoire directly controls the agent's movements. This paper also introduces a novel approach to automatically define behavioral descriptors thanks to an unsupervised neural network that organizes the produced high-level behaviors. The experiments show that the proposed architecture enables a robot to learn how to draw digits in an unsupervised manner after having learned to draw lines and arcs. Compared to traditional behavioral repertoires, the proposed architecture reduces the dimensionality of the optimization problems by orders of magnitude and provides behaviors with a twice better fitness. More importantly, it enables the transfer of knowledge between robots: a hierarchical repertoire evolved for a robotic arm to draw digits can be transferred to a humanoid robot by simply changing the lowest layer of the hierarchy. This enables the humanoid to draw digits although it has never been trained for this task.},
archivePrefix = {arXiv},
arxivId = {1804.07127},
author = {Cully, Antoine and Demiris, Yiannis},
doi = {10.1145/3205455.3205571},
eprint = {1804.07127},
file = {:home/sarios/papers/Evolution/Hierarchical Behavioral Repertoires with Unsupervised Descriptors.pdf:pdf},
isbn = {9781450356183},
keywords = {behavioral repertoires},
title = {{Hierarchical Behavioral Repertoires with Unsupervised Descriptors}},
url = {http://arxiv.org/abs/1804.07127{\%}0Ahttp://dx.doi.org/10.1145/3205455.3205571},
year = {2018}
}
@article{Schaul2011,
abstract = {The family of natural evolution strategies (NES) offers a principled approach to real-valued evolutionary optimization. NES follows the natural gradient of the expected fitness on the parameters of its search distribution. While general in its formulation, previous research has focused on multivariate Gaussian search distributions. Here we exhibit problem classes for which other search distributions are more appropriate, and then derive corresponding NES-variants. First, for separable distributions we obtain SNES, whose complexity is only O(d) instead of O(d(3)). We apply SNES to problems of previously unattainable dimensionality, recovering lowest-energy structures on the Lennard-Jones atom clusters, and obtaining state-of-the-art results on neuro-evolution benchmarks. Second, we develop a new, equivalent formulation based on invariances. This allows for generalizing NES to heavy-tailed distributions, even those with undefined variance, which aids in overcoming deceptive local optima.},
author = {Schaul, Tom and Glasmachers, Tobias and Schmidhuber, J{\"{u}}rgen},
doi = {10.1145/2001576.2001692},
file = {:home/sarios/papers/Evolution/High Dimensions and Heavy Tails for Natural Evolution Strategies.pdf:pdf},
isbn = {9781450305570},
journal = {Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11},
keywords = {black-box optimiza-,evolution strategies,global optimization,natural gradient,tion},
pages = {845},
title = {{High dimensions and heavy tails for natural evolution strategies}},
url = {http://portal.acm.org/citation.cfm?doid=2001576.2001692},
year = {2011}
}
@article{Mouret2015,
abstract = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.},
archivePrefix = {arXiv},
arxivId = {1504.04909},
author = {Mouret, Jean-Baptiste and Clune, Jeff},
eprint = {1504.04909},
file = {:home/sarios/papers/Evolution/Illuminating search spaces by mapping elites.pdf:pdf},
isbn = {9781479999934},
pages = {1--15},
title = {{Illuminating search spaces by mapping elites}},
url = {http://arxiv.org/abs/1504.04909},
year = {2015}
}
@article{Merrild2018,
abstract = {Recent developments within memory-augmented neural networks have solved sequential problems requiring long-term memory, which are intractable for traditional neural networks. However, current approaches still struggle to scale to large memory sizes and sequence lengths. In this paper we show how access to memory can be encoded geometrically through a HyperNEAT-based Neural Turing Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT encoding allows for training on small memory vectors in a bit-vector copy task and then applying the knowledge gained from such training to speed up training on larger size memory vectors. Additionally, we demonstrate that in some instances, networks trained to copy bit-vectors of size 9 can be scaled to sizes of 1,000 without further training. While the task in this paper is simple, these results could open up the problems amendable to networks with external memories to problems with larger memory vectors and theoretically unbounded memory sizes.},
archivePrefix = {arXiv},
arxivId = {1710.04748},
author = {Merrild, Jakob and Rasmussen, Mikkel Angaju and Risi, Sebastian},
doi = {10.1007/978-3-319-77538-8_50},
eprint = {1710.04748},
file = {:home/sarios/papers/Evolution/HyperENTM- Evolving Scalable Neural Turing Machines through HyperNEAT.pdf:pdf},
isbn = {9783319775371},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {HyperNEAT,Indirect encoding,Neural Turing Machine,Neuroevolution},
pages = {750--766},
title = {{HyperNTM: Evolving Scalable Neural Turing Machines Through HyperNEAT}},
volume = {10784 LNCS},
year = {2018}
}
@article{Conti2017,
abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
archivePrefix = {arXiv},
arxivId = {1712.06560},
author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
eprint = {1712.06560},
file = {:home/sarios/papers/Evolution/Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents.pdf:pdf},
pages = {1--17},
title = {{Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents}},
url = {http://arxiv.org/abs/1712.06560},
year = {2017}
}
@article{F.J.GomezandR.Miikkulainen1997,
author = {{F. J. Gomez and R. Miikkulainen}},
file = {:home/sarios/papers/Evolution/Incremental Evolution of Complex General Behavior.pdf:pdf},
pages = {1--21},
title = {{“Incremental Evolution of Complex General Behavior,”}},
volume = {vol. 5, no},
year = {1997}
}
@article{Alba2000,
abstract = {Parallel genetic algorithms (PGAs) have been traditionally used to overcome the intense use of CPU and memory that serial GAs show in complex problems. Non-parallel GAs can be classified into two classes: panmictic and structured-population algorithms. The difference lies in whether any individual in the population can mate with any other one or not. In this work, they are both considered as two reproductive loop types executed in the islands of a parallel distributed GA. Our aim is to extend the existing studies from more conventional sequential islands to other kinds of evolution. A key issue in such a coarse grain PGA is the migration policy, since it governs the exchange of individuals among the islands. This paper investigates the influence of migration frequency and migrant selection in a ring of islands running either steady-state, generational, or cellular GAs. A diversity analysis is also offered from an entropy point of view. The study uses different problem types, namely easy, deceptive, multimodal, NP-Complete, and epistatic search landscapes in order to provide a wide spectrum of problem difficulties to support the results. Large isolation values and random selection of the migrants are demonstrated as providing a larger probability of success and a smaller number of visited points. Also, interesting observations on the relative performance of the different models are offered, as well as we point out the considerable benefits that can accrue from asynchronous migration.},
author = {Alba, Enrique and Troya, Jos{\'{e}} M.},
doi = {10.1023/A:1008358805991},
file = {:home/sarios/papers/Evolution/Influence of the Migration Policy in Parallel Distributed GAs with Structured and Panmictic Populations.pdf:pdf},
issn = {0924669X},
journal = {Applied Intelligence},
keywords = {basic island evolution,complex search spaces,entropy,migration policy,parallel genetic algorithms},
number = {3},
pages = {163--181},
title = {{Influence of the migration policy in parallel distributed GAs with structured and panmictic populations}},
volume = {12},
year = {2000}
}
@article{Cuccu2011,
abstract = {Neuroevolution, the artificial evolution of neural networks, has shown great promise on continuous reinforcement learning tasks that require memory. However, it is not yet directly applicable to realistic embedded agents using high-dimensional (e.g. raw video images) inputs, requiring very large networks. In this paper, neuroevolution is combined with an unsupervised sensory pre-processor or compressor that is trained on images generated from the environment by the population of evolving recurrent neural network controllers. The compressor not only reduces the input cardinality of the controllers, but also biases the search toward novel controllers by rewarding those controllers that discover images that it reconstructs poorly. The method is successfully demonstrated on a vision-based version of the well-known mountain car benchmark, where controllers receive only single high-dimensional visual images of the environment, from a third-person perspective, instead of the standard two-dimensional state vector which includes information about velocity.},
author = {Cuccu, Giuseppe and Luciw, Matthew and Schmidhuber, J{\"{u}}rgen and Gomez, Faustino},
doi = {10.1109/DEVLRN.2011.6037324},
file = {:home/sarios/papers/Evolution/Intrinsically Motivated NeuroEvolution for Vision-Based Reinforcement Learning.pdf:pdf},
isbn = {9781612849904},
issn = {2161-9476},
journal = {2011 IEEE International Conference on Development and Learning, ICDL 2011},
pages = {1--7},
title = {{Intrinsically motivated neuroevolution for vision-based reinforcement learning}},
year = {2011}
}
@article{Nguyen2015,
abstract = {The Achilles Heel of stochastic optimization algorithms is getting trapped on local optima. Novelty Search avoids this problem by encouraging a search in all interesting directions. That occurs by replacing a performance objective with a reward for novel behaviors, as defined by a human-crafted, and often simple, behavioral distance function. While Novelty Search is a major conceptual breakthrough and outperforms traditional stochastic optimization on certain problems, it is not clear how to apply it to challenging, high-dimensional problems where specifying a useful behavioral distance function is difficult. For example, in the space of images, how do you encourage novelty to produce hawks and heroes instead of endless pixel static? Here we propose a new algorithm, the Innovation Engine, that builds on Novelty Search by replacing the human-crafted behavioral distance with a Deep Neural Network (DNN) that can recognize interesting differences between phenotypes. The key insight is that DNNs can recognize similarities and differences between phenotypes at an abstract level, wherein novelty means interesting novelty. For example, a novelty pressure in image space does not explore in the low-level pixel space, but instead creates a pressure to create new types of images (e.g. churches, mosques, obelisks, etc.). Here we describe the long-term vision for the Innovation Engine algorithm, which involves many technical challenges that remain to be solved. We then implement a simplified version of the algorithm that enables us to explore some of the algorithm's key motivations. Our initial results, in the domain of images, suggest that Innovation Engines could ultimately automate the production of endless streams of interesting solutions in any domain: e.g. producing intelligent software, robot controllers, optimized physical components, and art.},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1145/2739480.2754703},
file = {:home/sarios/papers/Evolution/Innovation Engines- Automated creativity and improved stochastic optimization via Deep Learning.pdf:pdf},
isbn = {9781450334723},
journal = {Proceedings of the 2015 on Genetic and Evolutionary Computation Conference - GECCO '15},
keywords = {deep learning,deep neural networks,map-elites,novelty search},
number = {x},
pages = {959--966},
title = {{Innovation Engines: Automated Creativity and Improved Stochastic Optimization via Deep Learning}},
url = {http://dl.acm.org/citation.cfm?id=2739480.2754703},
volume = {0},
year = {2015}
}
@article{Wang2017,
abstract = {IEEE The processes of evolution and learning interact. Learning is an evolved strategy that improves fitness, especially in a world where some aspects cannot realistically be encoded in the genome. We endeavored to see if evolution could sculpt a generic neuroplasticity mechanism into a learning rule that would give virtual organisms an advantage in a simulated foraging environment. Our virtual organisms have brains with nine neurons. The connections between those neurons are adjusted by a plasticity rule that is computed by another fixed neural network. Evolution experiments repeatedly found plasticity networks that conferred an adaptive advantage, even outperforming populations that were given a parametric Hebbian plasticity mechanism. Evolution also favored the inclusion of genetically encoded heterogeneity. We also investigate how behavior is influenced by various brain- and movement-related energy penalty terms in the fitness function.},
author = {Wang, Lin and Orchard, Jeff},
doi = {10.1109/TSMC.2017.2755066},
file = {:home/sarios/papers/Evolution/Investigating the Evolution of a Neuroplasticity Network for Learning.pdf:pdf},
issn = {21682232},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
keywords = {Animals,Biological neural networks,Evolution (biology),Evolutionary computation,Neurons,Neuroplasticity,learning rules,neural networks},
pages = {1--13},
title = {{Investigating the Evolution of a Neuroplasticity Network for Learning}},
year = {2017}
}
@article{Sipper2018,
abstract = {The practice of evolutionary algorithms involves the tuning of many parameters. How big should the population be? How many generations should the algorithm run? What is the (tournament selection) tournament size? What probabilities should one assign to crossover and mutation? Through an extensive series of experiments over multiple evolutionary algorithm implementations and problems we show that parameter space tends to be rife with viable parameters, at least for 25 the problems studied herein. We discuss the implications of this finding in practice.},
archivePrefix = {arXiv},
arxivId = {1706.04119},
author = {Sipper, Moshe and Fu, Weixuan and Ahuja, Karuna and Moore, Jason H.},
doi = {10.1186/s13040-018-0164-x},
eprint = {1706.04119},
file = {:home/sarios/papers/Evolution/Investigating the parameter space of evolutionary algorithms.pdf:pdf},
issn = {17560381},
journal = {BioData Mining},
keywords = {Evolutionary algorithms,Genetic programming,Hyper-parameter,Meta-genetic algorithm,Parameter tuning},
number = {1},
pages = {1--14},
publisher = {BioData Mining},
title = {{Investigating the parameter space of evolutionary algorithms}},
volume = {11},
year = {2018}
}
@article{Real2017,
abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6{\%} (95.6{\%} for ensemble) and 77.0{\%}, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
archivePrefix = {arXiv},
arxivId = {1703.01041},
author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc and Kurakin, Alex},
eprint = {1703.01041},
file = {:home/sarios/papers/Evolution/Large-Scale Evolution of Image Classifiers.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Large-Scale Evolution of Image Classifiers}},
url = {http://arxiv.org/abs/1703.01041},
year = {2017}
}
@article{Butz2007,
author = {Butz, Martin V},
file = {:home/sarios/papers/Evolution/Learning Classifier Systems.pdf:pdf},
isbn = {9781595936981},
journal = {Cognitive Psychology},
pages = {3035--3055},
title = {{Overview – Part 1 Learning Classifier Systems LCS Renaissance Since 1990s LCSs : Frameworks and Problem Types : Problem Types :}},
year = {2007}
}
@article{Churchill2016,
abstract = {Neural networks and evolutionary computation have a rich intertwined history. They most commonly appear together when an evolutionary algorithm optimises the parameters and topology of a neural network for reinforcement learning problems, or when a neural network is applied as a surrogate fitness function to aid the evolutionary optimisation of expensive fitness functions. In this paper we take a different approach, asking the question of whether a neural network can be used to provide a mutation distribution for an evolutionary algorithm, and what advantages this approach may offer? Two modern neural network models are investigated, a Denoising Autoencoder modified to produce stochastic outputs and the Neural Autoregressive Distribution Estimator. Results show that the neural network approach to learning genotypes is able to solve many difficult discrete problems, such as MaxSat and HIFF, and regularly outperforms other evolutionary techniques.},
archivePrefix = {arXiv},
arxivId = {1604.04153},
author = {Churchill, Alexander W. and Sigtia, Siddharth and Fernando, Chrisantha},
eprint = {1604.04153},
file = {:home/sarios/papers/Evolution/Learning to Generate Genotypes with Neural Networks.pdf:pdf},
title = {{Learning to Generate Genotypes with Neural Networks}},
url = {http://arxiv.org/abs/1604.04153},
year = {2016}
}
@article{Virgo2017,
author = {Virgo, Nathaniel and Agmon, Eran and Fernando, Chrisantha},
file = {:home/sarios/papers/Evolution/Lineage selection leads to evolvability at large population sizes.pdf:pdf},
journal = {Proceedings of the European Conference on Artificial Life},
number = {September},
pages = {4--8},
title = {{Lineage selection leads to evolvability at large population sizes}},
year = {2017}
}
@article{,
file = {:home/sarios/papers/Evolution/Methods for Competitive Co-evolution- Finding Opponents Worth Beating.pdf:pdf},
title = {{No Title}}
}
@article{Brant2017,
author = {Brant, Jonathan C and Stanley, Kenneth O},
doi = {10.1145/3071178.3071186},
file = {:home/sarios/papers/Evolution/Minimal Criterion Coevolution- A New Approach to Open-Ended Search.pdf:pdf},
isbn = {978-1-4503-4920-8},
journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
keywords = {NEAT,artificial life,coevolution,non-objective search,novelty search,open-ended evolution},
number = {Gecco},
pages = {67--74},
title = {{Minimal Criterion Coevolution: A New Approach to Open-ended Search}},
url = {http://doi.acm.org/10.1145/3071178.3071186},
year = {2017}
}
@article{Cheng2018,
author = {Cheng, Ran and He, Cheng and Jin, Yaochu and Yao, Xin},
doi = {10.1007/s40747-018-0080-1},
file = {:home/sarios/papers/Evolution/Model-based evolutionary algorithms- a short survey.pdf:pdf},
isbn = {2017030317},
issn = {2199-4536},
journal = {Complex {\&} Intelligent Systems},
keywords = {Model-based evolutionary algorithms,Estimation of ,estimation of distribution algorithms,inverse,model-based evolutionary algorithms,surrogate modelling},
publisher = {Springer Berlin Heidelberg},
title = {{Model-based evolutionary algorithms: a short survey}},
url = {http://link.springer.com/10.1007/s40747-018-0080-1},
year = {2018}
}
@article{Hanne2000,
abstract = {As the name suggests, multi-objective optimization involves optimizing a number of objectives si- multaneously. The problem becomes challenging when the objectives are of conflict to each other, that is, the optimal solution of an objective function is different from that of the other. In solving such problems, with or without the presence of constraints, these problems give rise to a set of trade-off opti- mal solutions, popularly known as Pareto-optimal solutions. Due to the multiplicity in solutions, these problems were proposed to be solved suitably using evolutionary algorithms which use a population ap- proach in its search procedure. Starting with parameterized procedures in early nineties, the so-called evolutionary multi-objective optimization (EMO) algorithms is now an established field of research and application with many dedicated texts and edited books, commercial softwares and numerous freely downloadable codes, a biannual conference series running successfully since 2001, special sessions and workshops held at all major evolutionary computing conferences, and full-time researchers from uni- versities and industries from all around the globe. In this chapter, we provide a brief introduction to its operating principles and outline the current research and application studies of EMO.},
author = {Hanne, Thomas},
doi = {2011003},
file = {:home/sarios/papers/Evolution/Multi-Objective Optimization Using Evolutionary Algorithms- An Introduction.pdf:pdf},
isbn = {0-471-87339-X},
issn = {13811231},
journal = {Journal of Heuristics},
number = {3},
pages = {1--24},
title = {{Multi-Objective Optimization Using Evolutionary Algorithms : An Introduction}},
url = {http://www.springerlink.com/index/R12156725766056J.pdf},
volume = {6},
year = {2000}
}
@article{Clune2008,
abstract = {The rate of mutation is central to evolution. Mutations are required for adaptation, yet most mutations with phenotypic effects are deleterious. As a consequence, the mutation rate that maximizes adaptation will be some intermediate value. Here, we used digital organisms to investigate the ability of natural selection to adjust and optimize mutation rates. We assessed the optimal mutation rate by empirically determining what mutation rate produced the highest rate of adaptation. Then, we allowed mutation rates to evolve, and we evaluated the proximity to the optimum. Although we chose conditions favorable for mutation rate optimization, the evolved rates were invariably far below the optimum across a wide range of experimental parameter settings. We hypothesized that the reason that mutation rates evolved to be suboptimal was the ruggedness of fitness landscapes. To test this hypothesis, we created a simplified landscape without any fitness valleys and found that, in such conditions, populations evolved near-optimal mutation rates. In contrast, when fitness valleys were added to this simple landscape, the ability of evolving populations to find the optimal mutation rate was lost. We conclude that rugged fitness landscapes can prevent the evolution of mutation rates that are optimal for long-term adaptation. This finding has important implications for applied evolutionary research in both biological and computational realms.},
author = {Clune, Jeff and Misevic, Dusan and Ofria, Charles and Lenski, Richard E. and Elena, Santiago F. and Sanju{\'{a}}n, Rafael},
doi = {10.1371/journal.pcbi.1000187},
file = {:home/sarios/papers/Evolution/Natural Selection Fails to Optimize Mutation Rates for Long-Term Adaptation on Rugged Fitness Landscapes.pdf:pdf},
isbn = {1553-7358 (Electronic)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {9},
pmid = {18818724},
title = {{Natural selection fails to optimize mutation rates for long-term adaptation on rugged fitness landscapes}},
volume = {4},
year = {2008}
}
@article{Churchill2013,
author = {Churchill, Alexander W. and Husbands, Phil and Philippides, Andrew},
doi = {10.1007/978-3-642-37140-0_45},
file = {:home/sarios/papers/Evolution/Multi-objectivization of the Tool Selection Problem on a Budget of Evaluations.pdf:pdf},
isbn = {9783642371394},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {CAM,Evolutionary Multicriterion Optimization,Evolutionary Multiobjective Optimization,Guided Elitism,Micro GA,NSGA-II,Preferential Search,Roughing,Tool Selection},
pages = {600--614},
title = {{Multi-objectivization of the tool selection problem on a budget of evaluations}},
volume = {7811 LNCS},
year = {2013}
}
@article{Ellefsen2015,
abstract = {A long-standing goal in artificial intelligence is creating agents that can learn a variety of different skills for different problems. In the artificial intelligence subfield of neural networks, a barrier to that goal is that when agents learn a new skill they typically do so by losing previously acquired skills, a problem called catastrophic forgetting. That occurs because, to learn the new task, neural learning algorithms change connections that encode previously acquired skills. How networks are organized critically affects their learning dynamics. In this paper, we test whether catastrophic forgetting can be reduced by evolving modular neural networks. Modularity intuitively should reduce learning interference between tasks by separating functionality into physically distinct modules in which learning can be selectively turned on or off. Modularity can further improve learning by having a reinforcement learning module separate from sensory processing modules, allowing learning to happen only in response to a positive or negative reward. In this paper, learning takes place via neuromodulation, which allows agents to selectively change the rate of learning for each neural connection based on environmental stimuli (e.g. to alter learning in specific locations based on the task at hand). To produce modularity, we evolve neural networks with a cost for neural connections. We show that this connection cost technique causes modularity, confirming a previous result, and that such sparsely connected, modular networks have higher overall performance because they learn new skills faster while retaining old skills more and because they have a separate reinforcement learning module. Our results suggest (1) that encouraging modularity in neural networks may help us overcome the long-standing barrier of networks that cannot learn new skills without forgetting old ones, and (2) that one benefit of the modularity ubiquitous in the brains of natural animals might be to alleviate the problem of catastrophic forgetting.},
author = {Ellefsen, Kai Olav and Mouret, Jean Baptiste and Clune, Jeff},
doi = {10.1371/journal.pcbi.1004128},
file = {:home/sarios/papers/Evolution/Neural Modularity Helps Organisms Evolve to Learn New Skills without Forgetting Old Skills.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {4},
pages = {1--24},
pmid = {25837826},
title = {{Neural Modularity Helps Organisms Evolve to Learn New Skills without Forgetting Old Skills}},
volume = {11},
year = {2015}
}
@article{Parker2009,
author = {Parker, Matt and Advisor, Thesis},
file = {:home/sarios/papers/Evolution/Neuro-Visual Control in the Quake II Environment.pdf:pdf},
journal = {East},
number = {August},
pages = {1--23},
title = {{in the Quake II Environment}},
volume = {4},
year = {2009}
}
@article{Reinforcement2018,
author = {Reinforcement, Deep and Problems, Learning},
file = {:home/sarios/papers/Evolution/Neuroevolution for deep reinforcement learning problems.pdf:pdf},
isbn = {9781450357647},
pages = {421--431},
title = {{Neuroevolution for • Case Study : Bipedal Walker in OpenAI Gym OpenAI Gym Environments Why Evolve Weights for RL ? Neuroevolution Algorithms ❖ Easy to get stuck in local optima . Neuroevolution Algorithms CMA-ES for Dummies}},
year = {2018}
}
@article{Gajurel2018,
abstract = {This paper uses neuroevolution of augmenting topologies to evolve control tactics for groups of units in real-time strategy games. In such games, players build economies to generate armies composed of multiple types of units with different attack and movement characteristics to combat each other. This paper evolves neural networks to control movement and attack commands, also called micro, for a group of ranged units skirmishing with a group of melee units. Our results show that neuroevolution of augmenting topologies can effectively generate neural networks capable of good micro for our ranged units against a group of hand-coded melee units. The evolved neural networks lead to kiting behavior for the ranged units which is a common tactic used by professional players in ranged versus melee skirmishes in popular real-time strategy games like Starcraft. The evolved neural networks also generalized well to other starting positions and numbers of units. We believe these results indicate the potential of neuroevolution for generating effective micro in real-time strategy games.},
archivePrefix = {arXiv},
arxivId = {1803.10288},
author = {Gajurel, Aavaas and Louis, Sushil J and Mendez, Daniel J and Liu, Siming},
eprint = {1803.10288},
file = {:home/sarios/papers/Evolution/Neuroevolution for RTS Micro.pdf:pdf},
isbn = {9781538643594},
keywords = {evolution,neat,neural networks,rts micro},
title = {{Neuroevolution for RTS Micro}},
url = {http://arxiv.org/abs/1803.10288},
year = {2018}
}
@article{Jallov2017,
abstract = {Neuroevolution (i.e. evolving artificial neural networks (ANNs) through evolutionary algorithms) has shown promise in evolving agents and robot controllers, which display complex behaviours and can adapt to their environments. These properties are also relevant to video games, since they can increase their longevity and replayability. However, the design of most current games precludes the use of any techniques which might yield unpredictable or even open-ended results. This article describes the game EvoCommander, with the goal to further demonstrate the potential of neuroevolution in games. In EvoCommander the player incrementally evolves an arsenal of ANN-controlled behaviors (e.g. ranged attack, flee, etc.) for a simple robot that has to battle other player and computer controlled robots. The game introduces the novel game mechanic of {\&}{\#}x201C;brain switching{\&}{\#}x201D;, selecting which evolved neural network is active at any point during battle. Results from playtests indicate that brain switching is a promising new game mechanic, leading to players employing interesting different strategies when training their robots and when controlling them in battle.},
author = {Jallov, Daniel and Risi, Sebastian and Togelius, Julian},
doi = {10.1109/TCIAIG.2016.2535416},
file = {:home/sarios/papers/Evolution/EvoCommander- A Novel Game Based on Evolving and Switching Between Artificial Brains.pdf:pdf},
isbn = {1943-068X VO  - PP},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Interactive evolution,NEAT,Neural networks,Neuroevolution},
number = {2},
pages = {181--191},
title = {{EvoCommander: A novel game based on evolving and switching between artificial brains}},
volume = {9},
year = {2017}
}
@article{Runarsson2000,
author = {Runarsson, Thomas Philip and Jonsson, Magnus Thor},
file = {:home/sarios/papers/Evolution/Evolution and Design of Distributed Learning Rules.pdf:pdf},
journal = {IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks},
pages = {59--63},
title = {{Evolution and Design of Distributed Learning Rules}},
year = {2000}
}
@article{Schembri2007,
author = {Schembri, Massimiliano and Mirolli, Marco and Baldassarre, Gianluca},
doi = {10.1007/978-3-540-74913-4},
file = {:home/sarios/papers/Evolution/Evolution and Learning in an Intrinsically Motivated Reinforcement Learning Robot.pdf:pdf},
isbn = {978-3-540-74912-7},
issn = {0302-9743},
number = {May 2014},
title = {{Advances in Artificial Life}},
url = {http://link.springer.com/10.1007/978-3-540-74913-4},
volume = {4648},
year = {2007}
}
@article{Salimans2017,
abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
archivePrefix = {arXiv},
arxivId = {1703.03864},
author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
doi = {10.1.1.51.6328},
eprint = {1703.03864},
file = {:home/sarios/papers/Evolution/Evolution Strategies as a Scalable Alternative to Reinforcement Learning.pdf:pdf},
isbn = {3-540-63746-X},
issn = {ISSN 0302-9743},
pmid = {27474269},
title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.03864},
year = {2017}
}
@book{Whiteson2006,
abstract = {Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efficient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the field of autonomic computing. The results demonstrate that evolutionary function approximation can significantly improve the performance of TD methods and on-line evolutionary computation can significantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difficult in practice.},
author = {Whiteson, Shimon and Stone, Peter},
booktitle = {Journal of Machine Learning Research},
file = {:home/sarios/papers/Evolution/Evolutionary Function Approximation for Reinforcement Learning.pdf:pdf},
isbn = {0000000000000},
issn = {15324435},
keywords = {evolutionary computation,neu-,on-line learning,reinforcement learning,roevolution,temporal difference methods},
number = {AI05-320},
pages = {877--917},
title = {{Evolutionary Function Approximation for Reinforcement Learning}},
url = {http://portal.acm.org/citation.cfm?id=1248578},
volume = {7},
year = {2006}
}
@article{Soltoggio2008,
abstract = {Neuromodulation is considered a key factor for learning and memory in biological neural networks. Similarly, artificial neural networks could benefit from modulatory dynamics when facing certain types of learning problem. Here we test this hypothesis by introducing modulatory neurons to enhance or dampen neural plasticity at target neural nodes. Simulated evolution is employed to design neural control networks for T-maze learning problems, using both stan-dard and modulatory neurons. The results show that exper-iments where modulatory neurons are enabled achieve better learning in comparison to those where modulatory neurons are disabled. We conclude that modulatory neurons evolve autonomously in the proposed learning tasks, allowing for increased learning and memory capabilities.},
author = {Soltoggio, Andrea and Bullinaria, John A and Mattiussi, Claudio and D{\"{u}}rr, Peter and Floreano, Dario},
doi = {10.1.1.210.6989},
file = {:home/sarios/papers/Evolution/Evolutionary Advantages of Neuromodulated Plasticity in Dynamic, Reward-based Scenarios.pdf:pdf},
isbn = {9780262750172},
journal = {Artificial Life XI: Proceedings of the 11th International Conference on Simulation and Synthesis of Living Systems (ALIFE 2008)},
pages = {569--576},
title = {{Evolutionary Advantages of Neuromodulated Plasticity in Dynamic, Reward-based Scenarios}},
url = {https://dspace.lboro.ac.uk/2134/17039},
year = {2008}
}
@article{Ray1992,
abstract = {... code for self-replication, interactions such as parasitism, im- munity, hyper-parasitism, sociality and cheating have emerged spontaneously. This paper presents a methodology and some rst results. Apart from its value as a tool for the study or teaching of ecology and evolution ...},
author = {Ray, T S},
file = {:home/sarios/papers/Evolution/Evolution, Ecology and Optimization of Digital Organisms.pdf:pdf},
journal = {Santa Fe},
title = {{Evolution, ecology and optimization of digital organisms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5380{\&}rep=rep1{\&}type=pdf{\%}5Cnpapers2://publication/uuid/9CF4BB7A-510D-44CE-BB80-665C4CDB786D},
year = {1992}
}
@article{Sipper2017,
abstract = {Editorial Artificial intelligence (AI), a broad field that deals with the ongoing pursuit to render machines capable of performing intelligent tasks, has taken the academic and industrial worlds by storm in a breathtakingly short time span. These days, when you engage in the daily surf of your favorite news website, some mention of AI will probably ensue. Machine learning, currently the most prominent subfield of AI, focuses on algorithms that learn from data, with deep learning—employing artificial neural networks with several hidden layers—being the jewel in the crown. From playing Go to processing radiological images, machine learning's success and breadth of scope is undeniable. Yet we mustn't forget that the parent field of AI has birthed many other offspring. In particular, we wish to shine a light on the field of evolutionary computation (EC), which we believe is poised to be " The Next Big Thing " . In EC, core concepts from evolutionary biology—inheritance, random variation, and selection—are harnessed in algorithms that are applied to complex computational prob-lems. The field of EC, whose origins can be traced back to the 1950s and 60s, has come into its own over the past decade. EC techniques have been shown to solve numerous dif-ficult problems from widely diverse domains, in particular producing human-competitive machine intelligence [1]. As argued by the authors of this latter paper, " Surpassing humans in the ability to solve complex problems is a grand challenge, with potentially far-reaching, transformative implications. " EC is applicable over a wide range of problem categories, including classification, regression, clustering, design, optimization, planning, and generating computer pro-grams. Moreover, the range of applications for which EC has worked well is staggering, including such disparate domains as antenna design [2], generating winning game strate-gies [3], automated program improvement [4], and bioinformatics [5]. EC presents many important benefits over popular deep learning methods: • EC relies to a far lesser extent on the existence of a known or discoverable gradient within the search space. • EC thrives when applied to design problems, where the objective is to design new entities from scratch (e.g., antennas [2] and game strategies [3]). • EC algorithms require fewer a priori assumptions regarding the problem being investigated. {\textcopyright} The Author(s). 2017 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http:// creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated. Sipper et al. BioData Mining (2017) 10:26 Page 2 of 3 • However, EC seamlessly lends itself to the integration of human expert knowledge as needed (e.g, [3]). • EC can solve problems with no known solutions, where human expertise is limited or absent altogether [6]. • EC has proven to work well in combination with many other AI techniques, including artificial neural networks [7] and other machine learning algorithms [8]. • EC algorithms are inherently distributed, and are ripe for running in parallel on multi-core or distributed cloud-computing systems [9]. • EC algorithms are anytime algorithms, meaning that they can provide a reasonable solution to a problem even when prematurely interrupted. • The representation of solutions in EC algorithms can be quite flexible, which lends itself to highly interpretable models if interpretable solution representations are used. • EC algorithms require little to no data to solve a problem; they can provide a solution based on any criteria in the fitness function. • Several EC algorithms can beautifully handle multiple objectives [10]. • EC is conceptually simple and easy for non-experts to learn and apply. Fogel [11] discusses additional benefits of EC, while [12] cogently presents EC's advan-tages from an industrial perspective. For these reasons we believe that EC is poised to rise to prominence in the near future, with evolutionary algorithms put to use far more than they are today. This development will come as no surprise to seasoned EC practitioners, who have been aware of the mer-its of evolution for a very long time. After all, since evolution by natural selection has given rise to human intelligence, surely artificial intelligence will greatly benefit from this process.},
author = {Sipper, Moshe and Olson, Randal S. and Moore, Jason H.},
doi = {10.1186/s13040-017-0147-3},
file = {:home/sarios/papers/Evolution/Evolutionary computation- the next major transition of artificial intelligence{\_}.pdf:pdf},
isbn = {17560381},
issn = {17560381},
journal = {BioData Mining},
number = {1},
pages = {1--3},
publisher = {BioData Mining},
title = {{Evolutionary computation: The next major transition of artificial intelligence?}},
volume = {10},
year = {2017}
}
@article{Liang2018,
abstract = {Multitask learning, i.e. learning several tasks at once with the same neural network, can improve performance in each of the tasks. Designing deep neural network architectures for multitask learning is a challenge: There are many ways to tie the tasks together, and the design choices matter. The size and complexity of this problem exceeds human design ability, making it a compelling domain for evolutionary optimization. Using the existing state of the art soft ordering architecture as the starting point, methods for evolving the modules of this architecture and for evolving the overall topology or routing between modules are evaluated in this paper. A synergetic approach of evolving custom routings with evolved, shared modules for each task is found to be very powerful, significantly improving the state of the art in the Omniglot multitask, multialphabet character recognition domain. This result demonstrates how evolution can be instrumental in advancing deep neural network and complex system design in general.},
archivePrefix = {arXiv},
arxivId = {1803.03745},
author = {Liang, Jason and Meyerson, Elliot and Miikkulainen, Risto},
doi = {10.1145/3205455.3205489},
eprint = {1803.03745},
file = {:home/sarios/papers/Evolution/Evolutionary Architecture Search For Deep Multitask Networks.pdf:pdf},
isbn = {9781450356183},
keywords = {artificial intelligence,deep learn-,ing,machine learning,neural networks,parameter tuning,pattern recognition and classification},
title = {{Evolutionary Architecture Search For Deep Multitask Networks}},
url = {http://arxiv.org/abs/1803.03745},
year = {2018}
}
@article{Wu2017a,
abstract = {{\textcopyright} Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Real-Time Strategy (RTS) games involve multiple agents acting simultaneously, and result in enormous state dimensionality. In this paper, we propose an abstracted and simplified model for the famous game StarCraft, and design a dynamic programming algorithm to solve the building order problem, which takes minimal time to achieve a specific target. In addition, Genetic Algorithms (GA) are used to find an optimal target for the opening stage.},
author = {Wu, Lianlong and Markham, Andrew},
file = {:home/sarios/papers/Evolution/Evolutionary Machine Learning for RTS Game StarCraft.pdf:pdf},
journal = {Proceedings of the 31th Conference on Artificial Intelligence (AAAI 2017)},
keywords = {Student Abstracts},
pages = {5007--5008},
title = {{Evolutionary Machine Learning for RTS Game StarCraft}},
year = {2017}
}
@article{Dubey2018,
abstract = {We investigate an evolutionary multi-objective approach to good micro for real-time strategy games. Good micro helps a player win skirmishes and is one of the keys to developing better real-time strategy game play. In prior work, the same multi-objective approach of maximizing damage done while minimizing damage received was used to evolve micro for a group of ranged units versus a group of melee units. We extend this work to consider groups composed from two types of units. Specifically, this paper uses evolutionary multi-objective optimization to generate micro for one group composed from both ranged and melee units versus another group of ranged and melee units. Our micro behavior representation uses influence maps to represent enemy spatial information and potential fields generated from distance, health, and weapons cool down to guide unit movement. Experimental results indicate that our multi-objective approach leads to a Pareto front of diverse high-quality micro encapsulating multiple possible tactics. This range of micro provided by the Pareto front enables a human or AI player to trade-off among short term tactics that better suit the player's longer term strategy - for example, choosing to minimize friendly unit damage at the cost of only lightly damaging the enemy versus maximizing damage to the enemy units at the cost of increased damage to friendly units. We believe that our results indicate the usefulness of potential fields as a representation, and of evolutionary multi-objective optimization as an approach, for generating good micro.},
archivePrefix = {arXiv},
arxivId = {1803.10316},
author = {Dubey, Rahul and Ghantous, Joseph and Louis, Sushil and Liu, Siming},
eprint = {1803.10316},
file = {:home/sarios/papers/Evolution/Evolutionary Multi-objective Optimization of Real-Time Strategy Micro.pdf:pdf},
isbn = {9781538643594},
journal = {Arxiv},
title = {{Evolutionary Multi-objective Optimization of Real-Time Strategy Micro}},
url = {http://arxiv.org/abs/1803.10316},
year = {2018}
}
@article{Lehman2011,
abstract = {An ambitious challenge in artificial life is to craft an evolutionary process that discovers a wide diversity of well-adapted virtual creatures within a single run. Unlike in nature, evolving creatures in virtual worlds tend to converge to a single morphology because selection therein greedily rewards the morphology that is easiest to exploit. However, novelty search, a technique that explicitly rewards diverging, can potentially mitigate such convergence. Thus in this paper an existing creature evolution platform is extended with multi-objective search that balances drives for both novelty and performance. However, there are different ways to combine performance-driven search and novelty search. The suggested approach is to provide evolution with both a novelty objective that encourages diverse morphologies and a local competition objective that rewards individuals outperforming those most similar in morphology. The results in an experiment evolving locomoting virtual creatures show that novelty search with local competition discovers more functional morphological diversity within a single run than models with global competition, which are more predisposed to converge. The conclusions are that novelty search with local competition may complement recent advances in evolving virtual creatures and may in general be a principled approach to combining novelty search with pressure to achieve.},
author = {Lehman, Joel and Stanley, Kenneth O.},
doi = {10.1145/2001576.2001606},
file = {:home/sarios/papers/Evolution/Evolving a Diversity of Creatures through Novelty Search and Local Competition.pdf:pdf},
isbn = {9781450305570},
journal = {Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11},
keywords = {artificial life,natural evolution,novelty,virtual creatures},
number = {Gecco},
pages = {211},
title = {{Evolving a diversity of virtual creatures through novelty search and local competition}},
url = {http://portal.acm.org/citation.cfm?doid=2001576.2001606},
year = {2011}
}
@article{Stanley2003,
abstract = {A potentially powerful application of evolutionary computation (EC) is to evolve neural networks for automated control tasks. However, in such tasks environments can be unpredictable and fixed control policies may fail when conditions suddenly change. Thus, there is a need to evolve neural networks that can adapt, i.e. change their control policy dynamically as conditions change. In this paper, we examine two methods for evolving neural networks with dynamic policies. The first method evolves recurrent neural networks with fixed connection weights, relying on internal state changes to lead to changes in behavior. The second method evolves local rules that govern connection weight changes. The surprising experimental result is that the former method can be more effective than evolving networks with dynamic weights, calling into question the intuitive notion that networks with dynamic synapses are necessary for evolving solutions to adaptive tasks.},
author = {Stanley, K.O. and Bryant, B.D. and Miikkulainen, Risto},
doi = {10.1109/CEC.2003.1299410},
file = {:home/sarios/papers/Evolution/Evolving Adaptive Neural Networks with and without Adaptive Synapses.pdf:pdf},
isbn = {0-7803-7804-0},
issn = {1941-0026},
journal = {The 2003 Congress on Evolutionary Computation, 2003. CEC '03.},
number = {2},
pages = {2557--2564},
title = {{Evolving adaptive neural networks with and without adaptive synapses}},
url = {http://ieeexplore.ieee.org/document/1299410/},
volume = {4},
year = {2003}
}
@article{Paine2004,
abstract = {This study describes how complex goal-directed behavior can evolve in a hierarchically organized recurrent neural network controlling a simulated Khepera robot. Different types of dynamic structures self-organize in the lower and higher levels of a network for the purpose of achieving complex navigation tasks. The parametric bifurcation structures that appear in the lower level ex- plain the mechanism of how behavior primitives are switched in a top-down way. In the higher level, a topologically ordered mapping of initial cell activa- tion states to motor-primitive sequences self-organizes by utilizing the initial sensitivity characteristics of nonlinear dynamical systems. A further experi- ment tests the evolved controller's adaptability to changes in its environment. The biological plausibility of the model's essential principles is discussed.},
author = {Paine, Rainer W and Tani, Jun},
file = {:home/sarios/papers/Evolution/Evolved Motor Primitives and Sequences in a Hierarchical Recurrent Neural Network.pdf:pdf},
issn = {03029743},
journal = {Gecco (1)},
keywords = {CTRNN,Hierarchical control,Neural Networks},
pages = {603--614},
title = {{Evolved Motor Primitives and Sequences in a Hierarchical Recurrent Neural Network.}},
year = {2004}
}
@article{Liu2017,
abstract = {Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic play-testing. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based play-testing is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation Hill-Climber and the Multi-Armed Bandit Random Mutation Hill-Climber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.},
archivePrefix = {arXiv},
arxivId = {1703.06275},
author = {Liu, Jialin and Togelius, Julian and Perez-Liebana, Diego and Lucas, Simon M.},
doi = {10.1109/CEC.2017.7969583},
eprint = {1703.06275},
file = {:home/sarios/papers/Evolution/Evolving Game Skill-Depth using General Video Game AI Agents.pdf:pdf},
isbn = {9781509046010},
journal = {2017 IEEE Congress on Evolutionary Computation, CEC 2017 - Proceedings},
keywords = {Automatic game design,GVG-AI,Game tuning,Optimisation,RMHC},
pages = {2299--2307},
title = {{Evolving Game Skill-Depth using General Video Game AI agents}},
year = {2017}
}
@article{DeCampos2015,
abstract = {The aim of this paper is to present a biologically inspired Neuro Evolutive Algorithm (NEA) able to generate modular, hierarchical and recurrent neural structures as those often found in the nervous system of live beings, and that enable them to solve intricate survival problems. In our approach we consider that a nervous system design and organization is a constructive process carried out by genetic information encoded in DNA. Our NEA evolves Artificial Neural Networks (ANNs) using a Lindenmayer System with memory that implements the principles of organization, modularity, repetition (multiple use of the same sub-structure), hierarchy (recursive composition of sub-structures), as a metaphor for development of neurons and its connections. In our method, this basic neural codification is integrated to a Genetic Algorithm (GA) that implements the constructive approach found in the evolutionary process, making it closest to the biological ones. Our method was initially tested on a simple, but non-trivial, XOR problem. We also submit our method to two other problems of increasing complexity: Prediction of the effect of a new drug on Breast Cancer and KDDCUP'99 benchmark intrusion detection dataset. Some advantages of the proposed methodology are that it increases the level of implicit parallelism of the GA and seems to be capable of generating minimal satisfactory neural architectures, resulting in a reduction of project costs and increasing the performance of the evolved ANN, suggesting a promising potential for future applications.},
author = {{De Campos}, L{\'{i}}dio Mauro Lima and {De Oliveira}, Roberto C{\'{e}}lio Lim{\~{a}}o and Roisenberg, Mauro},
doi = {10.1109/IJCNN.2015.7280535},
file = {:home/sarios/papers/Evolution/Evolving Artificial Neural Networks through L-System and Evolutionary Computation.pdf:pdf},
isbn = {9781479919604},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {Bioinformatics,Biological information theory,Genomics,Legged locomotion,RNA},
title = {{Evolving Artificial Neural Networks through L-system and evolutionary computation}},
volume = {2015-Septe},
year = {2015}
}
@article{DeJong2008,
abstract = {Already in the early 1960s, Larry Fogel and his colleagues were exploring the possibility of creating artificial intelligence using simulated evolution. Over the past 50 years, that idea has captured the imagination of many people and has led to a wide variety of approaches. In this article, this quest is summarized, the current state of the art is described, and some of the remaining open questions are discussed.},
author = {{De Jong}, Kenneth A.},
doi = {10.1109/MCI.2007.913370},
file = {:home/sarios/papers/Evolution/Evolving intelligent agents- A 50 year quest.pdf:pdf},
issn = {1556603X},
journal = {IEEE Computational Intelligence Magazine},
number = {1},
pages = {12--17},
title = {{Evolving Intelligent Agents: A 50 Year Quest}},
volume = {3},
year = {2008}
}
@article{Koutnik2013,
author = {Koutn{\'{i}}k, Jan},
file = {:home/sarios/papers/Evolution/Evolving Large-Scale Neural Networks for Vision-Based Reinforcement Learning.pdf:pdf},
isbn = {9781450319638},
keywords = {indirect encodings,neuroevolution,vision-based torcs},
title = {{Evolving Large-Scale Neural Networks for Vision-Based Reinforcement Learning}},
year = {2013}
}
@article{Kameoka2009,
abstract = {This paper presents a new sparse representation for acoustic signals which is based on a mixing model defined in the complex-spectrum domain (where additivity holds), and allows us to extract recurrent patterns of magnitude spectra that underlie observed complex spectra and the phase estimates of constituent signals. An efficient iterative algorithm is derived, which reduces to the multiplicative update algorithm for non-negative matrix factorization developed by Lee under a particular condition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {Kameoka, Hirokazu and Ono, Nobutaka and Kashino, Kunio and Sagayama, Shigeki},
doi = {10.1109/ICASSP.2009.4960364},
eprint = {1609.03499},
file = {:home/sarios/papers/Evolution/Evolving modular neural sequence architectures with genetic programming.pdf:pdf},
isbn = {9781424423545},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Data-driven approach,Non-negative matrix factorization,Sparse coding,Sparse signal representation},
pages = {2353--2356},
pmid = {4960364},
title = {{Complex NMF: A new sparse representation for acoustic signals}},
year = {2009}
}
@article{Huizinga2018,
abstract = {An important challenge in reinforcement learning, including evolutionary robotics, is to solve multimodal problems, where agents have to act in qualitatively different ways depending on the circumstances. Because multimodal problems are often too difficult to solve directly, it is helpful to take advantage of staging, where a difficult task is divided into simpler subtasks that can serve as stepping stones for solving the overall problem. Unfortunately, choosing an effective ordering for these subtasks is difficult, and a poor ordering can reduce the speed and performance of the learning process. Here, we provide a thorough introduction and investigation of the Combinatorial Multi-Objective Evolutionary Algorithm (CMOEA), which avoids ordering subtasks by allowing all combinations of subtasks to be explored simultaneously. We compare CMOEA against two algorithms that can similarly optimize on multiple subtasks simultaneously: NSGA-II and Lexicase Selection. The algorithms are tested on a multimodal robotics problem with six subtasks as well as a maze navigation problem with a hundred subtasks. On these problems, CMOEA either outperforms or is competitive with the controls. Separately, we show that adding a linear combination over all objectives can improve the ability of NSGA-II to solve these multimodal problems. Lastly, we show that, in contrast to NSGA-II and Lexicase Selection, CMOEA can effectively leverage secondary objectives to achieve state-of-the-art results on the robotics task. In general, our experiments suggest that CMOEA is a promising, state-of-the-art algorithm for solving multimodal problems.},
archivePrefix = {arXiv},
arxivId = {1807.03392},
author = {Huizinga, Joost and Clune, Jeff},
eprint = {1807.03392},
file = {:home/sarios/papers/Evolution/Evolving Multimodal Robot Behavior via Many Stepping Stones with the Combinatorial Multi-Objective Evolutionary Algorithm.pdf:pdf},
pages = {1--21},
title = {{Evolving Multimodal Robot Behavior via Many Stepping Stones with the Combinatorial Multi-Objective Evolutionary Algorithm}},
url = {http://arxiv.org/abs/1807.03392},
year = {2018}
}
@book{Belew1992,
abstract = {It is appealing to consider hybrids of neural network learnning algorithms with evolutionary search procedures simply because nature has successfully done so, suggesting that such hybrids may be more efficient than either technique applied in isolation. We survey recent work in this area and report our own experiments on using the GA to search the space of initial conditions for backpropagation networks. We find that use of the GA provides much greater confidence in the face of the dependence on initial conditions that plague gradient techniques, and allows a reduction of individual training time by as much as two orders of magnitude. We conclude that the GA's {\{}$\backslash$em global sampling{\}} characteristics complement connectionist {\{}$\backslash$em local search{\}} techniques well, leading to efficient and robust hybrids.},
author = {Belew, Richard K. and McInerney, John and Schraudolph, Nicol N.},
booktitle = {Artificial Life II},
file = {:home/sarios/papers/Evolution/Evolving Networks- Using the Genetic Algorithm with Connectionist Learning.pdf:pdf},
isbn = {1819181919181},
pages = {511--547},
title = {{Evolving Networks: Using the Genetic Algorithm with Connectionist Learning}},
url = {http://nic.schraudolph.org/bib2html/b2hd-BelMcISch92.html},
volume = {10},
year = {1992}
}
@article{Fehervari2010,
abstract = {Self-organizing systems obtain a global system behavior via typically simple local interactions among a number of components or agents, respectively. The emergent ser- vice often displays properties like adaptability, robust- ness, and scalability, which makes the self-organizing paradigm interesting for technical applications like coop- erative autonomous robots. The behavior for the local interactions is usually simple, but it is often difficult to de- fine the right set of interaction rules in order to achieve a desired global behavior. In this paper we describe a novel design approach using an evolutionary algorithm and ar- tificial neural networks to automatize the part of the de- sign process that requires most of the effort. A simulated robot soccer game was implemented to test and evaluate the proposed method. A new approach in evolving com- petitive behavior is also introduced using Swiss System instead of the full tournament to cut down the number of necessary simulations.},
author = {Feh{\'{e}}rv{\'{a}}ri, Istv{\'{a}}n and Elmenreich, Wilfried},
doi = {10.1155/2010/841286},
file = {:home/sarios/papers/Evolution/Evolving Neural Network Controllers for a Team of Self-organizing Robots.pdf:pdf},
issn = {1687-9600},
journal = {Journal of Robotics},
keywords = {artificial neural networks,cooperative robots,evolutionary algorithm,self-organizing systems},
number = {OCTOBER},
pages = {1--10},
title = {{Evolving Neural Network Controllers for a Team of Self-Organizing Robots}},
volume = {2010},
year = {2010}
}
@article{Huizinga2014,
abstract = {compares a modified hyperneat to another modified hyperneat so not best experimental methodology},
author = {Huizinga, Joost and Clune, Jeff and Mouret, Jean-Baptiste},
doi = {10.1145/2576768.2598232},
file = {:home/sarios/papers/Evolution/Evolving Neural Networks That Are Both Modular and Regular- HyperNeat Plus the Connection Cost Technique.pdf:pdf},
isbn = {9781450326629},
journal = {Proceedings of the 2014 conference on Genetic and evolutionary computation - GECCO '14},
keywords = {artificial neural networks,hyper-,modularity,regularity},
pages = {697--704},
title = {{Evolving neural networks that are both modular and regular}},
url = {http://dl.acm.org/citation.cfm?doid=2576768.2598232},
year = {2014}
}
@article{Wieland1991,
abstract = {The author describes how genetic algorithms (GAs) were used to$\backslash$ncreate recurrent neural networks to control a series of unstable$\backslash$nsystems. The systems considered are variations of the pole balancing$\backslash$nproblem: network controllers with two, one, and zero inputs, variable$\backslash$nlength pole, multiple poles on one cart, and a jointed pole. GAs were$\backslash$nable to quickly evolve networks for the one- and two-input pole$\backslash$nbalancing problems. Networks with zero inputs were only able to valance$\backslash$npoles for a few seconds of simulated time due to the network's inability$\backslash$nto maintain accurate estimates of their position and pole angle. Also,$\backslash$nwork in progress on a two-legged walker is briefly described},
author = {a.P. Wieland},
doi = {10.1109/IJCNN.1991.155416},
file = {:home/sarios/papers/Evolution/Evolving Neural Network Controllers for Unstable Systems.pdf:pdf},
isbn = {0-7803-0164-1},
journal = {IJCNN-91-Seattle International Joint Conference on Neural Networks},
pages = {667--673},
title = {{Evolving neural network controllers for unstable systems}},
volume = {ii},
year = {1991}
}
@article{Stanley2002,
abstract = {The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
archivePrefix = {arXiv},
arxivId = {1407.0576},
author = {Stanley, Kenneth O. and Miikkulainen, Risto},
doi = {10.1162/106365602320169811},
eprint = {1407.0576},
file = {:home/sarios/papers/Evolution/Evolving Neural Networks through Augmenting Topologies.pdf:pdf},
isbn = {1063-6560},
issn = {10636560},
journal = {Evolutionary Computation},
keywords = {Competing conventions,Genetic algorithms,Network topologies,Neural networks,Neuroevolution,Speciation},
number = {2},
pages = {99--127},
pmid = {12180173},
title = {{Evolving neural networks through augmenting topologies}},
volume = {10},
year = {2002}
}
@article{Greve2016,
abstract = {An unsolved problem in neuroevolution (NE) is to evolve artificial neural networks (ANN) that can store and use in-formation to change their behavior online. While plastic neural networks have shown promise in this context, they have difficulties retaining information over longer periods of time and integrating new information without losing pre-viously acquired skills. Here we build on recent work by Graves et al. [5] who extended the capabilities of an ANN by combining it with an external memory bank trained through gradient descent. In this paper, we introduce an evolvable version of their Neural Turing Machine (NTM) and show that such an approach greatly simplifies the neural model, generalizes better, and does not require accessing the entire memory content at each time-step. The Evolvable Neural Turing Machine (ENTM) is able to solve a simple copy tasks and for the first time, the continuous version of the double T-Maze, a complex reinforcement-like learning problem. In the T-Maze learning task the agent uses the memory bank to display adaptive behavior that normally requires a plas-tic ANN, thereby suggesting a complementary and effective mechanism for adaptive behavior in NE.},
author = {Greve, Rasmus Boll and Jacobsen, Emil Juul and Risi, Sebastian},
doi = {10.1145/2908812.2908930},
file = {:home/sarios/papers/Evolution/Evolving Neural Turing Machines for Reward-based Learning.pdf:pdf},
isbn = {9781450342063},
journal = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference - GECCO '16},
keywords = {adaptive neural networks,learning,machine,memory,neural plasticity,neural turing},
pages = {117--124},
title = {{Evolving Neural Turing Machines for Reward-based Learning}},
url = {http://dl.acm.org/citation.cfm?doid=2908812.2908930},
year = {2016}
}
@article{Reisinger2004,
abstract = {To study the synchronized oscillations among distant neurons in the visual cortex, we analyzed the synchronization between two modules of pulse neural networks using the phase response function. It was found that the intermodule connections from excitatory to excitatory ensembles tend to stabilize the antiphase synchronization and that the intermodule connections from excitatory to inhibitory ensembles tend to stabilize the in-phase synchronization. It was also found that the intermodule synchronization was more noticeable when the inner-module synchronization was weak.},
author = {Reisinger, Joseph and Stanley, Kenneth O. and Miikkulainen, Risto},
doi = {10.1007/978-3-540-24855-2_7},
file = {:home/sarios/papers/Evolution/Evolving Reusable Neural Modules.pdf:pdf},
isbn = {978-3-540-22343-6},
issn = {0899-7667},
pages = {69--81},
title = {{Evolving Reusable Neural Modules}},
url = {http://link.springer.com/10.1007/978-3-540-24855-2{\_}7},
year = {2004}
}
@article{Togelius2006a,
abstract = {Neural network-based controllers arc evolved for racing simulated R/C cars around several tracks of varying difficulty. The transferability of driving skills acquired when evolving for a single track is evaluated, and different ways of evolving controllers able to perform well on many different tracks are investigated, ft is further shown that such generally proficient controllers can reliably be developed into specialized controllers for individual tracks. Evolution of sensor parameters together with network weights is shown to lead to higher final fitness, but only if turned on after a general controller is developed, otherwise it hinders evolution, ft is argued that simulated car racing is a scalable and relevant testbed for evolutionary robotics research, and that the results of this research can be useful for commercial computer games.},
author = {Togelius, Julian and Lucas, S.M.},
doi = {10.1109/CEC.2006.1688444},
file = {:home/sarios/papers/Evolution/Evolving robust and specialized car racing skills.pdf:pdf},
isbn = {0-7803-9487-9},
journal = {2006 IEEE International Conference on Evolutionary Computation},
keywords = {Evolutionary robotics,car racing,driving,games,incremental evolution},
pages = {1187--1194},
title = {{Evolving robust and specialized car racing skills}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1688444},
year = {2006}
}
@article{Wilson2018,
abstract = {Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but effective strategies can be found.},
archivePrefix = {arXiv},
arxivId = {1806.05695},
author = {Wilson, Dennis G and Cussat-Blanc, Sylvain and Luga, Herv{\'{e}} and Miller, Julian F},
doi = {10.1145/3205455.3205578},
eprint = {1806.05695},
file = {:home/sarios/papers/Evolution/Evolving simple programs for playing Atari games.pdf:pdf},
isbn = {9781450356183},
keywords = {arti cial intelli-,games,genetic programming,image analysis},
title = {{Evolving simple programs for playing Atari games}},
url = {http://arxiv.org/abs/1806.05695},
year = {2018}
}
@article{Sims1994,
abstract = {This paper describes a novel system for creating virtual creatures$\backslash$r$\backslash$nthat  move  and  behave  in  simulated  three-dimensional  physical$\backslash$r$\backslash$nworlds. The morphologies of creatures and the neural systems for$\backslash$r$\backslash$ncontrolling  their  muscle  forces  are  both  generated  automatically$\backslash$r$\backslash$nusing genetic algorithms. Different fitness evaluation functions are$\backslash$r$\backslash$nused  to  direct  simulated  evolutions  towards  specific  behaviors$\backslash$r$\backslash$nsuch as swimming, walking, jumping, and following.$\backslash$r$\backslash$nA  genetic  language  is  presented  that  uses  nodes  and  connec-$\backslash$r$\backslash$ntions as its primitive elements to represent directed graphs, which$\backslash$r$\backslash$nare used to describe both the morphology and the neural circuitry$\backslash$r$\backslash$nof these creatures. This genetic language defines a hyperspace con-$\backslash$r$\backslash$ntaining an indefinite number of possible creatures with behaviors,$\backslash$r$\backslash$nand when it is searched using optimization techniques, a variety of$\backslash$r$\backslash$nsuccessful  and  interesting  locomotion  strategies  emerge,  some  of$\backslash$r$\backslash$nwhich would be difficult to invent or build by design.},
author = {Sims, Karl},
doi = {10.1145/192161.192167},
file = {:home/sarios/papers/Evolution/Evolving Virtual Creatures.pdf:pdf},
isbn = {0897916670},
issn = {10645462},
journal = {Proceedings of the 21st annual conference on Computer graphics and interactive techniques  - SIGGRAPH '94},
number = {July},
pages = {15--22},
pmid = {17355189},
title = {{Evolving virtual creatures}},
url = {http://portal.acm.org/citation.cfm?doid=192161.192167},
year = {1994}
}
@article{Rawal2018,
abstract = {Gated recurrent networks such as those composed of Long Short-Term Memory (LSTM) nodes have recently been used to improve state of the art in many sequential processing tasks such as speech recognition and machine translation. However, the basic structure of the LSTM node is essentially the same as when it was first conceived 25 years ago. Recently, evolutionary and reinforcement learning mechanisms have been employed to create new variations of this structure. This paper proposes a new method, evolution of a tree-based encoding of the gated memory nodes, and shows that it makes it possible to explore new variations more effectively than other methods. The method discovers nodes with multiple recurrent paths and multiple memory cells, which lead to significant improvement in the standard language modeling benchmark task. The paper also shows how the search process can be speeded up by training an LSTM network to estimate performance of candidate structures, and by encouraging exploration of novel solutions. Thus, evolutionary design of complex neural network structures promises to improve performance of deep learning architectures beyond human ability to do so.},
archivePrefix = {arXiv},
arxivId = {1803.04439},
author = {Rawal, Aditya and Miikkulainen, Risto},
eprint = {1803.04439},
file = {:home/sarios/papers/Evolution/From Nodes to Networks- Evolving Recurrent Neural Networks.pdf:pdf},
title = {{From Nodes to Networks: Evolving Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1803.04439},
year = {2018}
}
@article{Wierstra2008,
abstract = {We present Fitness Expectation Maximization (FEM), a novel method for performing ‘black box ' function optimization. FEM searches the fitness landscape of an objective function using an instantiation of the well-known Expectation Maximization algorithm, producing search points to match the sample distribution weighted according to higher expected fitness. FEM updates both candidate solution parameters and the search policy, which is represented as a multinormal distribution. Inheriting EM's stability and strong guarantees, the method is both elegant and competitive with some of the best heuristic search methods in the field, and performs well on a number of unimodal and multimodal benchmark tasks. To illustrate the potential practical applications of the approach, we also show experiments on finding the parameters for a controller of the challenging non-Markovian double pole balancing task.},
author = {Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, J{\"{u}}rgen},
doi = {10.1007/978-3-540-87700-4_34},
file = {:home/sarios/papers/Evolution/Fitness Expectation Maximization.pdf:pdf},
isbn = {3540876995},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {337--346},
title = {{Fitness expectation maximization}},
volume = {5199 LNCS},
year = {2008}
}
@article{Lehman2018,
abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
archivePrefix = {arXiv},
arxivId = {1803.03453},
author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fr{\'{e}}noy, Antoine and Gagn{\'{e}}, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, Fran{\c{c}}ois and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
eprint = {1803.03453},
file = {:home/sarios/papers/Evolution/The Surprising Creativity of Digital Evolution- A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities.pdf:pdf},
title = {{The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities}},
url = {http://arxiv.org/abs/1803.03453},
year = {2018}
}
@article{Churchill2013a,
author = {Churchill, Alexander W. and Husbands, Phil and Philippides, Andrew},
doi = {10.1109/CEC.2013.6557925},
file = {:home/sarios/papers/Evolution/Tool Sequence Optimization using Synchronous and Asynchronous Parallel Multi-Objective Evolutionary Algorithms with Heterogeneous Evaluations.pdf:pdf},
isbn = {9781479904549},
journal = {2013 IEEE Congress on Evolutionary Computation, CEC 2013},
pages = {2924--2931},
title = {{Tool sequence optimization using synchronous and asynchronous parallel multi-objective evolutionary algorithms with heterogeneous evaluations}},
year = {2013}
}
@article{Preuss2010,
abstract = {Players of real-time strategy (RTS) games are often annoyed by the inability of the game AI to select and move teams of units in a natural way. Units travel and battle separately, resulting in huge losses and the AI looking unintelligent, as can the choice of units sent to counteract the opponents. Players are affected as well as computer commanded factions because they cannot micromanage all team related issues. We suggest improving AI behavior by combining well-known computational intelligence techniques applied in an original way. Team composition for battling spatially distributed opponent groups is supported by a learning self-organizing map (SOM) that relies on an evolutionary algorithm (EA) to adapt it to the game. Different abilities of unit types are thus employed in a near-optimal way, reminiscent of human ad hoc decisions. Team movement is greatly enhanced by flocking and influence map-based path finding, leading to a more natural behavior by preserving individual motion types. The team decision to either attack or avoid a group of enemy units is easily parametrizable, incorporating team characteristics from fearful to daredevil. We demonstrate that these two approaches work well separately, but also that they go together naturally, thereby leading to an improved and flexible group behavior.},
author = {Preuss, Mike and Beume, Nicola and Danielsiek, Holger and Hein, Tobias and Naujoks, Boris and Piatkowski, Nico and St{\"{u}}er, Raphael and Thom, Andreas and Wessing, Simon},
doi = {10.1109/TCIAIG.2010.2047645},
file = {:home/sarios/papers/Evolution/Towards Intelligent Team Composition and Maneuvering in Real-Time Strategy Games.pdf:pdf},
isbn = {1943-068X},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Evolutionary algorithms (EAs),flocking,influence maps,neural networks,path finding,real-time strategy games,tactical decision making},
number = {2},
pages = {82--98},
title = {{Towards intelligent team composition and maneuvering in real-time strategy games}},
volume = {2},
year = {2010}
}
@article{Cliff1995,
abstract = {Co-evolution can give rise to the Red Queen effect, where interacting populations alter each other's fitness landscapes. The Red Queen effect significantly complicates any measurement of co-evolutionary progress, introducing fitness ambiguities where improvements in performance of co-evolved individuals can appear as a decline or stasis in the usual measures of evolutionary progress. Unfortunately, no appropriate measures of fitness given the Red Queen effect have been developed in artificial life, theoretical biology, population dynamics, or evolutionary genetics. We propose a set of appropriate performance measures based on both genetic and behavioral data, and illustrate their use in a simulation of co-evolution between genetically specified continuous-time noisy recurrent neural networks which generate pursuit and evasion behaviors in autonomous agents.},
author = {Cliff, Dave and Miller, Geoffrey F.},
doi = {10.1007/3-540-59496-5_300},
file = {:home/sarios/papers/Evolution/Tracking the Red Queen- Measurements of Adaptive Progress in Co-Evolutionary Simulations.pdf:pdf},
isbn = {3540594965},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {200--218},
title = {{Tracking the red queen: Measurements of adaptive progress in co-evolutionary simulations}},
volume = {929},
year = {1995}
}
@article{Xie2013,
abstract = {Selection pressure controls the selection of$\backslash$nindividuals from the current population to produce a$\backslash$nnew population in the next generation. It gives$\backslash$nindividuals of higher quality a higher probability of$\backslash$nbeing used to create the next generation so that$\backslash$nEvolutionary Algorithms (EAs) can focus on promising$\backslash$nregions in the search space. An evolutionary learning$\backslash$nprocess is dynamic and requires different selection$\backslash$npressures at different learning stages in order to$\backslash$nspeed up convergence or avoid local optima. Therefore,$\backslash$nit desires selection mechanisms being able to$\backslash$nautomatically tune selection pressure during evolution.$\backslash$nTournament selection is a popular selection method in$\backslash$nEAs. This paper focuses on tournament selection and$\backslash$nshows that standard tournament selection is unaware of$\backslash$nthe dynamics in the evolutionary process thus is unable$\backslash$nto tune selection pressure automatically. This paper$\backslash$nthen presents a novel approach which integrates the$\backslash$nknowledge of the Fitness Rank Distribution (FRD) of a$\backslash$npopulation into tournament selection. Through$\backslash$nmathematical modelling, simulations and experimental$\backslash$nstudy, this paper shows that the new approach is$\backslash$neffective and using the knowledge of FRD is a promising$\backslash$nway to modify the standard tournament selection method$\backslash$nfor tuning the selection pressure dynamically and$\backslash$nautomatically along evolution.},
author = {Xie, Huayang and Zhang, Mengjie},
file = {:home/sarios/papers/Evolution/Tuning Selection Pressure in Tournament Selection.pdf:pdf},
journal = {IEEE Transactions on Evolutionary Computation},
number = {1},
pages = {1--19},
title = {{Tuning Selection Pressure in Tournament Selection}},
url = {https://ecs.victoria.ac.nz/foswiki/pub/Main/TechnicalReportSeries/ECSTR09-10.pdf},
volume = {17},
year = {2013}
}
@article{Scott2015,
abstract = {In many applications of evolutionary algorithms, the time required to evaluate the fitness of individuals is long and variable. When the variance in individual evaluation times is non-negligible, traditional, synchronous master-slave EAs incur idle time in CPU resources. An asynchronous ap-proach to parallelization of EAs promises to eliminate idle time and thereby to reduce the amount of wall-clock time it takes to solve a problem. However, the behavior of asyn-chronous evolutionary algorithms is not well understood. In particular, it is not clear exactly how much faster the asyn-chronous algorithm will tend to run, or whether its evolu-tionary trajectory may follow a sub-optimal search path that cancels out the promised benefits. This paper presents a pre-liminary analysis of simple asynchronous EA performance in terms of speed and problem-solving ability.},
author = {Scott, Eric O. and {De Jong}, Kenneth A.},
doi = {10.1145/2725494.2725509},
file = {:home/sarios/papers/Evolution/Understanding Simple Asynchronous Evolutionary Algorithms.pdf:pdf},
isbn = {9781450334341},
journal = {Proceedings of the 2015 ACM Conference on Foundations of Genetic Algorithms XIII - FOGA '15},
number = {July},
pages = {85--98},
title = {{Understanding Simple Asynchronous Evolutionary Algorithms}},
url = {http://dl.acm.org/citation.cfm?doid=2725494.2725509},
year = {2015}
}
@article{Bengio1994,
author = {Bengio, S. and Bengio, Y. and Cloutier, J.},
doi = {10.1109/ICEC.1994.349932},
file = {:home/sarios/papers/Evolution/Use of Genetic Programming for the Search of a New Learning Rule for Neural Networks.pdf:pdf},
isbn = {0-7803-1899-4},
journal = {Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence},
pages = {324--327},
title = {{Use of genetic programming for the search of a new learning rule for neural networks}},
url = {http://ieeexplore.ieee.org/document/349932/},
year = {1994}
}
@article{Vassiliades2018,
abstract = {The recently introduced Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) is an evolutionary algorithm capable of producing a large archive of diverse, high-performing solutions in a single run. It works by discretizing a continuous feature space into unique regions according to the desired discretization per dimension. While simple, this algorithm has a main drawback: it cannot scale to high-dimensional feature spaces since the number of regions increase exponentially with the number of dimensions. In this paper, we address this limitation by introducing a simple extension of MAP-Elites that has a constant, pre-defined number of regions irrespective of the dimensionality of the feature space. Our main insight is that methods from computational geometry could partition a high-dimensional space into well-spread geometric regions. In particular, our algorithm uses a centroidal Voronoi tessellation (CVT) to divide the feature space into a desired number of regions; it then places every generated individual in its closest region, replacing a less fit one if the region is already occupied. We demonstrate the effectiveness of the new "CVT-MAP-Elites" algorithm in high-dimensional feature spaces through comparisons against MAP-Elites in maze navigation and hexapod locomotion tasks.},
archivePrefix = {arXiv},
arxivId = {1610.05729},
author = {Vassiliades, Vassilis and Chatzilygeroudis, Konstantinos and Mouret, Jean Baptiste},
doi = {10.1109/TEVC.2017.2735550},
eprint = {1610.05729},
file = {:home/sarios/papers/Evolution/Using Centroidal Voronoi Tessellations to Scale Up the Multi-dimensional Archive of Phenotypic Elites Algorithm.pdf:pdf},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Behavioral diversity,centroidal Voronoi tessellation (CVT),illumination algorithms,multidimensional archive of phenotypic elites (MAP,quality diversity (QD)},
number = {4},
pages = {623--630},
title = {{Using Centroidal Voronoi Tessellations to Scale Up the Multidimensional Archive of Phenotypic Elites Algorithm}},
volume = {22},
year = {2018}
}
@article{Miconi2003,
author = {Miconi, Thomas},
file = {:home/sarios/papers/Evolution/When Evolving Populations is Better than Coevolving Individuals- The Blind Mice Problem.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {647--652},
title = {{When evolving populations is better than coevolving individuals: The blind mice problem}},
year = {2003}
}
@article{Cuccu2011a,
abstract = {The idea of evolving novel rather than fit solutions has re-cently been offered as a way to automatically discover the kind of complex solutions that exhibit truly intelligent behavior. So far, novelty search has only been studied in the context of problems where the number of possi-ble " different " solutions has been limited. In this paper, we show, using a task with a much larger solution space, that selecting for novelty alone does not offer an advantage over fitness-based selection. In addition, we examine how the idea of novelty search can be used to sustain diversity and improve the performance of standard, fitness-based search.},
author = {Cuccu, Giuseppe and Gomez, Faustino},
doi = {10.1007/978-3-642-20525-5_24},
file = {:home/sarios/papers/Evolution/When Novelty is Not Enough.pdf:pdf},
isbn = {9783642205248},
issn = {03029743},
journal = {Applications of Evolutionary Computation},
keywords = {novelty-fitness aggregation},
pages = {234--243},
title = {{When novelty is not enough}},
url = {http://www.idsia.ch/{\%}7B{~}giuse,{~}tino{\%}7D{\%}5Cnhttp://link.springer.com/chapter/10.1007/978-3-642-20525-5{\_}24},
year = {2011}
}
@article{Jablonka1998,
abstract = {Since the Modern Synthesis, evolutionary biologists have assumed that the genetic system is the sole provider of heritable variation, and that the generation of heritable variation is largely independent of environmental changes. However, adaptive mutation, epigenetic inheritance, behavioural inheritance through social learning, and language-based information transmission have properties that allow the inheritance of induced or learnt characters. The role of induced heritable variation in evolution therefore needs to be reconsidered, and the evolution of the systems that produce induced variation needs to be studied.},
author = {Jablonka, E and Lamb, M J and Avital, E},
doi = {10.1016/S0169-5347(98)01344-5},
file = {:home/sarios/papers/Evolution/‘Lamarckian' mechanisms in darwinian evolution.pdf:pdf},
isbn = {0169-5347},
issn = {0169-5347},
journal = {Trends in ecology {\&} evolution},
number = {5},
pages = {206--10},
pmid = {21238269},
title = {{'Lamarckian' mechanisms in darwinian evolution.}},
url = {http://www.sciencedirect.com/science/article/pii/S0169534798013445},
volume = {13},
year = {1998}
}
@article{Sygnowski2016,
abstract = {We train a number of neural networks to play games Bowling, Breakout and Seaquest using information stored in the memory of a video game console Atari 2600. We consider four models of neural networks which differ in size and architecture: two networks which use only information contained in the RAM and two mixed networks which use both information in the RAM and information from the screen. As the benchmark we used the convolutional model proposed in NIPS and received comparable results in all considered games. Quite surprisingly, in the case of Seaquest we were able to train RAM-only agents which behave better than the benchmark screen-only agent. Mixing screen and RAM did not lead to an improved performance comparing to screen-only and RAM-only agents.},
archivePrefix = {arXiv},
arxivId = {1605.01335},
author = {Sygnowski, Jakub and Michalewski, Henryk},
doi = {10.1007/978-3-319-57969-6_6},
eprint = {1605.01335},
file = {:home/sarios/papers/learning-from-memory-atari-2600.pdf:pdf},
isbn = {9783319579689},
issn = {18650929},
journal = {Communications in Computer and Information Science},
pages = {71--85},
title = {{Learning from the memory of Atari 2600}},
volume = {705},
year = {2017}
}
@article{Leibo2018,
abstract = {Psychlab is a simulated psychology laboratory inside the first-person 3D game world of DeepMind Lab (Beattie et al. 2016). Psychlab enables implementations of classical laboratory psychological experiments so that they work with both human and artificial agents. Psychlab has a simple and flexible API that enables users to easily create their own tasks. As examples, we are releasing Psychlab implementations of several classical experimental paradigms including visual search, change detection, random dot motion discrimination, and multiple object tracking. We also contribute a study of the visual psychophysics of a specific state-of-the-art deep reinforcement learning agent: UNREAL (Jaderberg et al. 2016). This study leads to the surprising conclusion that UNREAL learns more quickly about larger target stimuli than it does about smaller stimuli. In turn, this insight motivates a specific improvement in the form of a simple model of foveal vision that turns out to significantly boost UNREAL's performance, both on Psychlab tasks, and on standard DeepMind Lab tasks. By open-sourcing Psychlab we hope to facilitate a range of future such studies that simultaneously advance deep reinforcement learning and improve its links with cognitive science.},
archivePrefix = {arXiv},
arxivId = {1801.08116},
author = {Leibo, Joel Z. and D'Autume, Cyprien de Masson and Zoran, Daniel and Amos, David and Beattie, Charles and Anderson, Keith and Casta{\~{n}}eda, Antonio Garc{\'{i}}a and Sanchez, Manuel and Green, Simon and Gruslys, Audrunas and Legg, Shane and Hassabis, Demis and Botvinick, Matthew M.},
eprint = {1801.08116},
file = {:home/sarios/papers/psychlab.pdf:pdf},
pages = {1--28},
title = {{Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents}},
url = {http://arxiv.org/abs/1801.08116},
year = {2018}
}
@article{Ha2018,
abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
archivePrefix = {arXiv},
arxivId = {1803.10122},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
doi = {10.5281/zenodo.1207631},
eprint = {1803.10122},
file = {:home/sarios/papers/world-models.pdf:pdf},
title = {{World Models}},
url = {http://arxiv.org/abs/1803.10122{\%}0Ahttp://dx.doi.org/10.5281/zenodo.1207631},
year = {2018}
}
@article{VanDerRee2013,
abstract = {This paper compares three strategies in using reinforcement learning algorithms to let an artificial agent learn to play the game of Othello. The three strategies that are compared are: Learning by self-play, learning from playing against a fixed opponent, and learning from playing against a fixed opponent while learning from the opponent's moves as well. These issues are considered for the algorithms Q-learning, Sarsa and TD-learning. These three reinforcement learning algorithms are combined with multi-layer perceptrons and trained and tested against three fixed opponents. It is found that the best strategy of learning differs per algorithm. Q-learning and Sarsa perform best when trained against the fixed opponent they are also tested against, whereas TD-learning performs best when trained through self-play. Surprisingly, Q-learning and Sarsa outperform TD-learning against the stronger fixed opponents, when all methods use their best strategy. Learning from the opponent's moves as well leads to worse results compared to learning only from the learning agent's own moves.},
author = {{Van Der Ree}, Michiel and Wiering, Marco},
doi = {10.1109/ADPRL.2013.6614996},
file = {:home/sarios/papers/othello-self-play.pdf:pdf},
isbn = {9781467359252},
issn = {23251824},
journal = {IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, ADPRL},
keywords = {self-play},
mendeley-tags = {self-play},
pages = {108--115},
title = {{Reinforcement learning in the game of Othello: Learning against a fixed opponent and learning from self-play}},
year = {2013}
}
@article{TDGammon,
author = {Tesauro, Gerald},
journal = {Commun. ACM},
pages = {58--68},
title = {{Temporal Difference Learning and TD-Gammon.}},
volume = {38},
year = {1995}
}
@article{Silver2017a,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
eprint = {1610.00633},
file = {:home/sarios/papers/alphago.pdf:pdf},
isbn = {3013372370},
issn = {14764687},
journal = {Nature},
number = {7676},
pages = {354--359},
pmid = {29052630},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://dx.doi.org/10.1038/nature24270},
volume = {550},
year = {2017}
}
@article{Silver2017,
author = {{SILVER D, SCHRITTWIESER K A I H A G A H T B L L M B A, J.AND SIMONYAN}, CHEN Y.},
file = {:home/sarios/papers/alphago-zero.pdf:pdf},
journal = {Nature},
number = {550(7676)},
pages = {354.},
title = {{Mastering the game of go without human knowledge}},
year = {2017}
}
@article{Anthony2017,
abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.},
archivePrefix = {arXiv},
arxivId = {1705.08439},
author = {Anthony, Thomas and Tian, Zheng and Barber, David},
eprint = {1705.08439},
file = {:home/sarios/papers/expert-iteration.pdf:pdf},
isbn = {10495258 (ISSN)},
issn = {10495258},
number = {Il},
pages = {1--19},
title = {{Thinking Fast and Slow with Deep Learning and Tree Search}},
url = {http://arxiv.org/abs/1705.08439},
year = {2017}
}
@article{Risi2015,
abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
archivePrefix = {arXiv},
arxivId = {1410.7326},
author = {Risi, Sebastian and Togelius, Julian},
doi = {10.1109/TCIAIG.2015.2494596},
eprint = {1410.7326},
file = {:home/sarios/papers/Evolution/Neuroevolution in Games- State of the Art and Open Challenges.pdf:pdf},
isbn = {1943-068X VO  - PP},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
number = {99},
pages = {1--19},
title = {{Neuroevolution in games: State of the art and open challenges}},
volume = {PP},
year = {2015}
}
@article{Wagner,
author = {Wagner, Andreas},
doi = {10.1038/nrg2473},
file = {:home/sarios/papers/Evolution/Neutralism and selectionism- a network-based reconciliation.pdf:pdf},
isbn = {1471-0064 (Electronic)$\backslash$r1471-0056 (Linking)},
issn = {14710056},
pmid = {18957969},
title = {{Neutralism and selectionism: a network-based reconciliation}}
}
@article{Wessing2013,
abstract = {In this paper we investigate the ability of selection methods to enforce niching on multi modal problems. Using theoretical properties where possible, and relying on a sound experimental analysis, we show that the conventional single-objective optimization and novelty search are extreme cases of selection, striving only for quality or diversity. However, in between these well known cases, there are many more possibilities, of which we review eight (including the aforementioned two). Multiobjective selection approaches provide a well-balanced trade-off' between exploration and exploitation. For the multiobjectivization, we recommend to use nearest-better-neighbor information instead of the common nearest-neighbor approaches.},
author = {Wessing, Simon and Preuss, Mike and Rudolph, Gunter},
doi = {10.1109/CEC.2013.6557559},
file = {:home/sarios/papers/Evolution/Niching by multiobjectivization with neighbor information- Trade-offs and benefits.pdf:pdf},
isbn = {9781479904549},
journal = {2013 IEEE Congress on Evolutionary Computation, CEC 2013},
number = {April 2014},
pages = {103--110},
title = {{Niching by multiobjectivization with neighbor information: Trade-offs and benefits}},
year = {2013}
}
@article{Kimbrough2008,
author = {Kimbrough, Steven Orla and Lu, Ming and Koehler, Gary J and Wood, David Harlan},
file = {:home/sarios/papers/Evolution/On a Feasible{\_}Infeasible Two-Population (FI-2Pop) Genetic Algorithm for Constrained Optimization- Distance Tracing and no Free Lunch.pdf:pdf},
journal = {European Journal of Operational Research (2008)},
number = {2},
pages = {310--327},
title = {{Introducing a Feasible-Infeasible Two- Population ( FI-2Pop ) Genetic Algorithm for Constrained Optimization : Distance Tracing and No Free Lunch}},
volume = {190},
year = {2008}
}
@article{Bengio1992,
abstract = {This paper presents a new approach to neural modeling based on the idea of using an automated method to optimize the parameters of a synaptic learning rule. The synaptic modification rule is considered as a parametric function. This function has local inputs and is the same in many neurons. We can use standard optimization methods to select appropriate parameters for a given type of task. We also present a theoretical analysis permitting to study the generalization property of such parametric learning rules. By generalization, we mean the possibility for the learning rule to learn to solve new tasks. Experiments were performed on three types of problems: a biologically inspired circuit (for conditioning in Aplysia), Boolean functions (linearly separable as well as non linearly separable) and classification tasks. The neural network architecture as well as the form and initial parameter values of the synaptic learning function can be designed using a priori knowledge.},
author = {Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gecsei, Jan},
file = {:home/sarios/papers/Evolution/On the Optimization of a Synaptic Learning Rule.pdf:pdf},
journal = {Optimality in Artificial and Biological Neural Networks},
pages = {1--29},
title = {{On the Optimization of a Synaptic Learning Rule}},
url = {https://www.researchgate.net/profile/Y{\_}Bengio/publication/2389122{\_}On{\_}the{\_}Optimization{\_}of{\_}a{\_}Synaptic{\_}Learning{\_}Rule/links/546cd2710cf2a7492c55ab40.pdf{\%}0Ahttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.6969},
year = {1992}
}
@article{Lehman2017a,
author = {Lehman, Joel and Stanley, Kenneth O},
file = {:home/sarios/papers/Evolution/On the Potential Benefits of Knowing Everything.pdf:pdf},
title = {{On the Potential Benefits of Knowing Everything}},
year = {2017}
}
@article{Zhang2017,
abstract = {Because stochastic gradient descent (SGD) has shown promise optimizing neural networks with millions of parameters and few if any alternatives are known to exist, it has moved to the heart of leading approaches to reinforcement learning (RL). For that reason, the recent result from OpenAI showing that a particular kind of evolution strategy (ES) can rival the performance of SGD-based deep RL methods with large neural networks provoked surprise. This result is difficult to interpret in part because of the lingering ambiguity on how ES actually relates to SGD. The aim of this paper is to significantly reduce this ambiguity through a series of MNIST-based experiments designed to uncover their relationship. As a simple supervised problem without domain noise (unlike in most RL), MNIST makes it possible (1) to measure the correlation between gradients computed by ES and SGD and (2) then to develop an SGD-based proxy that accurately predicts the performance of different ES population sizes. These innovations give a new level of insight into the real capabilities of ES, and lead also to some unconventional means for applying ES to supervised problems that shed further light on its differences from SGD. Incorporating these lessons, the paper concludes by demonstrating that ES can achieve 99{\%} accuracy on MNIST, a number higher than any previously published result for any evolutionary method. While not by any means suggesting that ES should substitute for SGD in supervised learning, the suite of experiments herein enables more informed decisions on the application of ES within RL and other paradigms.},
archivePrefix = {arXiv},
arxivId = {1712.06564},
author = {Zhang, Xingwen and Clune, Jeff and Stanley, Kenneth O.},
eprint = {1712.06564},
file = {:home/sarios/papers/Evolution/On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent.pdf:pdf},
title = {{On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent}},
url = {http://arxiv.org/abs/1712.06564},
year = {2017}
}
@article{Fernando2017,
abstract = {For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).},
archivePrefix = {arXiv},
arxivId = {1701.08734},
author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
eprint = {1701.08734},
file = {:home/sarios/papers/Evolution/PathNet- Evolution Channels Gradient Descent in Super Neural Networks.pdf:pdf},
issn = {1701.08734},
keywords = {basal ganglia,continual learning,evolution and,giant networks,learning,multitask,path evolution algorithm,transfer learning},
title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
url = {http://arxiv.org/abs/1701.08734},
year = {2017}
}
@article{Cuccu2018,
abstract = {Deep reinforcement learning on Atari games maps pixel directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. Aiming at devoting entire deep networks to decision making alone, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by a novel algorithm based on Vector Quantization and Sparse Coding, trained online along with the network, and capable of growing its dictionary size over time. We also introduce new techniques allowing both the neural network and the evolution strategy to cope with varying dimensions. This enables networks of only 6 to 18 neurons to learn to play a selection of Atari games with performance comparable---and occasionally superior---to state-of-the-art techniques using evolution strategies on deep networks two orders of magnitude larger.},
archivePrefix = {arXiv},
arxivId = {1806.01363},
author = {Cuccu, Giuseppe and Togelius, Julian and Cudre-Mauroux, Philippe},
eprint = {1806.01363},
file = {:home/sarios/papers/Evolution/Playing Atari with Six Neurons.pdf:pdf},
title = {{Playing Atari with Six Neurons}},
url = {http://arxiv.org/abs/1806.01363},
year = {2018}
}
@article{Gangwani2017a,
abstract = {Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency.},
archivePrefix = {arXiv},
arxivId = {1711.01012},
author = {Gangwani, Tanmay and Peng, Jian},
eprint = {1711.01012},
file = {:home/sarios/papers/Evolution/Policy Optimization by Genetic Distillation.pdf:pdf},
title = {{Policy Optimization by Genetic Distillation}},
url = {http://arxiv.org/abs/1711.01012},
year = {2017}
}
@article{Phqjvkrho,
author = {Phqjvkrho, Roh M},
file = {:home/sarios/papers/Evolution/Probabilistic Crowding- Deterministic Crowding with Probabilistic Replacement.pdf:pdf},
title = {{{{hLM @ M * t | U hL {\_} ?{\}} G {\#} i | ih4 ? t | U hL {\_} ?{\}} | hLM @ M * t | U}}
}
@article{Pugh2016,
abstract = {While evolutionary computation and evolutionary robotics take inspiration from nature, they have long focused mainly on problems of performance optimization. Yet evolution in nature can be interpreted as more nuanced than a process of simple optimization. In particular, natural evolution is a divergent search that optimizes locally within each niche as it simultaneously diversifies. This tendency to discover both quality and diversity at the same time differs from many of the conventional algorithms of machine learning, and also thereby suggests a different foundation for inferring the approach of greatest potential for evolutionary algorithms. In fact, several recent evolutionary algorithms called quality diversity (QD) algorithms(e.g. novelty search with local competition and MAP-Elites) have drawn inspiration from this more nuanced view, aiming to fill a space of possibilities with the best possible example of each type of achievable behavior. The result is a new class of algorithms that return an archive of diverse, high-quality behaviors in a single run. The aim in this paper is to study the application of QD algorithms in challenging environments (in particular complex mazes) to establish their best practices for ambitious domains in the future. In addition to providing insight into cases when QD succeeds and fails, a new approach is investigated that hybridizes multiple views of behaviors (called behavior characterizations) in the same run, which succeeds in overcoming some of the challenges associated with searching for QD with respect to a behavior characterization that is not necessarily sufficient for generating both quality and diversity at the same time.},
author = {Pugh, Justin K. and Soros, Lisa B. and Stanley, Kenneth O.},
doi = {10.3389/frobt.2016.00040},
file = {:home/sarios/papers/Evolution/Quality Diversity- A New Frontier for Evolutionary Computation.pdf:pdf},
issn = {2296-9144},
journal = {Frontiers in Robotics and AI},
keywords = {behavioral diversity,evolutionary computation,non-objective search,novelty search,novelty search, non-objective search, quality dive,quality diversity},
number = {July},
pages = {1--17},
title = {{Quality Diversity: A New Frontier for Evolutionary Computation}},
url = {http://journal.frontiersin.org/Article/10.3389/frobt.2016.00040/abstract},
volume = {3},
year = {2016}
}
@article{Cully2018a,
abstract = {The optimization of functions to find the best solution according to one or several objectives has a central role in many engineering and research fields. Recently, a new family of optimization algorithms, named Quality-Diversity optimization, has been introduced, and contrasts with classic algorithms. Instead of searching for a single solution, Quality-Diversity algorithms are searching for a large collection of both diverse and high-performing solutions. The role of this collection is to cover the range of possible solution types as much as possible, and to contain the best solution for each type. The contribution of this paper is threefold. Firstly, we present a unifying framework of Quality-Diversity optimization algorithms that covers the two main algorithms of this family (Multi-dimensional Archive of Phenotypic Elites and the Novelty Search with Local Competition), and that highlights the large variety of variants that can be investigated within this family. Secondly, we propose algorithms with a new selection mechanism for Quality-Diversity algorithms that outperforms all the algorithms tested in this paper. Lastly, we present a new collection management that overcomes the erosion issues observed when using unstructured collections. These three contributions are supported by extensive experimental comparisons of Quality-Diversity algorithms on three different experimental scenarios.},
archivePrefix = {arXiv},
arxivId = {1708.09251},
author = {Cully, Antoine and Demiris, Yiannis},
doi = {10.1109/TEVC.2017.2704781},
eprint = {1708.09251},
file = {:home/sarios/papers/Evolution/Quality and Diversity Optimization- A Unifying Modular Framework.pdf:pdf},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Behavioral diversity,collection of solutions,novelty search,optimization methods,quality-diversity (QD)},
number = {2},
pages = {245--259},
title = {{Quality and Diversity Optimization: A Unifying Modular Framework}},
volume = {22},
year = {2018}
}
@article{Jansen2005,
abstract = {Mutation and crossover are the main search operators of different variants of evolutionary algorithms. Despite the many discussions on the importance of crossover nobody has proved rigorously for some explicitly defined fitness functions fn:{\{}0,1{\}}n→R that a genetic algorithm with crossover can optimize fn in expected polynomial time while all evolution strategies based only on mutation (and selection) need expected exponential time. Here such functions and proofs are presented for a genetic algorithm without any idealization. For some functions one-point crossover is appropriate while for others uniform crossover is the right choice. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Jansen, Thomas and Wegener, Ingo},
doi = {10.1016/j.dam.2004.02.019},
file = {:home/sarios/papers/Evolution/Real royal road functions—where crossover provably is essential.pdf:pdf},
issn = {0166218X},
journal = {Discrete Applied Mathematics},
keywords = {Crossover,Evolutionary algorithms,Genetic algorithms},
number = {1-3},
pages = {111--125},
title = {{Real royal road functions - Where crossover provably is essential}},
volume = {149},
year = {2005}
}
@article{Bojarski2010,
abstract = {REALM is a rule-based evolutionary computation agent for playing a modified version of Super Mario Bros. according to the rules stipulated in the Mario AI Competition held in the 2010 IEEE Symposium on Computational Intelligence and Games. Two alternate representations for the REALM rule sets are reported here, in both hand-coded and learned versions. Results indicate that the second version, with an abstracted action set, tends to perform better overall, but the first version shows a steeper learning curve. In both cases, learning quickly surpasses the hand-coded rule sets.},
author = {Bojarski, Slawomir and Congdon, Clare Bates},
doi = {10.1109/ITW.2010.5593367},
file = {:home/sarios/papers/Evolution/REALM- A rule-based evolutionary computation agent that learns to play Mario.pdf:pdf},
isbn = {9781424462971},
journal = {Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games, CIG2010},
pages = {83--90},
title = {{REALM: A rule-based evolutionary computation agent that learns to play Mario}},
year = {2010}
}
@article{Kounios2016,
abstract = {It has been hypothesized that one of the main reasons evolution has been able to produce such impressive adaptations is because it has improved its own ability to evolve -- "the evolution of evolvability". Rupert Riedl, for example, an early pioneer of evolutionary developmental biology, suggested that the evolution of complex adaptations is facilitated by a developmental organization that is itself shaped by past selection to facilitate evolutionary innovation. However, selection for characteristics that enable future innovation seems paradoxical: natural selection cannot favor structures for benefits they have not yet produced, and favoring characteristics for benefits that have already been produced does not constitute future innovation. Here we resolve this paradox by exploiting a formal equivalence between the evolution of evolvability and learning systems. We use the conditions that enable simple learning systems to generalize, i.e., to use past experience to improve performance on previously unseen, future test cases, to demonstrate conditions where natural selection can systematically favor developmental organizations that benefit future evolvability. Using numerical simulations of evolution on highly epistatic fitness landscapes, we illustrate how the structure of evolved gene regulation networks can result in increased evolvability capable of avoiding local fitness peaks and discovering higher fitness phenotypes. Our findings support Riedl's intuition: Developmental organizations that "mimic" the organization of constraints on phenotypes can be favored by short-term selection and also facilitate future innovation. Importantly, the conditions that enable the evolution of such surprising evolvability follow from the same non-mysterious conditions that permit generalization in learning systems.},
archivePrefix = {arXiv},
arxivId = {1612.05955},
author = {Kounios, Loizos and Clune, Jeff and Kouvaris, Kostas and Wagner, G{\"{u}}nter P. and Pavlicev, Mihaela and Weinreich, Daniel M. and Watson, Richard A.},
eprint = {1612.05955},
file = {:home/sarios/papers/Evolution/Resolving the paradox of evolvability with learning theory- How evolution learns to improve evolvability on rugged fitness landscapes.pdf:pdf},
pages = {5--12},
title = {{Resolving the paradox of evolvability with learning theory: How evolution learns to improve evolvability on rugged fitness landscapes}},
url = {http://arxiv.org/abs/1612.05955},
year = {2016}
}
@article{Stulp2013,
abstract = {{\textless}p{\textgreater}Policy improvement methods seek to optimize the parameters of a policy with respect to a utility function. Owing to current trends involving searching in parameter space (rather than action space) and using reward-weighted averaging (rather than gradient estimation), reinforcement learning algorithms for policy improvement, e.g. PoWER and PI{\textless}/p{\textgreater}},
author = {Stulp, Freek and Sigaud, Olivier},
doi = {10.2478/pjbr-2013-0003},
file = {:home/sarios/papers/Evolution/Robot Skill Learning- From Reinforcement Learning to Evolution Strategies.pdf:pdf},
issn = {2081-4836},
journal = {Paladyn, Journal of Behavioral Robotics},
keywords = {black-box optimization,dynamic movement primitives,evolution strategies,reinforcement learning},
number = {1},
pages = {49--61},
title = {{Robot Skill Learning: From Reinforcement Learning to Evolution Strategies}},
url = {http://www.degruyter.com/view/j/pjbr.2013.4.issue-1/pjbr-2013-0003/pjbr-2013-0003.xml},
volume = {4},
year = {2013}
}
@article{Lehman2017b,
abstract = {While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.},
archivePrefix = {arXiv},
arxivId = {1712.06563},
author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
doi = {10.1145/3205455},
eprint = {1712.06563},
file = {:home/sarios/papers/Evolution/Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients.pdf:pdf},
isbn = {9781450356183},
title = {{Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients}},
url = {http://arxiv.org/abs/1712.06563},
year = {2017}
}
@article{Smith1993,
abstract = {In typical applications, genetic algorithms (GAs) process populations of potential problem solutions to evolve a single population member that specifies an “optimized” solution. The majority of GA analysis has focused on these optimization applications. In other applications (notably learning classifier systems and certain connectionist learning systems), aGAsearches for a population of cooperative structures that jointly perform a computational task. This paper presents an analysis of this type of GA problem. The analysis considers a simplified genetics-based machine learning system: a model of an immune system. In this model, a GA must discover a set of pattern-matching antibodies that effectively match a set of antigen patterns. Analysis showshowaGAcan automatically evolve and sustain a diverse, cooperative population. The cooperation emerges as a natural part of the antigen-antibody matching procedure. This emergent effect is shown to be similar to fitness sharing, an explicit technique for multi-modal GA optimization. Further analysis shows how the GA population can adapt to express various degrees of generalization. The results show how GAs can automatically and simultaneously discover effective groups of cooperative computational structures.},
author = {Smith, Robert E. and Forrest, Stephanie and Perelson, Alan S.},
doi = {10.1162/evco.1993.1.2.127},
file = {:home/sarios/papers/Evolution/Searching for Diverse, Cooperative Populations with Genetic Algorithms.pdf:pdf},
issn = {1063-6560},
journal = {Evolutionary Computation},
keywords = {classifier systems,computational immunology,fitness sharing,genetic algorithms},
number = {2},
pages = {127--149},
title = {{Searching for Diverse, Cooperative Populations with Genetic Algorithms}},
volume = {1},
year = {1993}
}
@article{Back1899,
abstract = {-Due to its independenceof the actual search space and its impact on the exploration-exploitation tradeoff, se-lection is an important operator in any kind of Evolution-ary Algorithm. In this paper, all important selection o p erators are discussed and quantitatively compared with re-spect to their selective pressure. The comparison clarifies that only a few really different and useful selection oper-ators exist: Proportional selection (in combination with a scaling method), linear ranking, tournament selection, and (?,A)-selection (respectively (p+A)-selection). Their selec-tive pressure increases in the order as they are listed here. The theoretical results are confirmed by an experimental investigation using a Genetic Algorithm with different se-lection methods on a simple unimodal objective funrtion.},
author = {Back, T.},
doi = {10.1109/ICEC.1994.350042},
file = {:home/sarios/papers/Evolution/Selective Pressure in Evolutionary Algorithms- A Characterization of Selection Mechanisms.pdf:pdf},
isbn = {0-7803-1899-4},
journal = {Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence},
number = {1},
pages = {57--62},
pmid = {4812290},
title = {{Selective pressure in evolutionary algorithms: a characterization of selection mechanisms}},
url = {http://ieeexplore.ieee.org/document/350042/},
year = {1899}
}
@article{Morse2016,
abstract = {While evolutionary algorithms (EAs) have long offered an alternative approach to optimization, in recent years backpropagation through stochastic gradient descent (SGD) has come to dominate the fi elds of neural network optimization and deep learning. One hypothesis for the absence of EAs in deep learning is that modern neural networks have become so high dimensional that evolution with its inexact gradient cannot match the exact gradient calculations of backpropagation. Furthermore, the evaluation of a single individual in evolution on the big data sets now prevalent in deep learning would present a prohibitive obstacle towards efficient optimization. This paper challenges these views, suggesting that EAs can be made to run signi cantly faster than previously thought by evaluating individuals only on a small number of training examples per generation. Surprisingly, using this approach with only a simple EA (called the limited evaluation EA or LEEA) is competitive with the performance of the state-of-the-art SGD variant RMSProp on several benchmarks with neural networks with over 1,000 weights. More investigation is warranted, but these initial results suggest the possibility that EAs could be the fi rst viable training alternative for deep learning outside of SGD, thereby opening up deep learning to all the tools of evolutionary computation.},
author = {Morse, Gregory and Stanley, Kenneth O.},
doi = {10.1145/2908812.2908916},
file = {:home/sarios/papers/Evolution/Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks.pdf:pdf},
isbn = {9781450342063},
journal = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference - GECCO '16},
keywords = {all or part of,artificial,deep learning,intelligence,machine learning,neural networks,or,or hard copies of,pattern recognition and classification,permission to make digital,this work for personal},
number = {Gecco},
pages = {477--484},
title = {{Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks}},
url = {http://dl.acm.org/citation.cfm?doid=2908812.2908916},
year = {2016}
}
@article{Lessin2015,
abstract = {In the past, evolved virtual creatures (EVCs) have been developed with rigid, segmented bodies, and with soft bodies, but never before with a combination of the two. In nature, however, creatures combining a rigid skeleton and non-rigid muscles are some of the most complex and successful examples of life on earth. Now, for the first time, creatures with fully evolved rigid-body skeletons and soft-body muscles can be developed in the virtual world, as well. By exploiting and re-purposing the capabilities of existing soft-body simulation systems, we can evolve complex and effective simulated muscles, able to drive a rigid-body skeleton. In this way, we can begin to bridge the gap between articulated and soft-bodied EVCs, and take the next step on a nature-inspired path to meaningful morphological complexity for evolved virtual creatures.},
author = {Lessin, Dan and Risi, Sebastian},
doi = {10.1145/2739482.2764897},
file = {:home/sarios/papers/Evolution/Soft-Body Muscles for Evolved Virtual Creatures- The Next Step on a Bio-Mimetic Path to Meaningful Morphological Complexity.pdf:pdf},
isbn = {9781450334884},
journal = {European Conference on Artificial Life},
keywords = {evolved virtual creatures,morphological complexity,mus-},
pages = {761--762},
title = {{Soft-Body Muscles for Evolved Virtual Creatures: The Next Step on a Bio-Mimetic Path to Meaningful Morphological Complexity}},
year = {2015}
}
@article{Krawiec2017,
author = {Krawiec, Krzysztof and Heywood, Malcolm},
doi = {10.1145/3067695.3067705},
file = {:home/sarios/papers/Evolution/Solving Complex Problems with Coevolutionary Algorithms.pdf:pdf},
isbn = {9781450349390},
journal = {Proceedings of the Genetic and Evolutionary Computation Conference Companion on   - GECCO '17},
number = {July},
pages = {782--806},
title = {{Solving complex problems with coevolutionary algorithms}},
url = {http://dl.acm.org/citation.cfm?doid=3067695.3067705},
year = {2017}
}
@article{Jin2011,
abstract = {Surrogate-assisted, or meta-model based evolutionary computation uses efficient computational models, often known as surrogates or meta-models, for approximating the fitness function in evolutionary algorithms. Research on surrogate-assisted evolutionary computation began over a decade ago and has received considerably increasing interest in recent years. Very interestingly, surrogate-assisted evolutionary computation has found successful applications not only in solving computationally expensive single- or multi-objective optimization problems, but also in addressing dynamic optimization problems, constrained optimization problems and multi-modal optimization problems. This paper provides a concise overview of the history and recent developments in surrogate-assisted evolutionary computation and suggests a few future trends in this research area. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Jin, Yaochu},
doi = {10.1016/j.swevo.2011.05.001},
file = {:home/sarios/papers/Evolution/Surrogate-assisted evolutionary computation- Recent advances and future challenges.pdf:pdf},
issn = {22106502},
journal = {Swarm and Evolutionary Computation},
keywords = {Evolutionary computation,Expensive optimization problems,Machine learning,Meta-models,Model management,Surrogates},
number = {2},
pages = {61--70},
publisher = {Elsevier B.V.},
title = {{Surrogate-assisted evolutionary computation: Recent advances and future challenges}},
url = {http://dx.doi.org/10.1016/j.swevo.2011.05.001},
volume = {1},
year = {2011}
}
@article{Orchard2016,
abstract = {{\textcopyright} 2016 IEEE. Evolution is extremely creative. The mere availability of a mechanism for synaptic change seems to be enough for evolution to derive a learning rule. Many simulations of evolution have evolved learning in a highly guided manner. Either by constraining the update function to a Hebbian form, or by supplying an error/teaching signal. In this paper, we aim to evolve a more general learning rule. And since neural networks are so versatile, we construct the learning function itself out of a neural network. Our evolved networks excel at the foraging task they evolved in. Amazingly, they even function robustly when tested outside of their historical niche. The same cannot be said for the Hebbian learning networks we compare to.},
author = {Orchard, Jeff and Wang, Lin},
doi = {10.1109/IJCNN.2016.7727815},
file = {:home/sarios/papers/Evolution/The Evolution of a Generalized Neural Learning Rule.pdf:pdf},
isbn = {9781509006199},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {4688--4694},
title = {{The evolution of a generalized neural learning rule}},
volume = {2016-Octob},
year = {2016}
}
@article{Izquierdo2008,
abstract = {We present a case of a genotype-phenotype map, that when evolved in variable environments optimizes its genetic representation to structure phenotypic variability properties, allowing rapid adaptation to novel environments. How genetic representations evolved is a relatively neglected topic in evolutionary theory. Furthermore, the black art of genetic algorithms depends on the practitioner to choose a representation that captures problem structure. Nature has achieved remarkably efficient heuristic search mechanisms without top-down design. We propose that an important example of this, ubiquitous in biology is the structuring of the phenotypic variability properties of gene networks. By studying a simple model of gene networks in which topology is a function of interactions between transcription factor proteins and transcription factor binding sites (TFBS), we show that transcription factor binding matrices (TFBM) evolve to positively constrain phenotypic variability in response to transcription factor binding sequence mutations.},
author = {Izquierdo, Eduardo J and Fernando, Chrisantha T},
file = {:home/sarios/papers/Evolution/The Evolution of Evolvability in Gene Transcription Networks.pdf:pdf},
isbn = {9780262750172},
journal = {Artificial Life},
pages = {265--273},
title = {{The Evolution of Evolvability in Gene Transcription Networks}},
url = {http://www.cogs.susx.ac.uk/users/eji21/pubs/IzquierdoFernando{\_}AlifeXI.pdf},
year = {2008}
}
@article{Lenski2003,
abstract = {A long-standing challenge to evolutionary theory has been whether it can explain the origin of complex organismal features. We examined this issue using digital organisms—computer programs that self-replicate, mutate, compete and evolve. Populations of digital organisms often },
author = {Lenski, R and Ofria, C and Pennock, R and Adami, C},
doi = {10.1038/nature01568},
file = {:home/sarios/papers/Evolution/The evolutionary origin of complex features.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {May},
pages = {139--144},
title = {{The evolutionary origin of complex}},
url = {http://chaos.swarthmore.edu/courses/SOC002a/Nature{\_}EvoComplex.pdf{\%}5Cnpapers2://publication/uuid/88CDF474-B083-40BB-9C8D-AD22B8D7DF49},
volume = {423},
year = {2003}
}
@article{Harper2009,
abstract = {The replicator equation is interpreted as a continuous inference equation and a formal similarity between the discrete replicator equation and Bayesian inference is described. Further connections between inference and the replicator equation are given including a discussion of information divergences and exponential families as solutions for the replicator dynamic, using Fisher information and information geometry.},
archivePrefix = {arXiv},
arxivId = {0911.1763},
author = {Harper, Marc},
eprint = {0911.1763},
file = {:home/sarios/papers/Evolution/The Replicator Equation as an Inference Dynamic.pdf:pdf},
keywords = {and phrases,bayesian inference,evolutionary game theory,infor-,information geometry,mation divergence,replicator equation},
pages = {1--10},
title = {{The Replicator Equation as an Inference Dynamic}},
url = {http://arxiv.org/abs/0911.1763},
year = {2009}
}
@article{Harvey,
author = {Harvey, Inman},
file = {:home/sarios/papers/Evolution/The Microbial Genetic Algorithm.pdf:pdf},
keywords = {genetic},
title = {{The Microbial Genetic Algorithm 1 Introduction 2 GAs Stripped to the Minimum}}
}
@article{Chalmers1990,
abstract = {This paper explores how an evolutionary process can produce systems that learn. A general framework for the evolution of learning is outlined, and is applied to the task of evolving mechanisms suitable for supervised learning in single-layer neural networks. Dynamic properties of a network's information-processing capacity are encoded genetically, and these properties are subjected to selective pressure based on their success in producing adaptive behavior in diverse environments. As a result of selection and genetic recombination, various successful learning mechanisms evolve, including the well-known delta rule. The effect of environmental diversity on the evolution of learning is investigated, and the role of different kinds of emergent phenomena in genetic and connectionist systems is discussed.},
author = {Chalmers, David J.},
file = {:home/sarios/papers/Evolution/The Evolution of Learning- An Experiment in Genetic Connectionism.pdf:pdf},
isbn = {1-55860-156-2},
issn = {9781483214481},
journal = {Proceedings of the 1990 Connectionist Models Summer School},
keywords = {evolutionary learning},
pages = {1--20},
title = {{The evolution of learning: An experiment in genetic connectionism}},
url = {http://consc.net/papers/evolution.pdf},
year = {1990}
}
@article{DeJong2004,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 2940304 The - Coevolution Conference DOI : 10 . 1007 / 978 - 3 - 540 - 24854 - 5{\_}55 : CiteSeer CITATIONS 60 READS 53 1 : Some : Genetic Edwin . de IMC 75 , 727 SEE All . de . The . Abstract . Coevolution can in principle provide progress for problems where no accurate evaluation function is available . An important open question however is how coevolution can be set up such that progress can be ensured . Previous work has provided progress guarantees either for limited cases or using strict acceptance conditions that can result in stalling . We present a monotonically improving archive for the gen - eral asymmetric case of coevolution where learners and tests may be of distinct types , for which any detectable improvement can be accepted into the archive . The Incremental Pareto - Coevolution Archive is demon - strated in experiments .},
author = {{De Jong}, Edwin D},
doi = {10.1007/978-3-540-24854-5_55},
file = {:home/sarios/papers/Evolution/The Incremental Pareto-Coevolution Archive.pdf:pdf},
pages = {525--536},
title = {{The Incremental Pareto - Coevolution Archive}},
url = {https://www.researchgate.net/profile/Edwin{\_}De{\_}Jong/publication/2940304{\_}The{\_}Incremental{\_}Pareto-Coevolution{\_}Archive/links/00b7d535871cb46d08000000.pdf},
year = {2004}
}
@article{WHITLEY1999,
abstract = {Parallel Genetic Algorithms have often been reported to yield better performance than Genetic Algorithms which use a single large panmictic population. In the case of the Island Model genetic algorithm, it has been informally argued that having multiple subpopulations helps to preserve genetic diversity, since each island can potentially follow a different search trajectory through the search space. It is also possible that since linearly separable problems are often used to test Genetic Algorithms, that Island Models may simply be particularly well suited to exploiting the separable nature of the test problems. We explore this possibility by using the infinite population models of simple genetic algorithms to study how Island Models can track multiple search trajectories. We also introduce a simple model for better understanding when Island Model genetic algorithms may have an advantage when processing some test problems. We provide empirical results for both linearly separable and nonseparable parameter optimization functions.},
author = {WHITLEY, D and RANA, S and HECKENDORN, RB},
doi = {10.1.1.36.7225},
file = {:home/sarios/papers/Evolution/The Island Model Genetic Algorithm- On Separability, Population Size and Convergence.pdf:pdf},
issn = {1330-1136},
journal = {CIT. Journal of computing and information technology},
number = {1},
pages = {33--47},
title = {{The island Model Genetic algorithm: On separability, population size and convergence}},
url = {http://cit.fer.hr/index.php/CIT/article/view/2919},
volume = {7},
year = {1999}
}
@article{Jaderberg2017,
abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present $\backslash$emph{\{}Population Based Training (PBT){\}}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
annote = {The paper presents a method to find a good set of hyperparameters and trainable parameters for a model. The procedure not only learns hyper parameters, but also a hyperparameter schedule (different set of hyper parameters as the training progresses).

It presents itself as an alternative to random / grid search and bayesian optimization. It's main advantage over the latter is that it can be used asynchronously in parallel. Bayesian optimization is (as far as I know), sequential.

Population based training is based on the idea of evaluating the performance of a model's hyper parameters and parameters (network weights), for instance by average reward in the last 10 episodes. If a model is performing worse than average, the hyper parameters and / or parameters of a better model will be copied over and mutated (to further explore the space of (hyper)parameters). 

Because of the greedy nature of the algorithm, they recommend a population of at least 20. Diminishing returns with bigger population sizes.},
archivePrefix = {arXiv},
arxivId = {1711.09846},
author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
eprint = {1711.09846},
file = {:home/sarios/papers/population-based-training.pdf:pdf},
title = {{Population Based Training of Neural Networks}},
url = {http://arxiv.org/abs/1711.09846},
year = {2017}
}
@article{Littman1994,
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly...},
author = {Littman, Michael L.},
doi = {10.1016/B978-1-55860-335-6.50027-1},
file = {:home/sarios/papers/markov-games-as-gramework-for-multi-agent-rl.pdf:pdf},
isbn = {1-55860-335-2},
issn = {00493848},
journal = {Machine Learning Proceedings 1994},
keywords = {Environments},
mendeley-tags = {Environments},
pages = {157--163},
pmid = {17034835},
title = {{Markov games as a framework for multi-agent reinforcement learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271},
year = {1994}
}
@article{Brown2018,
abstract = {A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.},
archivePrefix = {arXiv},
arxivId = {1805.08195},
author = {Brown, Noam and Sandholm, Tuomas and Amos, Brandon},
eprint = {1805.08195},
file = {:home/sarios/papers/Depth-Limited-Solving-for-imperfect-information-games.pdf:pdf},
title = {{Depth-Limited Solving for Imperfect-Information Games}},
url = {http://arxiv.org/abs/1805.08195},
year = {2018}
}
@article{Lipton2018,
archivePrefix = {arXiv},
arxivId = {1807.03341},
author = {Lipton, Zachary C and Steinhardt, Jacob},
eprint = {1807.03341},
file = {:home/sarios/papers/troubling-trends-in-machine-learning-scholarship.pdf:pdf},
pages = {1--15},
title = {{Troubling Trends in Machine Learning Scholarship}},
year = {2018}
}
@article{Lucas2018,
abstract = {This paper describes the N-Tuple Bandit Evolutionary Algorithm (NTBEA), an optimisation algorithm developed for noisy and expensive discrete (combinatorial) optimisation problems. The algorithm is applied to two game-based hyper-parameter optimisation problems. The N-Tuple system directly models the statistics, approximating the fitness and number of evaluations of each modelled combination of parameters. The model is simple, efficient and informative. Results show that the NTBEA significantly outperforms grid search and an estimation of distribution algorithm.},
archivePrefix = {arXiv},
arxivId = {1802.05991},
author = {Lucas, Simon M and Liu, Jialin and Perez-Liebana, Diego},
eprint = {1802.05991},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lucas, Liu, Perez-Liebana - 2018 - The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation.pdf:pdf},
title = {{The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation}},
url = {http://arxiv.org/abs/1802.05991},
year = {2018}
}
@article{Hessel2017,
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
eprint = {1710.02298},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcement Learning.pdf:pdf},
keywords = {dqn},
mendeley-tags = {dqn},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1710.02298},
year = {2017}
}
@article{Firoiu2017,
abstract = {There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.},
archivePrefix = {arXiv},
arxivId = {1702.06230},
author = {Firoiu, Vlad and Whitney, William F. and Tenenbaum, Joshua B.},
eprint = {1702.06230},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Firoiu, Whitney, Tenenbaum - 2017 - Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning.pdf:pdf},
keywords = {self-play},
mendeley-tags = {self-play},
title = {{Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.06230},
year = {2017}
}
@article{Li2018,
archivePrefix = {arXiv},
arxivId = {1805.02070},
author = {Li, Yu-Jhe and Chang, Hsin-Yu and Lin, Yu-Jing and Wu, Po-Wei and Wang, Yu-Chiang Frank},
eprint = {1805.02070},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2018 - Deep Reinforcement Learning for Playing 2.5D Fighting Games.pdf:pdf},
title = {{Deep Reinforcement Learning for Playing 2.5D Fighting Games}},
url = {http://arxiv.org/abs/1805.02070},
year = {2018}
}
@article{Graepel2004,
abstract = {We apply reinforcement learning to the problem of finding good policies for a fighting agent in a commercial computer game. The learning agent is trained using the SARSA algorithm for on-policy learning of an action-value function represented by linear and neural network function approximators. We discuss the selection and construction of features, actions, and rewards as well as other design choices necessary to integrate the learning process into the game. The learning agent is trained against the built-in AI of the game with different rewards encouraging aggressive or defensive behaviour. We show that the learning agent finds interesting (and partly near optimal) policies in accordance with the reward functions provided. We also discuss the particular challenges arising in the application of reinforcement learning to the domain of computer games.},
author = {Graepel, Thore and Herbrich, Ralf and Gold, Julian},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graepel, Herbrich, Gold - 2004 - Learning to Fight.pdf:pdf},
issn = {0885-842X},
journal = {Proceedings of the International Conference on Computer Games: Artificial Intelligence, Design and Education},
keywords = {fighting games,markov decision process,q-learning,reinforcement learning,sarsa},
pages = {193--200},
pmid = {3347383},
title = {{Learning to Fight}},
url = {http://www.herbrich.me/papers/graehergol04.pdf},
year = {2004}
}
@inproceedings{Bown2011,
abstract = {One of the goals of artificial life in the arts is to develop systems that exhibit creativity. We argue that creativity per se is a confusing goal for artificial life systems because of the complexity of the relationship between the system, its designers and users, and the creative domain. We analyse this confusion in terms of factors affecting individual human motivation in the arts, and the methods used to measure the success of artificial creative systems. We argue that an attempt to understand creative agency as a common thread in nature, human culture, human individuals and computational systems is a necessary step towards a better understanding of computational creativity. We define creative agency with respect to existing theories of creativity and consider human creative agency in terms of human social behaviour. We then propose how creative agency can be used to analyse the creativity of computational systems in artistic domains.},
address = {Berlin, Heidelberg},
author = {Bown, Oliver and McCormack, Jon},
booktitle = {Advances in Artificial Life. Darwin Meets von Neumann},
editor = {Kampis, George and Karsai, Istv{\'{a}}n and Szathm{\'{a}}ry, E{\"{o}}rs},
isbn = {978-3-642-21314-4},
pages = {254--261},
publisher = {Springer Berlin Heidelberg},
title = {{Creative Agency: A Clearer Goal for Artificial Life in the Arts}},
year = {2011}
}
@inproceedings{DInverno2015,
annote = {General comments. There are many questions posed in this paper. Some of which are not answered.


Summary. The paper explores, solely at a theoretical level, the role of AI system in the process of art creation from three different perspectives (as artists, AI researchers and as audience). (add a bit on some of the questions?)

The paper introduces the concept of Heroic AI and Collaborative AI in the framework of art creation. The former encapsualtes any AI system that autonomously produces a piece of art, without intervention or collaboration with outside entities. The latter encompases more abstract systems that communicate with external systems (humans or other AI systems), recieving and communicating feedback on the process of art creation in order to collaboratively generate art.

There are some insightful comments on the trajectory that AI research has been taking over the decades, focusing mainly on systems that could by themself compare or surpass human performance in various fields. Instead, the main body of the paper pays more attention to AI systems that have the potential to aid and support humans in their own work, in this case being the creation of art. From this, the authors propose to move aside from generating heroic AI systems in favour of researching systems of collaborative nature.

Major points:
-Most of the phrased questions are never answered. It is mentioned that questions will be studied in further detail, but most questions are only presented, not discussed.
- The last paragraph of section 2 talks about how the intended outcome of automation is to save time, and how increased automation has not yielded an increase in a human's average free time. The statement is well referenced and the view presented is well balanced. However, I failed to see the relevance of this statement in the broader context of the paper. This paper dwells into AI systems and their relevance in the process of creating art, not on the time efficiency that could come from automating more parts of the artistic process, or the unexpected increase of "time famine" that could come from it. 

Minor points:
-Interesting small comentary on the relevancy of art created by other specices.
- The comment "This tradition [bringing new form of creativity] into the world is renewed... with a greater ironic twist". You indicate that the irony lies in the fact that a heroic AI is restricted to its programmed behaviour, which is based on the author's already existing notion of art. The tone of this note in the paper is dismissal, however this is a systematical restriction that modern "heroic AI" systems suffer from.},
author = {D'Inverno, Mark and McCormack, Jon},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/D'Inverno, McCormack - 2015 - Heroic versus collaborative AI for the arts.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
pages = {2438--2444},
title = {{Heroic versus collaborative AI for the arts}},
volume = {2015-Janua},
year = {2015}
}
@article{Mccormack2016,
author = {Mccormack, Jon and Inverno, Mark},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mccormack, Inverno - 2016 - Designing Improvisational Interfaces.pdf:pdf},
number = {June},
pages = {103--110},
title = {{Designing Improvisational Interfaces}},
year = {2016}
}
@article{Silver2014,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value func- tion. This simple form means that the deter- ministic policy gradient can be estimated much more efficiently than the usual stochastic pol- icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter- parts in high-dimensional action spaces.},
annote = {Deterministic policy gradient is an off-policy, model free policy gradient algorithm. The agent moves in the parameter space of the policy to approximate a target deterministic policy from an exploratory behavioural policy.

100{\%} Talk how this is a corner case of stochastic policy gradient algorithms in your literature review.},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2014 - Deterministic Policy Gradient Algorithms.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
pages = {387--395},
title = {{Deterministic Policy Gradient Algorithms}},
year = {2014}
}
@article{Andersen2017,
author = {Andersen, Per-arne},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andersen - 2017 - Deep RTS A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games.pdf:pdf},
title = {{Deep RTS : A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games}},
year = {2017}
}
@article{Ontanon2013,
abstract = {Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Na¨ ıve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS al- gorithm based on Na¨ ıve Sampling called Na¨ ıveMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, Na¨ ıveMCTS performs significantly better than other algorithms. Introduction},
author = {Onta{\~{n}}{\'{o}}n, Santiago},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Onta{\~{n}}{\'{o}}n - 2013 - The combinatorial multi-armed bandit problem and its application to real-time strategy games.pdf:pdf},
journal = {Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
pages = {58--64},
title = {{The combinatorial multi-armed bandit problem and its application to real-time strategy games}},
url = {http://www.aaai.org/ocs/index.php/AIIDE/AIIDE13/paper/viewPaper/7377},
year = {2013}
}
@article{Lin1993,
abstract = {Reinforcement learning agents are adaptive, reactive, and self-supervised. The aim of this dissertation is to extend the state of the art of reinforcement learning and enable its applications to complex robot-learning problems. In particular, it focuses on two issues. First, learning from sparse and delayed reinforcement signals is hard and in general a slow process. Techniques for reducing learning time must be devised. Second, most existing reinforcement learning methods assume that the world is a Markov decision process. This assumption is too strong for many robot tasks of interest.$\backslash$r$\backslash$nThis dissertation demonstrates how we can possibly overcome the slow learning problem and tackle non-Markovian environments, making reinforcement learning more practical for realistic robot tasks: (1) Reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization, resulting in a significant learning speedup. Neural networks are used in this dissertation, and they generalize effectively even in the presence of noise and a large of binary and real-valued inputs. (2) Reinforcement learning agents can save many learning trials by using an action model, which can be learned on-line. With a model, an agent can mentally experience the effects of its actions without actually executing them. Experience replay is a simple technique that implements this idea, and is shown to be effective in reducing the number of action executions required. (3) Reinforcement learning agents can take advantage of instructive training instances provided by human teachers, resulting in a significant learning speedup. Teaching can also help learning agents avoid local optima during the search for optimal control. Simulation experiments indicate that even a small amount of teaching can save agents many learning trials. (4) Reinforcement learning agents can significantly reduce learning time by hierarchical learning--they first solve elementary learning problems and then combine solutions to the elementary problems to solve a complex problem. Simulation experiments indicate that a robot with hierarchical learning can solve a complex problem, which otherwise is hardly solvable within a reasonable time. (5) Reinforcement learning agents can deal with a wide range of non-Markovian environments by having a memory of their past. Three memory architectures are discussed. They work reasonably well for a variety of simple problems. One of them is also successfully applied to a nontrivial non-Markovian robot task.$\backslash$r$\backslash$n$\backslash$r$\backslash$nThe results of this dissertation rely on computer simulation, including (1) an agent operating in a dynamic and hostile environment and (2) a mobile robot operating in a noisy and non-Markovian environment. The robot simulator is physically realistic. This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning.},
author = {Lin, Long-ji},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin - 1993 - Reinforcement Learning for Robots Using Neural Networks.pdf:pdf},
journal = {Report, CMU},
keywords = {off policy,replay buffer},
mendeley-tags = {off policy,replay buffer},
pages = {1--155},
title = {{Reinforcement Learning for Robots Using Neural Networks}},
year = {1993}
}
@article{Bhatnagar2009,
abstract = {We present four new reinforcement learning algorithms based on actor-critic, natural-gradient and function-approximation ideas, and we provide their convergence proofs. Actor-critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function-approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor-critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor-critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms. {\textcopyright} 2009 Elsevier Ltd.},
author = {Bhatnagar, Shalabh and Sutton, Richard S. and Ghavamzadeh, Mohammad and Lee, Mark},
doi = {10.1016/j.automatica.2009.07.008},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatnagar et al. - 2009 - Natural actor-critic algorithms.pdf:pdf},
isbn = {0005-1098},
issn = {00051098},
journal = {Automatica},
keywords = {Actor-critic reinforcement learning algorithms,Approximate dynamic programming,Function approximation,Natural gradient,Policy-gradient methods,Temporal difference learning,Two-timescale stochastic approximation,actor critic},
mendeley-tags = {actor critic},
number = {11},
pages = {2471--2482},
publisher = {Elsevier Ltd},
title = {{Natural actor-critic algorithms}},
url = {http://dx.doi.org/10.1016/j.automatica.2009.07.008},
volume = {45},
year = {2009}
}
@article{Degris2012,
annote = {First off policy algorithm, using a behavioural policy for exploration and importance sampling techniques to weigh the update to actor parameters. Eligibility traces are used to calculate updates.},
archivePrefix = {arXiv},
arxivId = {arXiv:1205.4839v5},
author = {Degris, Thomas and White, Martha and Sutton, Richard S},
doi = {10.1.1.385.4471},
eprint = {arXiv:1205.4839v5},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Degris, White, Sutton - 2012 - Off-Policy Actor-Critic.pdf:pdf},
isbn = {978-1-4503-1285-1},
issn = {{\textless}null{\textgreater}},
keywords = {actor critic},
mendeley-tags = {actor critic},
title = {{Off-Policy Actor-Critic}},
year = {2012}
}
@article{Konda2000,
abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information pro-vided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
archivePrefix = {arXiv},
arxivId = {1607.07086},
author = {Konda, Vijay R and Tsitsiklis, John N},
doi = {10.1137/S0363012901385691},
eprint = {1607.07086},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konda, Tsitsiklis - 2000 - Actor-Critic Algorithms.pdf:pdf},
isbn = {0363-0129},
issn = {0363-0129},
journal = {Nips},
keywords = {actor critic,actor-critic algorithms,markov decision processes,reinforcement learning,stochas-},
mendeley-tags = {actor critic},
number = {4},
pages = {1143--1166},
pmid = {21222527},
title = {{Actor-Critic Algorithms}},
volume = {42},
year = {2000}
}
@article{Baxter2001,
abstract = {Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes POMDPs controlled by parameterized stochastic policies. A similar algorithm was proposed by (Kimura et al. 1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free beta (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of GPOMDP, and show how the correct choice of the parameter beta is related to the mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter et al., this volume) we show how the gradient estimates generated by GPOMDP can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward.},
archivePrefix = {arXiv},
arxivId = {1106.0665},
author = {Baxter, Jonathan and Bartlett, Peter L.},
doi = {10.1613/jair.806},
eprint = {1106.0665},
isbn = {10769757 (ISSN)},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {319--350},
title = {{Infinite-horizon policy-gradient estimation}},
volume = {15},
year = {2001}
}
@article{Song2018,
abstract = {In this technical report, we consider an approach that combines the PPO objective and K-FAC natural gradient optimization, for which we call PPOKFAC. We perform a range of empirical analysis on various aspects of the algorithm, such as sample complexity, training speed, and sensitivity to batch size and training epochs. We observe that PPOKFAC is able to outperform PPO in terms of sample complexity and speed in a range of MuJoCo environments, while being scalable in terms of batch size. In spite of this, it seems that adding more epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to be worse than its A2C counterpart, ACKTR.},
annote = {Useful overview of policy gradient and trust region famous methods.},
archivePrefix = {arXiv},
arxivId = {1801.05566},
author = {Song, Jiaming and Wu, Yuhuai},
eprint = {1801.05566},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Wu - 2018 - An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients.pdf:pdf},
pages = {1--8},
title = {{An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients}},
url = {http://arxiv.org/abs/1801.05566},
year = {2018}
}
@article{UnityMLAgents,
author = {Unity},
title = {{Unity Machine Learning Agents}},
url = {https://unity3d.com/machine-learning/},
year = {2017}
}
@misc{Broodwar,
author = {Heinermann, Adam},
title = {{BWAPI: Brood war api, an api for interacting with starcraft: Broodwar}},
url = {https://bwapi.github.io/},
year = {2009}
}
@article{Synnaeve2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.00625v2},
author = {Synnaeve, Gabriel and Nardelli, Nantas and Auvolat, Alex and Chintala, Soumith and Lacroix, Timoth{\'{e}}e and Lin, Zeming and Richoux, Florian and Usunier, Nicolas},
eprint = {arXiv:1611.00625v2},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Synnaeve et al. - 2016 - TorchCraft a Library for Machine Learning Research on Real-Time Strategy Games arXiv 1611 . 00625v2 cs . LG.pdf:pdf},
pages = {1--6},
title = {{TorchCraft : a Library for Machine Learning Research on Real-Time Strategy Games arXiv : 1611 . 00625v2 [ cs . LG ] 3 Nov 2016}},
year = {2016}
}
@inproceedings{Kempka2017,
abstract = {The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.},
archivePrefix = {arXiv},
arxivId = {1605.02097},
author = {Kempka, Michal and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Jaskowski, Wojciech},
booktitle = {IEEE Conference on Computatonal Intelligence and Games, CIG},
doi = {10.1109/CIG.2016.7860433},
eprint = {1605.02097},
isbn = {9781509018833},
issn = {23254289},
keywords = {FPS,deep reinforcement learning,first-person perspective games,neural networks,video games,visual learning,visual-based reinforcement learning},
title = {{ViZDoom: A Doom-based AI research platform for visual reinforcement learning}},
year = {2017}
}
@article{Brockman2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brockman et al. - 2016 - OpenAI Gym.pdf:pdf},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Vinyals2017,
abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
archivePrefix = {arXiv},
arxivId = {1708.04782},
author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"{u}}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and van Hasselt, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
doi = {https://deepmind.com/documents/110/sc2le.pdf},
eprint = {1708.04782},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinyals et al. - 2017 - StarCraft II A New Challenge for Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1708.04782},
title = {{StarCraft II: A New Challenge for Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.04782},
year = {2017}
}
@inproceedings{Perot2017,
abstract = {The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.},
archivePrefix = {arXiv},
arxivId = {1605.02097},
author = {Perot, Etienne and Jaritz, Maximilian and Toromanoff, Marin and Charette, Raoul De},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.64},
eprint = {1605.02097},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perot et al. - 2017 - End-to-End Driving in a Realistic Racing Game with Deep Reinforcement Learning.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
keywords = {deep reinforcement learning,first-person perspective games,fps,neural networks,video games,visual learning,visual-based reinforcement learning},
pages = {474--475},
title = {{End-to-End Driving in a Realistic Racing Game with Deep Reinforcement Learning}},
volume = {2017-July},
year = {2017}
}
@inproceedings{Bellemare2015,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
archivePrefix = {arXiv},
arxivId = {1207.4708},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.1613/jair.3912},
eprint = {1207.4708},
isbn = {9781577357384},
issn = {10450823},
pages = {4148--4152},
title = {{The arcade learning environment: An evaluation platform for general agents}},
volume = {2015-Janua},
year = {2015}
}
@article{Lanctot2017,
abstract = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
archivePrefix = {arXiv},
arxivId = {1711.00832},
author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
eprint = {1711.00832},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanctot et al. - 2017 - A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.pdf:pdf},
number = {Nips},
title = {{A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning}},
url = {http://arxiv.org/abs/1711.00832},
year = {2017}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
doi = {10.1561/2200000006},
eprint = {1509.02971},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learning.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
pmid = {24966830},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Deisenroth2011,
abstract = {In this paper, we introduce pilco, a practi-cal, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforce-ment learning, in a principled way. By learn-ing a probabilistic dynamics model and ex-plicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprece-dented learning efficiency on challenging and high-dimensional control tasks.},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deisenroth, Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach to Policy Search.pdf:pdf},
title = {{PILCO: A Model-Based and Data-Efficient Approach to Policy Search}},
url = {http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf},
year = {2011}
}
@article{Pascanu2017,
abstract = {Conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the "Imagination-based Planner", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a "plan context" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex "imagination tree" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.},
archivePrefix = {arXiv},
arxivId = {1707.06170},
author = {Pascanu, Razvan and Li, Yujia and Vinyals, Oriol and Heess, Nicolas and Buesing, Lars and Racani{\`{e}}re, Sebastien and Reichert, David and Weber, Th{\'{e}}ophane and Wierstra, Daan and Battaglia, Peter},
doi = {10.1287/mnsc.45.4.560},
eprint = {1707.06170},
number = {November},
title = {{Learning model-based planning from scratch}},
url = {http://arxiv.org/abs/1707.06170},
year = {2017}
}
@article{Tapas1999,
author = {Das, Tapas K. and Gosavi, Abhijit and Mahadevan, Sridhar and Marchalleck, Nicholas},
doi = {10.1287/mnsc.45.4.560},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Das et al. - 1999 - Solving Semi-Markov Decision Problems Using Average Reward Reinforcement Learning.pdf:pdf},
issn = {0025-1909},
journal = {Management Science},
month = {apr},
number = {4},
pages = {560--574},
title = {{Solving Semi-Markov Decision Problems Using Average Reward Reinforcement Learning}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.45.4.560},
volume = {45},
year = {1999}
}
@article{Neto2005,
abstract = {Interest in robotic and software agents has increased a lot in the last decades. They allow us to do tasks that we would hardly accomplish otherwise. Par-ticularly, multi-agent systems motivate distributed solutions that can be cheaper and more efficient than centralized single-agent ones. In this context, reinforcement learning provides a way for agents to com-pute optimal ways of performing the required tasks, with just a small in-struction indicating if the task was or was not accomplished. Learning in multi-agent systems, however, poses the problem of non-stationarity due to interactions with other agents. In fact, the RL methods for the single agent domain assume stationarity of the environment and cannot be applied directly. This work is divided in two main parts. In the first one, the reinforcement learning framework for single-agent domains is analyzed and some classical solutions presented, based on Markov decision processes. In the second part, the multi-agent domain is analyzed, borrowing tools from game theory, namely stochastic games, and the most significant work on learning optimal decisions for this type of systems is presented. ii Contents Abstract 1},
author = {Neto, Gon{\c{c}}alo},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neto - 2005 - From Single-Agent to Multi-Agent Reinforcement Learning Foundational Concepts and Methods Learning Theory Course.pdf:pdf},
number = {May},
title = {{From Single-Agent to Multi-Agent Reinforcement Learning: Foundational Concepts and Methods Learning Theory Course}},
year = {2005}
}
@article{Wu2017,
abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
archivePrefix = {arXiv},
arxivId = {1708.05144},
author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
eprint = {1708.05144},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.pdf:pdf},
pages = {1--14},
title = {{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}},
url = {http://arxiv.org/abs/1708.05144},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
doi = {10.1177/0956797613514093},
eprint = {1602.01783},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
keywords = {actor critic},
mendeley-tags = {actor critic},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Sutton1999,
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the ¯rst time that a version of policy iteration with arbitrary di{\textregistered}erentiable function approximation is convergent to a locally optimal policy.},
annote = {This paper introduces a (somewhat dated) introduction to policy gradient methods which can prove valuable in lit review. It offers a digsted version of policy gradient therom as compared with many other papers.},
author = {Sutton, Richard S and Mcallester, David and Singh, Satinder and Mansour, Yishay},
doi = {10.1.1.37.9714},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf:pdf},
isbn = {0-262-19450-3},
issn = {0047-2875},
journal = {In Advances in Neural Information Processing Systems 12},
pages = {1057--1063},
title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
year = {1999}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
annote = {This is the article where introducing a baseline in reinforce is shown to keep the estimation unbiased.},
author = {Williams, Ronald J.},
doi = {10.1023/A:1022672621406},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - 1992 - Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning(2).pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis,policy-gradient},
mendeley-tags = {policy-gradient},
number = {3},
pages = {229--256},
pmid = {903},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
volume = {8},
year = {1992}
}
@article{Abdallah2016,
abstract = {Q-learning (QL) is a popular reinforcement learning algorithm that is guaranteed to converge to op-timal policies in Markov decision processes. However, QL exhibits an artifact: in expectation, the effective rate of updating the value of an action depends on the probability of choosing that action. In other words, there is a tight coupling between the learning dynamics and underlying execution policy. This coupling can cause performance degradation in noisy non-stationary environments. Here, we introduce Repeated Update Q-learning (RUQL), a learning algorithm that resolves the undesirable artifact of Q-learning while maintaining simplicity. We analyze the similarities and differences between RUQL, QL, and the closest state-of-the-art algorithms theoretically. Our analysis shows that RUQL maintains the convergence guarantee of QL in stationary environments, while relaxing the coupling between the execution policy and the learning dynamics. Experimental results confirm the theoretical insights and show how RUQL outperforms both QL and the closest state-of-the-art algorithms in noisy non-stationary environments.},
author = {Abdallah, Sherief and Org, Shario@ieee and Kaisers, Michael and Nl, Kaisers@cwi},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdallah et al. - 2016 - Addressing Environment Non-Stationarity by Repeating Q-learning Updates.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {multi-agent learning,non-stationary environ-,q-learning,qlearning,reinforcement learning},
mendeley-tags = {qlearning},
pages = {1--31},
title = {{Addressing Environment Non-Stationarity by Repeating Q-learning Updates}},
url = {http://www.jmlr.org/papers/volume17/14-037/14-037.pdf},
volume = {17},
year = {2016}
}
@article{browne2012survey,
author = {Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browne et al. - 2012 - A survey of {\{}Monte Carlo Tree Search{\}} methods.pdf:pdf},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {mcts},
mendeley-tags = {mcts},
number = {1},
pages = {1--43},
publisher = {IEEE},
title = {{A survey of {\{}Monte Carlo Tree Search{\}} methods}},
volume = {4},
year = {2012}
}
@article{Sutton1991,
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S.},
doi = {10.1145/122344.122377},
eprint = {arXiv:1011.1669v3},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton - 1991 - Dyna, an integrated architecture for learning, planning, and reacting.pdf:pdf},
isbn = {1-55860-141-4},
issn = {01635719},
journal = {ACM SIGART Bulletin},
number = {4},
pages = {160--163},
pmid = {15003161},
title = {{Dyna, an integrated architecture for learning, planning, and reacting}},
url = {http://portal.acm.org/citation.cfm?doid=122344.122377},
volume = {2},
year = {1991}
}
@article{Soemers2014,
abstract = {This thesis describes how Monte-Carlo Tree Search (MCTS) can be applied to perform tac-tical planning for an intelligent agent playing full games of StarCraft: Brood War. StarCraft is a Real-Time Strategy game, which has a large state-space, is played in real-time, and com-monly features two opposing players, capable of acting simultaneously. Using the MCTS al-gorithm for tactical planning is shown to in-crease the performance of the agent, compared to a scripted approach, when competing on a bot ladder. A combat model, based on Lanch-ester's Square Law, is described, and shown to achieve another gain in performance when used in Monte-Carlo simulations as replacement for a heuristic linear model. Finally, the MAST enhancement to the Playout Policy of MCTS is described, but it is found not to have a signifi-cant impact on the agent's performance.},
author = {Soemers, Dennis},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soemers - 2014 - Tactical Planning Using MCTS in the Game of StarCraft.pdf:pdf},
keywords = {lanchester,monte-carlo tree,s square law,search,starcraft},
pages = {12},
title = {{Tactical Planning Using MCTS in the Game of StarCraft}},
url = {https://project.dke.maastrichtuniversity.nl/games/files/bsc/Soemers{\_}BSc-paper.pdf},
year = {2014}
}
@inproceedings{Tijsma2017,
abstract = {—Balancing the ratio between exploration and ex-ploitation is an important problem in reinforcement learning. This paper evaluates four different exploration strategies com-bined with Q-learning using random stochastic mazes to inves-tigate their performances. We will compare: UCB-1, softmax, -greedy, and pursuit. For this purpose we adapted the UCB-1 and pursuit strategies to be used in the Q-learning algorithm. The mazes consist of a single optimal goal state and two suboptimal goal states that lie closer to the starting position of the agent, which makes efficient exploration an important part of the learning agent. Furthermore, we evaluate two different kinds of reward functions, a normalized one with rewards between 0 and 1, and an unnormalized reward function that penalizes the agent for each step with a negative reward. We have performed an extensive grid-search to find the best parameters for each method and used the best parameters on novel randomly generated maze problems of different sizes. The results show that softmax exploration outperforms the other strategies, although it is harder to tune its temperature parameter. The worst performing exploration strategy is -greedy.},
author = {Tijsma, Arryon D and Drugan, Madalina M and Wiering, Marco A},
booktitle = {2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016},
doi = {10.1109/SSCI.2016.7849366},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tijsma, Drugan, Wiering - 2017 - Comparing exploration strategies for Q-learning in random stochastic mazes.pdf:pdf},
isbn = {9781509042401},
title = {{Comparing exploration strategies for Q-learning in random stochastic mazes}},
year = {2017}
}
@article{Kaisers2010,
abstract = {Multi-agent learning is a crucial method to control or find solutions for systems, in which more than one entity needs to be adaptive. In todays interconnected world, such sys- tems are ubiquitous in many domains, including auctions in economics, swarm robotics in computer science, and politics in social sciences. Multi-agent learning is inherently more complex than single-agent learning and has a relatively thin theoretical framework supporting it. Recently, multi-agent learning dynamics have been linked to evolutionary game theory, allowing the interpretation of learning as an evolu- tion of competing policies in the mind of the learning agents. The dynamical system from evolutionary game theory that has been linked to Q-learning predicts the expected behav- ior of the learning agents. Closer analysis however allows for two interesting observations: the predicted behavior is not always the same as the actual behavior, and in case of deviation, the predicted behavior is more desirable. This discrepancy is elucidated in this article, and based on these new insights Frequency Adjusted Q- (FAQ-) learning is pro- posed. This variation of Q-learning perfectly adheres to the predictions of the evolutionary model for an arbitrarily large part of the policy space. In addition to the theoretical dis- cussion, experiments in the three classes of two-agent two- action games illustrate the superiority of FAQ-learning.},
author = {Kaisers, Michael and Tuyls, Karl},
doi = {10.11451838206},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaisers, Tuyls - 2010 - Frequency Adjusted Multi-agent Q-learning.pdf:pdf},
isbn = {978-0-9826571-1-9},
issn = {15582914},
journal = {Learning},
keywords = {dynamics,evolutionary game theory,multi agent learning,q learning,qlearning,replicator},
mendeley-tags = {qlearning},
pages = {309--316},
title = {{Frequency Adjusted Multi-agent Q-learning}},
url = {http://portal.acm.org/citation.cfm?id=1838250},
year = {2010}
}
@article{Guzdial2017,
abstract = {Intelligent agents need to be able to make predic-tions about their environment. In this work we present a novel approach to learn a forward simula-tion model via simple search over pixel input. We make use of a video game, Super Mario Bros., as an initial test of our approach as it represents a physics system that is significantly less complex than real-ity. We demonstrate the significant improvement of our approach in predicting future states compared with a baseline CNN and apply the learned model to train a game playing agent. Thus we evaluate the algorithm in terms of the accuracy and value of its output model.},
author = {Guzdial, Matthew and Li, Boyang and Riedl, Mark O},
doi = {10.24963/ijcai.2017/518},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guzdial, Li, Riedl - 2017 - Game Engine Learning from Video.pdf:pdf},
isbn = {9780999241103},
journal = {International Conference on Artificial Intelligence (IJCAI)},
title = {{Game Engine Learning from Video}},
url = {http://www.cc.gatech.edu/{~}riedl/pubs/ijcai17.pdf},
year = {2017}
}
@book{Sutton1998,
abstract = {Norton, B., {\&} Toohey, K. (Eds). (2004). Critical pedagogies and language learning. New York: Cambridge University Press. (362pp).},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
isbn = {0262193981},
issn = {0959-4388},
pages = {331},
pmid = {7888773},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@article{Montague1999,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Montague, P.Read},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}
@article{Montague1999a,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Montague, P.Read},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}
@article{Borkar1988,
abstract = {This paper develops a new framework for the study of Markov decision processes in which the control problem is viewed as an optimization problem on the set of canonically induced measures on the trajectory space of the joint state and control process. This set is shown to be compact convex. One then associates with each of the usual cost criteria (infinite horizon discounted cost, finite horizon, control up to an exit time) a naturally defined occupation measure such that the cost is an integral of some function with respect to this measure. These measures are shown to form a compact convex set whose extreme points are characterized. Classical results about existence of optimal strategies are recovered from this and several applications to multicriteria and constrained optimization problems are briefly indicated.},
author = {Borkar, Vivek S.},
doi = {10.1007/BF00353877},
issn = {01788051},
journal = {Probability Theory and Related Fields},
number = {4},
pages = {583--602},
pmid = {20840904},
title = {{A convex analytic approach to Markov decision processes}},
volume = {78},
year = {1988}
}
@book{Bertsekas2007,
abstract = {A major revision of the second volume of a textbook on the far-ranging algorithmic methododogy of Dynamic Programming, which can be used for optimal control, Markovian decision problems, planning and sequential decision making under uncertainty, and discrete/combinatorial optimization. The second volume is oriented towards mathematical analysis and computation, and treats infinite horizon problems extensively. New features of the 3rd edition are: 1) A major enlargement in size and scope: the length has increased by more than 50{\%}, and most of the old material has been restructured and/or revised. 2) Extensive coverage (more than 100 pages) of recent research on simulation-based approximate dynamic programming (neuro-dynamic programming), which allow the practical application of dynamic programming to large and complex problems. 3) An in-depth development of the average cost problem (more than 100 pages), including a full analysis of multichain problems, and an extensive analysis of infinite-spaces problems. 4) An introduction to infinite state space stochastic shortest path problems. 5) Expansion of the theory and use of contraction mappings in infinite state space problems and in neuro-dynamic programming. 6) A substantive appendix on the mathematical measure-theoretic issues that must be addressed for a rigorous theory of stochastic dynamic programming. Much supplementary material can be found in the book's web page: http://www.athenasc.com/dpbook.html},
author = {Bertsekas, Dimitri P},
booktitle = {Journal of the Operational Research Society},
doi = {10.1057/jors.1996.103},
isbn = {1886529264},
number = {6},
pages = {543},
title = {{Dynamic Programming and Optimal Control, Vol. II}},
url = {http://portal.acm.org/citation.cfm?id=1396348},
volume = {2},
year = {2007}
}
@book{Bellman1957,
abstract = {Dynamic Programming is a recursive method for solving sequential decision problems (hereafter abbre- viated as SDP). Also known as backward induction, it is used to find optimal decision rules in games against nature and subgame perfect equilibria of dynamic multi-agent games, and competitive equilib- ria in dynamic economic models. Dynamic programming has enabled economists to formulate and solve a huge variety of problems involving sequential decision making under uncertainty, and as a result it is now widely regarded as the single most important tool in economics. Section 2 provides a brief history of dynamic programming. Section 3 discusses some of the main theoretical results underlying dynamic programming, and its relation to game theory and optimal control theory. Section 4 provides a brief survey on numerical dynamic programming. Section 5 surveys the experimental and econometric literature that uses dynamic programming to construct empirical models economic behavior.},
annote = {In this book, Bellman presents the notion of Markov Decision Processes (MDP)s and their use in control theory.},
author = {Bellman, R},
booktitle = {Princeton University Press Princeton New Jersey},
doi = {10.1108/eb059970},
isbn = {978-0-691-07951-6},
issn = {00029092},
pages = {342},
title = {{Dynamic Programming}},
url = {http://ajae.oxfordjournals.org/cgi/doi/10.2307/1241949},
volume = {70},
year = {1957}
}
@article{Tamar2017,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {1602.02867},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tamar et al. - 2017 - Value iteration networks.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {value iteration},
mendeley-tags = {value iteration},
number = {Nips},
pages = {4949--4953},
pmid = {172808},
title = {{Value iteration networks}},
year = {2017}
}
@article{Jaderberg2016,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880$\backslash${\%} expert human performance, and a challenging suite of first-person, three-dimensional $\backslash$emph{\{}Labyrinth{\}} tasks leading to a mean speedup in learning of 10{\$}\backslashtimes{\$} and averaging 87$\backslash${\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {1611.05397},
author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.05397},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2016 - Reinforcement Learning with Unsupervised Auxiliary Tasks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--14},
pmid = {23459267},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
url = {http://arxiv.org/abs/1611.05397},
year = {2016}
}
@article{Lau2012,
abstract = {In this paper, we propose to guide reinforcement learning (RL) with expert coordination knowledge for multi-agent problems managed by a central controller. The aim is to learn to use expert coordination knowledge to restrict the joint action space and to direct exploration towards more promising states, thereby improving the overall learning rate. We model such coordination knowledge as constraints and propose a two-level RL system that utilizes these constraints for online applications. Our declarative approach towards specifying coordination in multi-agent learning allows knowl-edge sharing between constraints and features (basis func-tions) for function approximation. Results on a soccer game and a tactical real-time strategy game show that coordi-nation constraints improve the learning rate compared to using only unary constraints. The two-level RL system also outperforms existing single-level approach that utilizes joint action selection via coordination graphs.},
author = {Lau, Qiangfeng Peter and Lee, Mong Li and Hsu, Wynne},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lau, Lee, Hsu - 2012 - Coordination Guided Reinforcement Learning.pdf:pdf},
isbn = {0-9817381-1-7, 978-0-9817381-1-6},
journal = {In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1. International Foundation for Autonomous Agents and Multiagent Systems},
keywords = {Control Methods,Experimentation Keywords Reinforcement learning,I28 [Artificial Intelligence],Learning,Performance,Problem Solving,Search General Terms Algorithms,coordination constraints,factored Markov decision process,guiding exploration},
pages = {215--222},
title = {{Coordination Guided Reinforcement Learning}},
url = {http://www.ifaamas.org/Proceedings/aamas2012/papers/1B{\_}1.pdf},
year = {2012}
}
@phdthesis{Watkins1989,
author = {Watkins, Christopher J.C.H.},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins - 1989 - Learning from delayed rewards.pdf:pdf},
school = {King's College},
title = {{Learning from delayed rewards}},
year = {1989}
}
@article{Thrun1993,
abstract = {Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failures—namely, a systematic overestimation of utility values. Using Watkins' Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings.},
author = {Thrun, Sebastian and Schwartz, Anton},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun, Schwartz - 1993 - Issues in Using Function Approximation for Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 4th Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum},
pages = {1--9},
title = {{Issues in Using Function Approximation for Reinforcement Learning}},
year = {1993}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Watkins, Christopher J.C.H. and Dayan, Peter},
doi = {10.1023/A:1022676722315},
eprint = {1412.3409},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Technical Note Q-Learning.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {15730565},
journal = {Machine Learning},
keywords = {(Formula presented.)-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
pmid = {7761831},
title = {{Technical Note: Q-Learning}},
volume = {8},
year = {1992}
}
@article{Greensmith2004,
abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
annote = {This article proposes various baselines to reduce varience in policy gradient methods without introducing variance.},
author = {Greensmith, Evan and Bartlett, Pl and Baxter, J},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greensmith, Bartlett, Baxter - 2004 - Variance reduction techniques for gradient estimates in reinforcement learning.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {The Journal of Machine Learning {\ldots}},
keywords = {actor-critic,baseline,gpomdp,policy gradient,reinforcement learning},
pages = {1471--1530},
title = {{Variance reduction techniques for gradient estimates in reinforcement learning}},
url = {http://dl.acm.org/citation.cfm?id=1044710},
volume = {5},
year = {2004}
}
@article{Henderson2017,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:pdf},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Henderson2017a,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Vodopivec2017,
author = {Vodopivec, Tom},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vodopivec - 2017 - On Monte Carlo Tree Search and Reinforcement Learning.pdf:pdf},
title = {{On Monte Carlo Tree Search and Reinforcement Learning}},
volume = {60},
year = {2017}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf:pdf},
isbn = {9781424469178},
issn = {1701.07274},
pages = {1--16},
pmid = {25719670},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240},
year = {2017}
}
@article{Lowe2017,
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
archivePrefix = {arXiv},
arxivId = {1706.02275},
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
eprint = {1706.02275},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe et al. - 2017 - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf:pdf},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}},
url = {http://arxiv.org/abs/1706.02275},
year = {2017}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Vodopivec2017a,
author = {Vodopivec, Tom},
title = {{On Monte Carlo Tree Search and Reinforcement Learning}},
volume = {60},
year = {2017}
}
@article{Jaderberg2016a,
abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local infor- mation. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
archivePrefix = {arXiv},
arxivId = {1608.05343},
author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1608.05343},
issn = {1938-7228},
journal = {arXiv},
number = {Nips},
pages = {1608.05343v1 [cs.LG]},
title = {{Decoupled Neural Interfaces using Synthetic Gradients}},
year = {2016}
}
@article{Pathak2017,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Prediction.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
volume = {2017-July},
year = {2017}
}
@article{Czarnecki2017,
abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
archivePrefix = {arXiv},
arxivId = {1703.00522},
author = {Czarnecki, Wojciech Marian and {\'{S}}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
eprint = {1703.00522},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Czarnecki et al. - 2017 - Understanding Synthetic Gradients and Decoupled Neural Interfaces.pdf:pdf},
issn = {1938-7228},
title = {{Understanding Synthetic Gradients and Decoupled Neural Interfaces}},
url = {http://arxiv.org/abs/1703.00522},
year = {2017}
}
@inproceedings{Pathak2017a,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
isbn = {9781538607336},
issn = {21607516},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
volume = {2017-July},
year = {2017}
}
@article{Wender2012,
author = {Wender, Stefan and Watson, Ian},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wender, Watson - 2012 - Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft Broodwar.pdf:pdf},
isbn = {9781467311946},
keywords = {broodwar,qlearning,sarsa,starcraft},
mendeley-tags = {broodwar,qlearning,sarsa,starcraft},
pages = {402--408},
title = {{Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft : Broodwar}},
year = {2012}
}
@article{Griffiths1999,
abstract = {Technology his always played a role in the development of gambling practices and will continue to play a critical role in the development of increased gambling opportunities (e.g., internet gambling). Although technological advance his long been associated with improved gambling opportunities, there is little written in the literature explicitly pointing out this link and its implications for problem gamblers. This paper therefore reviews this situation and examines the technological implications of situational and structural characteristics paying particular attention to slot machine gambling as there has been more empirical work on this type of gambling than any other technological form. The impact of technology on the sociability of gambling is also examined followed by a more speculative evolution of internet gambling as an area of potential concern.},
author = {Griffiths, Mark},
doi = {http://dx.doi.org/10.1023/A:1023053630588},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths - 1999 - Gambling Technologies Prospects for Problem Gambling.pdf:pdf},
isbn = {1050-5350 (Print)},
issn = {1573-3602},
journal = {Journal of gambling studies},
number = {3},
pages = {265--283},
pmid = {12766464},
title = {{Gambling Technologies: Prospects for Problem Gambling.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12766464},
volume = {15},
year = {1999}
}
@article{Commission2016,
author = {Commission, Gambling},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Commission - 2016 - Virtual currencies , eSports and social gaming – discussion paper(2).pdf:pdf},
keywords = {Virtual currencies eSports and social gaming  disc},
number = {August},
pages = {1--12},
title = {{Virtual currencies , eSports and social gaming – discussion paper}},
year = {2016}
}
@article{Griffiths2000,
abstract = {It has been noted that adolescents may be more susceptible to pathological gambling. Not only is it usually illegal, but it appears to be related to high levels of problem gambling and other delinquent activities such as illicit drug taking and alcohol abuse. This paper examines risk factors not only in adolescent gambling but also in videogame playing (which shares many similarities with gambling). There appear to be three main forms of adolescent gambling that have been widely researched. Adolescent gambling activities and general risk factors in adolescent gambling are provided. As well, the influence of technology on adolescents in the form of both videogames and the Internet are examined. It is argued that technologically advanced forms of gambling may be highly appealing to adolescents.},
author = {Griffiths, Mark D and Wood, R T},
doi = {10.1023/A:1009433014881},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths, Wood - 2000 - Risk factors in adolescence the case of gambling, videogame playing, and the internet.pdf:pdf},
isbn = {1050-5350},
issn = {1050-5350},
journal = {Journal of Gambling Studies},
keywords = {addiction,adolescence,adolescent gambling is a,be related to high,but it appears to,gambling,internet,is it usually illegal,levels of prob-,major problem in society,not only,problem adolescent gambling,today,videogames},
number = {2-3},
pages = {199--225},
pmid = {14634313},
title = {{Risk factors in adolescence: the case of gambling, videogame playing, and the internet.}},
volume = {16},
year = {2000}
}
@article{Commission2016a,
author = {Commission, Gambling},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Commission - 2016 - Virtual currencies , eSports and social gaming – discussion paper.pdf:pdf},
keywords = {Virtual currencies eSports and social gaming  disc},
number = {August},
pages = {1--12},
title = {{Virtual currencies , eSports and social gaming – discussion paper}},
year = {2016}
}
@article{IabInternetAdvertisingBureauUK2011,
author = {{(Iab) Internet Advertising Bureau UK}},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/(Iab) Internet Advertising Bureau UK - 2011 - Gaming Britain A Nation United by Digital Play.pdf:pdf},
title = {{Gaming Britain: A Nation United by Digital Play}},
year = {2011}
}
@article{Deasis2017,
abstract = {Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD($\lambda$) elegantly unifies one-step TD prediction with Monte Carlo meth-ods through the use of eligibility traces and the trace-decay parameter $\lambda$. Currently, there are a multitude of algorithms that can be used to per-form TD control, including Sarsa, Q-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be ex-tended across multiple time steps to achieve bet-ter performance. Each of these algorithms is seemingly distinct, and no one dominates the oth-ers for all problems. In this paper, we study a new multi-step action-value algorithm called Q($\sigma$) which unifies and generalizes these exist-ing algorithms, while subsuming them as special cases. A new parameter, $\sigma$, is introduced to al-low the degree of sampling performed by the al-gorithm at each step during its backup to be con-tinuously varied, with Sarsa existing at one ex-treme (full sampling), and Expected Sarsa exist-ing at the other (pure expectation). Q($\sigma$) is gen-erally applicable to both on-and off-policy learn-ing, but in this work we focus on experiments in the on-policy case. Our results show that an in-termediate value of $\sigma$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater per-formance.},
author = {{De Asis}, Kristopher and Ca, Kldeasis@ualberta and Hernandez-Garcia, J Fernando and Ca, Jfhernan@ualberta and Holland, G Zacharias and Sutton, Richard S},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Asis et al. - 2017 - Multi-step Reinforcement Learning A Unifying Algorithm.pdf:pdf},
keywords = {Unifying,multi-step},
mendeley-tags = {Unifying,multi-step},
title = {{Multi-step Reinforcement Learning: A Unifying Algorithm}},
url = {https://arxiv.org/pdf/1703.01327.pdf},
year = {2017}
}
@article{Hewitt2013,
abstract = {Other topics in this issue include coaching children with autism, teaching styles, serve correction and information on an exciting new tennis app. Since the launch of CSSR in English in 1992, the ITF has published over 560 articles from contributors of more than 35 different nationalities. Today the review is produced 3 times per year in the 3 official ITF languages of English, Spanish and French and made available free of charge on the ITF coaching web at http://www.itftennis.com/coaching/sportsscience. The 2012 launch of, 'Biomechanics for Advanced Tennis' as an e-book has proven to be very successful in the new electronic format. Interested readers can purchase their copy at: http://www.amazon.es/ITF-Biomechanics-Advanced-Tennis-ebook/dp/ B00A79U7MK The ITF Tennis iCoach website remains at the forefront of online coach education, with up to date and current research available to coaches across the world. For just {\$}30 per year you can keep up to date with then most current tennis specific coaching information. Please click on the following link for a tour of the site. www.tennisicoach.com In late 2013, the Tennis iCoach will be re-launched with mobile and tablet PC support on both Android and Apple devices. Version 3.0 of the site will also offer HD quality video, a new navigation and search system, as well as a range of new features that will enhance the user engagement and learning experience for users. The launch is set for autumn 2013 and will be officially released at the Worldwide conference in Mexico.},
author = {Hewitt, Mitchell and Aus, Kenneth Edwards and Pluim, Babette and Smit, Claudia and Driessen, Dorian and Oskam, Sandy and Aus, Geoff Quinlan},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hewitt et al. - 2013 - CONTENTS ISSUE 59 The Official Coaching and Sport Science Publication of the International Tennis Federation.pdf:pdf},
issn = {2225-4757},
journal = {ITF Coaching and Sport Science Review},
number = {59},
title = {{CONTENTS ISSUE 59 The Official Coaching and Sport Science Publication of the International Tennis Federation}},
url = {www.itftennis.com/coaching/sportsscience},
year = {2013}
}
@article{Klaassen2001,
abstract = {This article tests whether points in tennis are independent and/or identically distributed (iid). We model the probability of winning a point on service and show that points are neither independent nor identically distributed: winning the previous point has a positive ef- fect on winning the current point, and at ‘important' points it is more difficult for the server to win the point than at less important points. Furthermore, the weaker a player, the stronger are these effects. De- viations from iid are, however, small and hence the iid hypothesis will still provide a good approximation in many cases. The results are based on a large panel of matches played atWimbledon 1992—1995, in total almost 90,000 points. Our panel data model takes account of the binary character of the dependent variable, uses random effects to cap- ture theunobservedpartofaplayer's quality, andincludesdynamic explanatory variables.},
author = {Klaassen, Franc J. G. M. and Magnus, Jan R.},
doi = {10.1198/016214501753168217},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {binary choice,dependence,dynamic panel data,linear probability model,nonidentical distribution,random effects},
number = {454},
pages = {500--509},
title = {{Are Points in Tennis Independent and Identically Distributed? Evidence From a Dynamic Binary Panel Data Model}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753168217},
volume = {96},
year = {2001}
}
@article{Knottenbelt2012,
abstract = {Tennis features among the most popular sports internationally, with professional matches played for 11 months of the year around the globe. The rise of the internet has stimulated a dramatic increase in tennis-related financial activity, much of which depends on quantitative models. This paper presents a hierarchical Markov model which yields a pre-play estimate of the probability of each player winning a professional singles tennis match. Crucially, the model provides a fair basis of comparison between players by analysing match statistics for opponents that both players have encountered in the past. Subsequently the model exploits elements of transitivity to compute the probability of each player winning a point on their serve, and hence the match. When evaluated using a data set of historical match statistics and bookmakers odds, the model yields a 3.8{\%} return on investment over 2173 ATP matches played on a variety of surfaces during 2011. ?? 2012 Elsevier Ltd. All rights reserved.},
author = {Knottenbelt, William J. and Spanias, Demetris and Madurska, Agnieszka M.},
doi = {10.1016/j.camwa.2012.03.005},
issn = {08981221},
journal = {Computers and Mathematics with Applications},
keywords = {Sport,Stochastic modelling,Tennis},
number = {12},
pages = {3820--3827},
title = {{A common-opponent stochastic model for predicting the outcome of professional tennis matches}},
volume = {64},
year = {2012}
}
@article{Pfeiffer2010,
abstract = {The evaluation of the structure of sports performance is one of the important functions of diagnostics in competitive sport. Especially in game sports, it is important to obtain diagnostic information on competition because of the interactive process between the two teams or players. This interaction cannot be simulated or replicated in training or test situations. When it comes to table tennis, performance diagnostics offers many different techniques and methods to analyze a game. In this context, problems mostly occur in ...},
author = {Pfeiffer, Mark and Zhang, Hui and Hohmann, Andreas},
doi = {10.1260/1747-9541.5.2.205},
issn = {1747-9541},
journal = {International Journal of Sports Science and Coaching},
number = {2},
pages = {205--222},
title = {{A Markov chain model of elite table tennis competition}},
volume = {5},
year = {2010}
}
@article{Barnett2005,
abstract = {Tennis features among the most popular sports internationally, with professional matches played for 11 months of the year around the globe. The rise of the internet has stimulated a dramatic increase in tennis-related financial activity, much of which depends on quantitative models. This paper presents a hierarchical Markov model which yields a pre-play estimate of the probability of each player winning a professional singles tennis match. Crucially, the model provides a fair basis of comparison between players by analysing match statistics for opponents that both players have encountered in the past. Subsequently the model exploits elements of transitivity to compute the probability of each player winning a point on their serve, and hence the match. When evaluated using a data set of historical match statistics and bookmakers odds, the model yields a 3.8{\%} return on investment over 2173 ATP matches played on a variety of surfaces during 2011. ?? 2012 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1011.1761},
author = {Barnett, Tristan and Brown, Alan and Clarke, Stephen R. and Caron, Francois and Doucet, Arnaud and Glickman, Mark E Me Mark and Knottenbelt, William J. and Spanias, Demetris and Madurska, Agnieszka M. and Krajowski-kukiel, Maciej and Madurska, Agnieszka M. and McHale, Ian and Morton, Alex and Ralph, B Y and Bradley, Allan and Terry, Milton E and Brent, R P and Dekker, T J and Rios, Luis Miguel and Sahinidis, Nikolaos V. and Herbrich, Ralf and Minka, Tom and Graepel, Thore and Cattelan, Manuela and Varin, Cristiano},
doi = {10.1093/imaman/dpi001},
eprint = {1011.1761},
isbn = {1049-5258},
issn = {08981221},
journal = {Boston University},
keywords = {Australian Open,Betting,Bradley-Terry model,Derivative-free algorithms,Direct search methods,Excel,Gambling,Logit,Ranking evaluation,Scoring systems,Sport,Stochastic modelling,Surrogate models,Tennis,approximate,average process,bayesian estimation,bayesian learning,bradley,cumulative logit model,dynamic difficulty adjustment,exponentially weighted moving,index betting,markov chain,match-making,paired comparisons,sport,sports tournaments,tennis,terry model},
number = {3},
pages = {1--6},
pmid = {18268290},
title = {{Developing a Model that Reflects Outcomes of Tennis Matches}},
url = {http://research.microsoft.com/apps/pubs/default.aspx?id=67956{\%}5Cnhttp://dx.doi.org/10.1016/j.ijforecast.2010.04.004{\%}5Cnhttp://glicko.net/glicko/glicko2.pdf{\%}5Cnhttp://www.echecsonline.net/joueurs/doc/The{\_}Glicko{\_}system.pdf{\%}5Cnhttp://www.glicko.net/research/},
volume = {48},
year = {2005}
}
@article{Newtont2006,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Abstract. The probability of winning a game, set, match, or single elimination tournament in tennis is computed using Monte Carlo simulations based on each player's probability of winning a point on serve, which can be held constant or varied from point to point, game to game, or match to match. The theory, described in Newton and Keller [Stud. Appl. Math., 114 (2005), pp. 241-269], is based on the assumption that points in tennis are independent, identically distributed (i.i.d.) random variables. This is used as a baseline to compare with the simulations, which under similar circumstances are shown to converge quickly to the analytical curves in accordance with the weak law of large numbers. The concept of the importance of a point, game, and set to winning a match is described based on conditional probabilities and is used as a starting point to model non-i.i.d. effects, allowing each player to vary, from point to point, his or her probability of winning on serve. Several non-i.i.d. models are investigated, including the "hot-hand-effect," in which we increase each player's probability of winning a point on serve on the next point after a point is won. The "back to-the-wall" effect is modeled by increasing each player's probability of winning a point on serve on the next point after a point is lost. In all cases, we find that the results provided by the theoretical curves based on the i.i.d. assumption are remarkably robust and accurate, even when relatively strong non-i.i.d. effects are introduced. We end by showing examples of tournament predictions from the 2002 men's and women's U.S. Open draws based on the Monte Carlo simulations. We also describe Arrow's impossibility theorem and discuss its relevance with regard to sports ranking systems, and we argue for the development of probability-based ranking systems as a way to soften its consequences.},
author = {Newtont, Paul K and Aslamt, Kamran},
doi = {10.1137/050640278},
isbn = {00361445},
issn = {0036-1445},
journal = {SIAM Review SIAM REVIEW Society for Industrial and Applied Mathematics},
keywords = {60J20,65Q05,91A60,91B12,Arrow's theorem AMS subject classifications 65C05,Monte Carlo Method,non-iid effects,probabilistic ranking systems,tennis},
number = {4},
pages = {722--742},
pmid = {25468915},
title = {{Monte Carlo Tennis*}},
url = {http://www.jstor.org/stable/20453873{\%}5Cnhttp://about.jstor.org/terms},
volume = {48},
year = {2006}
}
@article{Sicart2008,
abstract = {This article defines game mechanics in relation to rules and challenges. Game mechanics are methods invoked by agents for interacting with the game world. I apply this definition to a comparative analysis of the games Rez, Every Extend Extra and Shadow of the Colossus that will show the relevance of a formal definition of game mechanics.},
author = {Sicart, Miguel},
doi = {1604-7982},
isbn = {1604-7982},
issn = {16047982},
journal = {Game Studies},
keywords = {Challenges,Game design,Game mechanics,Game research,Rules},
number = {2},
title = {{Defining game mechanics}},
volume = {8},
year = {2008}
}
@incollection{Nelson2007,
abstract = {Game generation systems perform automated, intelligent design of games (i.e. videogames, boardgames), reasoning about both the abstract rule system of the game and the visual realization of these rules. Although, as an instance of the problem of creative design, game generation shares some common research themes with other creative AI systems such as story and art generators, game generation extends such work by having to reason about dynamic, playable artifacts. Like AI work on creativity in other domains, work on game generation sheds light on the human game design process, offering opportunities to make explicit the tacit knowledge involved in game design and test game design theories. Finally, game generation enables new game genres which are radically customized to specific players or situations; notable examples are cell phone games customized for particular users and newsgames providing commentary on current events. We describe an approach to formalizing game mechanics and generating games using those mechanics, using WordNet and ConceptNet to assist in performing common-sense reasoning about game verbs and nouns. Finally, we demonstrate and describe in detail a prototype that designs micro-games in the style of Nintendo's WarioWare series.},
address = {Berlin, Heidelberg},
author = {Nelson, Mark J. and Mateas, Michael},
booktitle = {AI*IA 2007: Artificial Intelligence and Human-Oriented Computing},
doi = {10.1007/978-3-540-74782-6_54},
isbn = {978-3-540-74781-9},
issn = {03029743},
pages = {626--637},
publisher = {Springer Berlin Heidelberg},
title = {{Towards Automated Game Design}},
url = {http://www.springerlink.com/content/64921n6401026242/ http://link.springer.com/10.1007/978-3-540-74782-6{\_}54},
year = {2007}
}
@inproceedings{Zook2014,
abstract = {Game designs often center on the game mechanics—rules governing the logical evolution of the game. We seek to de- velop an intelligent system that generates computer games and assists humans in designing games. As first steps to- wards this goal we present a composable and cross-domain representation for game mechanics that draws from AI plan- ning action representations. We use a constraint solver to generate mechanics subject to design requirements on the form of those mechanics—what they do in the game. A planner takes a set of generated mechanics and tests whether those mechanics meet playability requirements—controlling how mechanics function in a game to affect player behav- ior. We demonstrate our system by modeling and generat- ing mechanics in a role-playing game, platformer game, and combined role-playing-platformer game.},
author = {Zook, Alexander and Riedl, Mo},
booktitle = {Proceedings of the 2014 Foundations of Digital Games Workshop on Procedural Content Generation in Games},
keywords = {game design,game mechanics,procedural content generation},
title = {{Generating and Adapting Game Mechanics}},
url = {http://www.cc.gatech.edu/{~}riedl/pubs/pcg14.pdf},
year = {2014}
}
@article{Lindley2008,
abstract = {Schema theory provides a foundation for the analysis of game play patterns created by players during their interaction with a game. Schema models derived from the analysis of play provide a rich explanatory framework for the cognitive processes underlying game play, as well as detailed hypotheses for the hierarchical structure of pleasures and rewards motivating players. Game engagement is accounted for as a process of schema selection or development, while immersion is explained in terms of levels of attentional demand in schema execution. However, schemas may not only be used to describe play, but might be used actively as cognitive models within a game engine. Predesigned schema models are knowledge representations constituting anticipated or desired learned cognitive outcomes of play. Automated analysis of player schemas and comparison with predesigned target schemas can provide a foundation for a game engine adapting or tuning game mechanics to achieve specific effects of engagement, immersion, and cognitive skill acquisition by players. Hence, schema models may enhance the play experience as well as provide a foundation for achieving explicitly represented pedagogical or therapeutic functions of games.},
author = {Lindley, Craig a. and Sennersten, Charlotte C.},
doi = {10.1155/2008/216784},
isbn = {86905-901-7},
issn = {1687-7047},
journal = {International Journal of Computer Games Technology},
pages = {1--7},
title = {{Game Play Schemas: From Player Analysis to Adaptive Game Mechanics}},
url = {http://www.hindawi.com/journals/ijcgt/2008/216784/},
volume = {2008},
year = {2008}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
