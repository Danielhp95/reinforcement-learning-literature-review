@article{Abdallah2016,
abstract = {Q-learning (QL) is a popular reinforcement learning algorithm that is guaranteed to converge to op-timal policies in Markov decision processes. However, QL exhibits an artifact: in expectation, the effective rate of updating the value of an action depends on the probability of choosing that action. In other words, there is a tight coupling between the learning dynamics and underlying execution policy. This coupling can cause performance degradation in noisy non-stationary environments. Here, we introduce Repeated Update Q-learning (RUQL), a learning algorithm that resolves the undesirable artifact of Q-learning while maintaining simplicity. We analyze the similarities and differences between RUQL, QL, and the closest state-of-the-art algorithms theoretically. Our analysis shows that RUQL maintains the convergence guarantee of QL in stationary environments, while relaxing the coupling between the execution policy and the learning dynamics. Experimental results confirm the theoretical insights and show how RUQL outperforms both QL and the closest state-of-the-art algorithms in noisy non-stationary environments.},
author = {Abdallah, Sherief and Org, Shario@ieee and Kaisers, Michael and Nl, Kaisers@cwi},
file = {:home/sarios/papers/addressing-environment-non-stationarity-by-repeating-q-learning-updates.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {multi-agent learning,non-stationary environ-,q-learning,qlearning,reinforcement learning},
mendeley-tags = {qlearning},
pages = {1--31},
title = {{Addressing Environment Non-Stationarity by Repeating Q-learning Updates}},
url = {http://www.jmlr.org/papers/volume17/14-037/14-037.pdf},
volume = {17},
year = {2016}
}
@article{browne2012survey,
author = {Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
file = {:home/sarios/papers/mcts-survey-master.pdf:pdf},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {mcts},
mendeley-tags = {mcts},
number = {1},
pages = {1--43},
publisher = {IEEE},
title = {{A survey of {\{}Monte Carlo Tree Search{\}} methods}},
volume = {4},
year = {2012}
}
@article{Sutton1991,
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S.},
doi = {10.1145/122344.122377},
eprint = {arXiv:1011.1669v3},
file = {:home/sarios/papers/dyna-an-integrated-architechture-for-learning-planning-and-reacting.pdf:pdf},
isbn = {1-55860-141-4},
issn = {01635719},
journal = {ACM SIGART Bulletin},
number = {4},
pages = {160--163},
pmid = {15003161},
title = {{Dyna, an integrated architecture for learning, planning, and reacting}},
url = {http://portal.acm.org/citation.cfm?doid=122344.122377},
volume = {2},
year = {1991}
}
@article{Soemers2014,
abstract = {This thesis describes how Monte-Carlo Tree Search (MCTS) can be applied to perform tac-tical planning for an intelligent agent playing full games of StarCraft: Brood War. StarCraft is a Real-Time Strategy game, which has a large state-space, is played in real-time, and com-monly features two opposing players, capable of acting simultaneously. Using the MCTS al-gorithm for tactical planning is shown to in-crease the performance of the agent, compared to a scripted approach, when competing on a bot ladder. A combat model, based on Lanch-ester's Square Law, is described, and shown to achieve another gain in performance when used in Monte-Carlo simulations as replacement for a heuristic linear model. Finally, the MAST enhancement to the Playout Policy of MCTS is described, but it is found not to have a signifi-cant impact on the agent's performance.},
author = {Soemers, Dennis},
file = {:home/sarios/papers/tactical-planning-using-mcts-in-game-of-starcraft.pdf:pdf},
keywords = {lanchester,monte-carlo tree,s square law,search,starcraft},
pages = {12},
title = {{Tactical Planning Using MCTS in the Game of StarCraft}},
url = {https://project.dke.maastrichtuniversity.nl/games/files/bsc/Soemers{\_}BSc-paper.pdf},
year = {2014}
}
@inproceedings{Tijsma2017,
abstract = {â€”Balancing the ratio between exploration and ex-ploitation is an important problem in reinforcement learning. This paper evaluates four different exploration strategies com-bined with Q-learning using random stochastic mazes to inves-tigate their performances. We will compare: UCB-1, softmax, -greedy, and pursuit. For this purpose we adapted the UCB-1 and pursuit strategies to be used in the Q-learning algorithm. The mazes consist of a single optimal goal state and two suboptimal goal states that lie closer to the starting position of the agent, which makes efficient exploration an important part of the learning agent. Furthermore, we evaluate two different kinds of reward functions, a normalized one with rewards between 0 and 1, and an unnormalized reward function that penalizes the agent for each step with a negative reward. We have performed an extensive grid-search to find the best parameters for each method and used the best parameters on novel randomly generated maze problems of different sizes. The results show that softmax exploration outperforms the other strategies, although it is harder to tune its temperature parameter. The worst performing exploration strategy is -greedy.},
author = {Tijsma, Arryon D and Drugan, Madalina M and Wiering, Marco A},
booktitle = {2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016},
doi = {10.1109/SSCI.2016.7849366},
file = {:home/sarios/papers/comparing-exploration-strategies-for-qlearning-random-stochastic-maces.pdf:pdf},
isbn = {9781509042401},
title = {{Comparing exploration strategies for Q-learning in random stochastic mazes}},
year = {2017}
}
@article{Kaisers2010,
abstract = {Multi-agent learning is a crucial method to control or find solutions for systems, in which more than one entity needs to be adaptive. In todays interconnected world, such sys- tems are ubiquitous in many domains, including auctions in economics, swarm robotics in computer science, and politics in social sciences. Multi-agent learning is inherently more complex than single-agent learning and has a relatively thin theoretical framework supporting it. Recently, multi-agent learning dynamics have been linked to evolutionary game theory, allowing the interpretation of learning as an evolu- tion of competing policies in the mind of the learning agents. The dynamical system from evolutionary game theory that has been linked to Q-learning predicts the expected behav- ior of the learning agents. Closer analysis however allows for two interesting observations: the predicted behavior is not always the same as the actual behavior, and in case of deviation, the predicted behavior is more desirable. This discrepancy is elucidated in this article, and based on these new insights Frequency Adjusted Q- (FAQ-) learning is pro- posed. This variation of Q-learning perfectly adheres to the predictions of the evolutionary model for an arbitrarily large part of the policy space. In addition to the theoretical dis- cussion, experiments in the three classes of two-agent two- action games illustrate the superiority of FAQ-learning.},
author = {Kaisers, Michael and Tuyls, Karl},
doi = {10.11451838206},
file = {:home/sarios/papers/frequency-adjusted-multi-agent-q-learning.pdf:pdf},
isbn = {978-0-9826571-1-9},
issn = {15582914},
journal = {Learning},
keywords = {dynamics,evolutionary game theory,multi agent learning,q learning,qlearning,replicator},
mendeley-tags = {qlearning},
pages = {309--316},
title = {{Frequency Adjusted Multi-agent Q-learning}},
url = {http://portal.acm.org/citation.cfm?id=1838250},
year = {2010}
}
@article{Guzdial2017,
abstract = {Intelligent agents need to be able to make predic-tions about their environment. In this work we present a novel approach to learn a forward simula-tion model via simple search over pixel input. We make use of a video game, Super Mario Bros., as an initial test of our approach as it represents a physics system that is significantly less complex than real-ity. We demonstrate the significant improvement of our approach in predicting future states compared with a baseline CNN and apply the learned model to train a game playing agent. Thus we evaluate the algorithm in terms of the accuracy and value of its output model.},
author = {Guzdial, Matthew and Li, Boyang and Riedl, Mark O},
doi = {10.24963/ijcai.2017/518},
file = {:home/sarios/papers/game-engine-learning-from-video.pdf:pdf},
isbn = {9780999241103},
journal = {International Conference on Artificial Intelligence (IJCAI)},
title = {{Game Engine Learning from Video}},
url = {http://www.cc.gatech.edu/{~}riedl/pubs/ijcai17.pdf},
year = {2017}
}
@book{Sutton1998,
abstract = {Norton, B., {\&} Toohey, K. (Eds). (2004). Critical pedagogies and language learning. New York: Cambridge University Press. (362pp).},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
isbn = {0262193981},
issn = {0959-4388},
pages = {331},
pmid = {7888773},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@article{Montague1999,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Montague, P.Read},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}
@article{Montague1999a,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Montague, P.Read},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}
@article{Borkar1988,
abstract = {This paper develops a new framework for the study of Markov decision processes in which the control problem is viewed as an optimization problem on the set of canonically induced measures on the trajectory space of the joint state and control process. This set is shown to be compact convex. One then associates with each of the usual cost criteria (infinite horizon discounted cost, finite horizon, control up to an exit time) a naturally defined occupation measure such that the cost is an integral of some function with respect to this measure. These measures are shown to form a compact convex set whose extreme points are characterized. Classical results about existence of optimal strategies are recovered from this and several applications to multicriteria and constrained optimization problems are briefly indicated.},
author = {Borkar, Vivek S.},
doi = {10.1007/BF00353877},
issn = {01788051},
journal = {Probability Theory and Related Fields},
number = {4},
pages = {583--602},
pmid = {20840904},
title = {{A convex analytic approach to Markov decision processes}},
volume = {78},
year = {1988}
}
@book{Bertsekas2007,
abstract = {A major revision of the second volume of a textbook on the far-ranging algorithmic methododogy of Dynamic Programming, which can be used for optimal control, Markovian decision problems, planning and sequential decision making under uncertainty, and discrete/combinatorial optimization. The second volume is oriented towards mathematical analysis and computation, and treats infinite horizon problems extensively. New features of the 3rd edition are: 1) A major enlargement in size and scope: the length has increased by more than 50{\%}, and most of the old material has been restructured and/or revised. 2) Extensive coverage (more than 100 pages) of recent research on simulation-based approximate dynamic programming (neuro-dynamic programming), which allow the practical application of dynamic programming to large and complex problems. 3) An in-depth development of the average cost problem (more than 100 pages), including a full analysis of multichain problems, and an extensive analysis of infinite-spaces problems. 4) An introduction to infinite state space stochastic shortest path problems. 5) Expansion of the theory and use of contraction mappings in infinite state space problems and in neuro-dynamic programming. 6) A substantive appendix on the mathematical measure-theoretic issues that must be addressed for a rigorous theory of stochastic dynamic programming. Much supplementary material can be found in the book's web page: http://www.athenasc.com/dpbook.html},
author = {Bertsekas, Dimitri P},
booktitle = {Journal of the Operational Research Society},
doi = {10.1057/jors.1996.103},
isbn = {1886529264},
number = {6},
pages = {543},
title = {{Dynamic Programming and Optimal Control, Vol. II}},
url = {http://portal.acm.org/citation.cfm?id=1396348},
volume = {2},
year = {2007}
}
@book{Bellman1957,
abstract = {Dynamic Programming is a recursive method for solving sequential decision problems (hereafter abbre- viated as SDP). Also known as backward induction, it is used to find optimal decision rules in games against nature and subgame perfect equilibria of dynamic multi-agent games, and competitive equilib- ria in dynamic economic models. Dynamic programming has enabled economists to formulate and solve a huge variety of problems involving sequential decision making under uncertainty, and as a result it is now widely regarded as the single most important tool in economics. Section 2 provides a brief history of dynamic programming. Section 3 discusses some of the main theoretical results underlying dynamic programming, and its relation to game theory and optimal control theory. Section 4 provides a brief survey on numerical dynamic programming. Section 5 surveys the experimental and econometric literature that uses dynamic programming to construct empirical models economic behavior.},
annote = {In this book, Bellman presents the notion of Markov Decision Processes (MDP)s and their use in control theory.},
author = {Bellman, R},
booktitle = {Princeton University Press Princeton New Jersey},
doi = {10.1108/eb059970},
isbn = {978-0-691-07951-6},
issn = {00029092},
pages = {342},
title = {{Dynamic Programming}},
url = {http://ajae.oxfordjournals.org/cgi/doi/10.2307/1241949},
volume = {70},
year = {1957}
}
@article{Tamar2017,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {1602.02867},
file = {:home/sarios/papers/value-iteration-networks.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {Nips},
pages = {4949--4953},
pmid = {172808},
title = {{Value iteration networks}},
year = {2017}
}
@article{Jaderberg2016,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880$\backslash${\%} expert human performance, and a challenging suite of first-person, three-dimensional $\backslash$emph{\{}Labyrinth{\}} tasks leading to a mean speedup in learning of 10{\$}\backslashtimes{\$} and averaging 87$\backslash${\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {1611.05397},
author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.05397},
file = {:home/sarios/papers/rl-with-unsupervised-auxiliary-tasks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--14},
pmid = {23459267},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
url = {http://arxiv.org/abs/1611.05397},
year = {2016}
}
@article{Lau2012,
abstract = {In this paper, we propose to guide reinforcement learning (RL) with expert coordination knowledge for multi-agent problems managed by a central controller. The aim is to learn to use expert coordination knowledge to restrict the joint action space and to direct exploration towards more promising states, thereby improving the overall learning rate. We model such coordination knowledge as constraints and propose a two-level RL system that utilizes these constraints for online applications. Our declarative approach towards specifying coordination in multi-agent learning allows knowl-edge sharing between constraints and features (basis func-tions) for function approximation. Results on a soccer game and a tactical real-time strategy game show that coordi-nation constraints improve the learning rate compared to using only unary constraints. The two-level RL system also outperforms existing single-level approach that utilizes joint action selection via coordination graphs.},
author = {Lau, Qiangfeng Peter and Lee, Mong Li and Hsu, Wynne},
file = {:home/sarios/papers/coordination-guided-reinforcement-learning.pdf:pdf},
isbn = {0-9817381-1-7, 978-0-9817381-1-6},
journal = {In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1. International Foundation for Autonomous Agents and Multiagent Systems},
keywords = {Control Methods,Experimentation Keywords Reinforcement learning,I28 [Artificial Intelligence],Learning,Performance,Problem Solving,Search General Terms Algorithms,coordination constraints,factored Markov decision process,guiding exploration},
pages = {215--222},
title = {{Coordination Guided Reinforcement Learning}},
url = {http://www.ifaamas.org/Proceedings/aamas2012/papers/1B{\_}1.pdf},
year = {2012}
}
@phdthesis{Watkins1989,
author = {Watkins, Christopher J.C.H.},
file = {:home/sarios/papers/learning-from-delayed-rewards.pdf:pdf},
school = {King's College},
title = {{Learning from delayed rewards}},
year = {1989}
}
@article{Thrun1993,
abstract = {Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failuresâ€”namely, a systematic overestimation of utility values. Using Watkins' Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings.},
author = {Thrun, Sebastian and Schwartz, Anton},
file = {:home/sarios/papers/issues-in-using-function-approximation-in-reinforcement-learning.pdf:pdf},
journal = {Proceedings of the 4th Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum},
pages = {1--9},
title = {{Issues in Using Function Approximation for Reinforcement Learning}},
year = {1993}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Watkins, Christopher J.C.H. and Dayan, Peter},
doi = {10.1023/A:1022676722315},
eprint = {1412.3409},
file = {:home/sarios/papers/technical-note-qlearning.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {15730565},
journal = {Machine Learning},
keywords = {(Formula presented.)-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
pmid = {7761831},
title = {{Technical Note: Q-Learning}},
volume = {8},
year = {1992}
}
@article{Greensmith2004,
abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
annote = {This article proposes various baselines to reduce varience in policy gradient methods without introducing variance.},
author = {Greensmith, Evan and Bartlett, Pl and Baxter, J},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greensmith, Bartlett, Baxter - 2004 - Variance reduction techniques for gradient estimates in reinforcement learning.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {The Journal of Machine Learning {\ldots}},
keywords = {actor-critic,baseline,gpomdp,policy gradient,reinforcement learning},
pages = {1471--1530},
title = {{Variance reduction techniques for gradient estimates in reinforcement learning}},
url = {http://dl.acm.org/citation.cfm?id=1044710},
volume = {5},
year = {2004}
}
@article{Williams1992,
annote = {This is the article where introducing a baseline in reinforce is shown to keep the estimation unbiased},
author = {Williams, Ronald J.},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - 1992 - Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.pdf:pdf},
pages = {229--256},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.}},
year = {1992}
}
@article{Henderson2017,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:pdf},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Henderson2017a,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:pdf},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Vodopivec2017,
author = {Vodopivec, Tom},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vodopivec - 2017 - On Monte Carlo Tree Search and Reinforcement Learning.pdf:pdf},
title = {{On Monte Carlo Tree Search and Reinforcement Learning}},
volume = {60},
year = {2017}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:home/sarios/papers/a-brief-survey-of-deep-reinforcement-learning.pdf:pdf},
isbn = {9781424469178},
issn = {1701.07274},
pages = {1--16},
pmid = {25719670},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240},
year = {2017}
}
@article{Lowe2017,
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
archivePrefix = {arXiv},
arxivId = {1706.02275},
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
eprint = {1706.02275},
file = {:home/sarios/papers/multi-agent-actor-critic-competitive-cooperative-environments.pdf:pdf},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}},
url = {http://arxiv.org/abs/1706.02275},
year = {2017}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/sarios/papers/dqn.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Vodopivec2017a,
author = {Vodopivec, Tom},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vodopivec - 2017 - On Monte Carlo Tree Search and Reinforcement Learning.pdf:pdf},
title = {{On Monte Carlo Tree Search and Reinforcement Learning}},
volume = {60},
year = {2017}
}
@article{Jaderberg2016a,
abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local infor- mation. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass â€“ amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
archivePrefix = {arXiv},
arxivId = {1608.05343},
author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1608.05343},
issn = {1938-7228},
journal = {arXiv},
number = {Nips},
pages = {1608.05343v1 [cs.LG]},
title = {{Decoupled Neural Interfaces using Synthetic Gradients}},
year = {2016}
}
@article{Pathak2017,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Prediction.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
volume = {2017-July},
year = {2017}
}
@article{Czarnecki2017,
abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
archivePrefix = {arXiv},
arxivId = {1703.00522},
author = {Czarnecki, Wojciech Marian and {\'{S}}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
eprint = {1703.00522},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Czarnecki et al. - 2017 - Understanding Synthetic Gradients and Decoupled Neural Interfaces.pdf:pdf},
issn = {1938-7228},
title = {{Understanding Synthetic Gradients and Decoupled Neural Interfaces}},
url = {http://arxiv.org/abs/1703.00522},
year = {2017}
}
@inproceedings{Pathak2017a,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Prediction.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
volume = {2017-July},
year = {2017}
}
@article{Wender2012,
author = {Wender, Stefan and Watson, Ian},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wender, Watson - 2012 - Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft Broodwar.pdf:pdf},
isbn = {9781467311946},
keywords = {broodwar,qlearning,sarsa,starcraft},
mendeley-tags = {broodwar,qlearning,sarsa,starcraft},
pages = {402--408},
title = {{Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft : Broodwar}},
year = {2012}
}
@article{Griffiths1999,
abstract = {Technology his always played a role in the development of gambling practices and will continue to play a critical role in the development of increased gambling opportunities (e.g., internet gambling). Although technological advance his long been associated with improved gambling opportunities, there is little written in the literature explicitly pointing out this link and its implications for problem gamblers. This paper therefore reviews this situation and examines the technological implications of situational and structural characteristics paying particular attention to slot machine gambling as there has been more empirical work on this type of gambling than any other technological form. The impact of technology on the sociability of gambling is also examined followed by a more speculative evolution of internet gambling as an area of potential concern.},
author = {Griffiths, Mark},
doi = {http://dx.doi.org/10.1023/A:1023053630588},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths - 1999 - Gambling Technologies Prospects for Problem Gambling.pdf:pdf},
isbn = {1050-5350 (Print)},
issn = {1573-3602},
journal = {Journal of gambling studies},
number = {3},
pages = {265--283},
pmid = {12766464},
title = {{Gambling Technologies: Prospects for Problem Gambling.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12766464},
volume = {15},
year = {1999}
}
@article{Commission2016,
author = {Commission, Gambling},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Commission - 2016 - Virtual currencies , eSports and social gaming â€“ discussion paper(2).pdf:pdf},
keywords = {Virtual currencies eSports and social gaming  disc},
number = {August},
pages = {1--12},
title = {{Virtual currencies , eSports and social gaming â€“ discussion paper}},
year = {2016}
}
@article{Griffiths2000,
abstract = {It has been noted that adolescents may be more susceptible to pathological gambling. Not only is it usually illegal, but it appears to be related to high levels of problem gambling and other delinquent activities such as illicit drug taking and alcohol abuse. This paper examines risk factors not only in adolescent gambling but also in videogame playing (which shares many similarities with gambling). There appear to be three main forms of adolescent gambling that have been widely researched. Adolescent gambling activities and general risk factors in adolescent gambling are provided. As well, the influence of technology on adolescents in the form of both videogames and the Internet are examined. It is argued that technologically advanced forms of gambling may be highly appealing to adolescents.},
author = {Griffiths, Mark D and Wood, R T},
doi = {10.1023/A:1009433014881},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths, Wood - 2000 - Risk factors in adolescence the case of gambling, videogame playing, and the internet.pdf:pdf},
isbn = {1050-5350},
issn = {1050-5350},
journal = {Journal of Gambling Studies},
keywords = {addiction,adolescence,adolescent gambling is a,be related to high,but it appears to,gambling,internet,is it usually illegal,levels of prob-,major problem in society,not only,problem adolescent gambling,today,videogames},
number = {2-3},
pages = {199--225},
pmid = {14634313},
title = {{Risk factors in adolescence: the case of gambling, videogame playing, and the internet.}},
volume = {16},
year = {2000}
}
@article{Commission2016a,
author = {Commission, Gambling},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Commission - 2016 - Virtual currencies , eSports and social gaming â€“ discussion paper.pdf:pdf},
keywords = {Virtual currencies eSports and social gaming  disc},
number = {August},
pages = {1--12},
title = {{Virtual currencies , eSports and social gaming â€“ discussion paper}},
year = {2016}
}
@article{IabInternetAdvertisingBureauUK2011,
author = {{(Iab) Internet Advertising Bureau UK}},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/(Iab) Internet Advertising Bureau UK - 2011 - Gaming Britain A Nation United by Digital Play.pdf:pdf},
title = {{Gaming Britain: A Nation United by Digital Play}},
year = {2011}
}
@article{DeAsis,
abstract = {Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD($\lambda$) elegantly unifies one-step TD prediction with Monte Carlo meth-ods through the use of eligibility traces and the trace-decay parameter $\lambda$. Currently, there are a multitude of algorithms that can be used to per-form TD control, including Sarsa, Q-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be ex-tended across multiple time steps to achieve bet-ter performance. Each of these algorithms is seemingly distinct, and no one dominates the oth-ers for all problems. In this paper, we study a new multi-step action-value algorithm called Q($\sigma$) which unifies and generalizes these exist-ing algorithms, while subsuming them as special cases. A new parameter, $\sigma$, is introduced to al-low the degree of sampling performed by the al-gorithm at each step during its backup to be con-tinuously varied, with Sarsa existing at one ex-treme (full sampling), and Expected Sarsa exist-ing at the other (pure expectation). Q($\sigma$) is gen-erally applicable to both on-and off-policy learn-ing, but in this work we focus on experiments in the on-policy case. Our results show that an in-termediate value of $\sigma$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater per-formance.},
author = {{De Asis}, Kristopher and Ca, Kldeasis@ualberta and Hernandez-Garcia, J Fernando and Ca, Jfhernan@ualberta and Holland, G Zacharias and Sutton, Richard S},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Asis et al. - Unknown - Multi-step Reinforcement Learning A Unifying Algorithm.pdf:pdf},
keywords = {Unifying,multi-step},
mendeley-tags = {Unifying,multi-step},
title = {{Multi-step Reinforcement Learning: A Unifying Algorithm}},
url = {https://arxiv.org/pdf/1703.01327.pdf}
}
@article{Spronck,
abstract = {Adaptive game AI aims at enhancing computer-controlled game-playing agents with the ability to self-correct mistakes, and with creativity in re-sponding to new situations. Before game publish-ers will allow the use of adaptive game AI in their games, they must be convinced of its reliability. In this paper we introduce a model for Reliable Adap-tive Game Intelligence (RAGI). The purpose of the model is to provide a conceptual framework for the implementation of reliable adaptive game AI. We discuss requirements for reliable adaptive game AI, the RAGI model's characteristics, and possible im-plementations of the model.},
author = {Spronck, Pieter},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spronck - Unknown - A Model for Reliable Adaptive Game Intelligence.pdf:pdf},
title = {{A Model for Reliable Adaptive Game Intelligence}},
url = {http://www.spronck.net/pubs/RAGI.pdf}
}
@article{Ricciardi2008,
abstract = {Traditionally, AI research for games has focused on developing static strategiesâ€”fixed maps from the game state to a set of actionsâ€”which maximize the probability of victory. This works well for discovering facts about the game itself, and can be very successfully applied to combinatorial games like chess and checkers, where the personality and play style of the opponent takes a backseat to the mathematical problem of the game. In addition, this is the most widely used sort of AI among commercial video games today [1]. While these algorithms can often fight effectively, they tend to become repetitive and transparent to the players of the game, because their strategies are fixed. Even the most advanced AI algorithms for complex games often have a hole in their programming, a simple strategy which can be repeated over and over to remove the challenge of the AI opponent. Because their algorithms are static and inflexible, these AIs are incapable of adapting to these strategies and restoring the balance of the game. In addition, many games simply do not take well to these sorts of algorithms. Fighting games, such as Street Fighter and Virtua Fighter, often have no objectively correct move in any given situation. Rather, any of the available moves could be good or bad, depending on the opponent's reaction. These games reward intuition, good guessing, and the ability to get into your opponent's mind, rather than mastery of abstract game concepts. Hence, the static AI algorithms that are most prevalent tend to do even worse in these games than in others. These two problems are perfect for applying the techniques of machine learning. A better AI would have to be able to adapt to the player and model his or her actions, in order to predict and react to them. In this paper, we discuss how we developed such an AI and tested its effectiveness in a fighting game.},
author = {Ricciardi, Antonio and Thill, Patrick},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ricciardi, Thill - 2008 - Adaptive AI for Fighting Games.pdf:pdf},
title = {{Adaptive AI for Fighting Games}},
url = {http://cs229.stanford.edu/proj2008/RicciardiThill-AdaptiveAIForFightingGames.pdf},
year = {2008}
}
@article{Hewitt2013,
abstract = {Other topics in this issue include coaching children with autism, teaching styles, serve correction and information on an exciting new tennis app. Since the launch of CSSR in English in 1992, the ITF has published over 560 articles from contributors of more than 35 different nationalities. Today the review is produced 3 times per year in the 3 official ITF languages of English, Spanish and French and made available free of charge on the ITF coaching web at http://www.itftennis.com/coaching/sportsscience. The 2012 launch of, 'Biomechanics for Advanced Tennis' as an e-book has proven to be very successful in the new electronic format. Interested readers can purchase their copy at: http://www.amazon.es/ITF-Biomechanics-Advanced-Tennis-ebook/dp/ B00A79U7MK The ITF Tennis iCoach website remains at the forefront of online coach education, with up to date and current research available to coaches across the world. For just {\$}30 per year you can keep up to date with then most current tennis specific coaching information. Please click on the following link for a tour of the site. www.tennisicoach.com In late 2013, the Tennis iCoach will be re-launched with mobile and tablet PC support on both Android and Apple devices. Version 3.0 of the site will also offer HD quality video, a new navigation and search system, as well as a range of new features that will enhance the user engagement and learning experience for users. The launch is set for autumn 2013 and will be officially released at the Worldwide conference in Mexico.},
author = {Hewitt, Mitchell and Aus, Kenneth Edwards and Pluim, Babette and Smit, Claudia and Driessen, Dorian and Oskam, Sandy and Aus, Geoff Quinlan},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hewitt et al. - 2013 - CONTENTS ISSUE 59 The Official Coaching and Sport Science Publication of the International Tennis Federation.pdf:pdf},
issn = {2225-4757},
journal = {ITF Coaching and Sport Science Review},
number = {59},
title = {{CONTENTS ISSUE 59 The Official Coaching and Sport Science Publication of the International Tennis Federation}},
url = {www.itftennis.com/coaching/sportsscience},
year = {2013}
}
@article{Klaassen2001,
abstract = {This article tests whether points in tennis are independent and/or identically distributed (iid). We model the probability of winning a point on service and show that points are neither independent nor identically distributed: winning the previous point has a positive ef- fect on winning the current point, and at â€˜important' points it is more difficult for the server to win the point than at less important points. Furthermore, the weaker a player, the stronger are these effects. De- viations from iid are, however, small and hence the iid hypothesis will still provide a good approximation in many cases. The results are based on a large panel of matches played atWimbledon 1992â€”1995, in total almost 90,000 points. Our panel data model takes account of the binary character of the dependent variable, uses random effects to cap- ture theunobservedpartofaplayer's quality, andincludesdynamic explanatory variables.},
author = {Klaassen, Franc J. G. M. and Magnus, Jan R.},
doi = {10.1198/016214501753168217},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {binary choice,dependence,dynamic panel data,linear probability model,nonidentical distribution,random effects},
number = {454},
pages = {500--509},
title = {{Are Points in Tennis Independent and Identically Distributed? Evidence From a Dynamic Binary Panel Data Model}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753168217},
volume = {96},
year = {2001}
}
@article{Knottenbelt2012,
abstract = {Tennis features among the most popular sports internationally, with professional matches played for 11 months of the year around the globe. The rise of the internet has stimulated a dramatic increase in tennis-related financial activity, much of which depends on quantitative models. This paper presents a hierarchical Markov model which yields a pre-play estimate of the probability of each player winning a professional singles tennis match. Crucially, the model provides a fair basis of comparison between players by analysing match statistics for opponents that both players have encountered in the past. Subsequently the model exploits elements of transitivity to compute the probability of each player winning a point on their serve, and hence the match. When evaluated using a data set of historical match statistics and bookmakers odds, the model yields a 3.8{\%} return on investment over 2173 ATP matches played on a variety of surfaces during 2011. ?? 2012 Elsevier Ltd. All rights reserved.},
author = {Knottenbelt, William J. and Spanias, Demetris and Madurska, Agnieszka M.},
doi = {10.1016/j.camwa.2012.03.005},
issn = {08981221},
journal = {Computers and Mathematics with Applications},
keywords = {Sport,Stochastic modelling,Tennis},
number = {12},
pages = {3820--3827},
title = {{A common-opponent stochastic model for predicting the outcome of professional tennis matches}},
volume = {64},
year = {2012}
}
@article{Barnett2005,
abstract = {Tennis features among the most popular sports internationally, with professional matches played for 11 months of the year around the globe. The rise of the internet has stimulated a dramatic increase in tennis-related financial activity, much of which depends on quantitative models. This paper presents a hierarchical Markov model which yields a pre-play estimate of the probability of each player winning a professional singles tennis match. Crucially, the model provides a fair basis of comparison between players by analysing match statistics for opponents that both players have encountered in the past. Subsequently the model exploits elements of transitivity to compute the probability of each player winning a point on their serve, and hence the match. When evaluated using a data set of historical match statistics and bookmakers odds, the model yields a 3.8{\%} return on investment over 2173 ATP matches played on a variety of surfaces during 2011. ?? 2012 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1011.1761},
author = {Barnett, Tristan and Brown, Alan and Clarke, Stephen R. and Caron, Francois and Doucet, Arnaud and Glickman, Mark E Me Mark and Knottenbelt, William J. and Spanias, Demetris and Madurska, Agnieszka M. and Krajowski-kukiel, Maciej and Madurska, Agnieszka M. and McHale, Ian and Morton, Alex and Ralph, B Y and Bradley, Allan and Terry, Milton E and Brent, R P and Dekker, T J and Rios, Luis Miguel and Sahinidis, Nikolaos V. and Herbrich, Ralf and Minka, Tom and Graepel, Thore and Cattelan, Manuela and Varin, Cristiano},
doi = {10.1093/imaman/dpi001},
eprint = {1011.1761},
isbn = {1049-5258},
issn = {08981221},
journal = {Boston University},
keywords = {Australian Open,Betting,Bradley-Terry model,Derivative-free algorithms,Direct search methods,Excel,Gambling,Logit,Ranking evaluation,Scoring systems,Sport,Stochastic modelling,Surrogate models,Tennis,approximate,average process,bayesian estimation,bayesian learning,bradley,cumulative logit model,dynamic difficulty adjustment,exponentially weighted moving,index betting,markov chain,match-making,paired comparisons,sport,sports tournaments,tennis,terry model},
number = {3},
pages = {1--6},
pmid = {18268290},
title = {{Developing a Model that Reflects Outcomes of Tennis Matches}},
url = {http://research.microsoft.com/apps/pubs/default.aspx?id=67956{\%}5Cnhttp://dx.doi.org/10.1016/j.ijforecast.2010.04.004{\%}5Cnhttp://glicko.net/glicko/glicko2.pdf{\%}5Cnhttp://www.echecsonline.net/joueurs/doc/The{\_}Glicko{\_}system.pdf{\%}5Cnhttp://www.glicko.net/research/},
volume = {48},
year = {2005}
}
@article{Pfeiffer2010,
abstract = {The evaluation of the structure of sports performance is one of the important functions of diagnostics in competitive sport. Especially in game sports, it is important to obtain diagnostic information on competition because of the interactive process between the two teams or players. This interaction cannot be simulated or replicated in training or test situations. When it comes to table tennis, performance diagnostics offers many different techniques and methods to analyze a game. In this context, problems mostly occur in ...},
author = {Pfeiffer, Mark and Zhang, Hui and Hohmann, Andreas},
doi = {10.1260/1747-9541.5.2.205},
issn = {1747-9541},
journal = {International Journal of Sports Science and Coaching},
number = {2},
pages = {205--222},
title = {{A Markov chain model of elite table tennis competition}},
volume = {5},
year = {2010}
}
@article{Newtont2006,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Abstract. The probability of winning a game, set, match, or single elimination tournament in tennis is computed using Monte Carlo simulations based on each player's probability of winning a point on serve, which can be held constant or varied from point to point, game to game, or match to match. The theory, described in Newton and Keller [Stud. Appl. Math., 114 (2005), pp. 241-269], is based on the assumption that points in tennis are independent, identically distributed (i.i.d.) random variables. This is used as a baseline to compare with the simulations, which under similar circumstances are shown to converge quickly to the analytical curves in accordance with the weak law of large numbers. The concept of the importance of a point, game, and set to winning a match is described based on conditional probabilities and is used as a starting point to model non-i.i.d. effects, allowing each player to vary, from point to point, his or her probability of winning on serve. Several non-i.i.d. models are investigated, including the "hot-hand-effect," in which we increase each player's probability of winning a point on serve on the next point after a point is won. The "back to-the-wall" effect is modeled by increasing each player's probability of winning a point on serve on the next point after a point is lost. In all cases, we find that the results provided by the theoretical curves based on the i.i.d. assumption are remarkably robust and accurate, even when relatively strong non-i.i.d. effects are introduced. We end by showing examples of tournament predictions from the 2002 men's and women's U.S. Open draws based on the Monte Carlo simulations. We also describe Arrow's impossibility theorem and discuss its relevance with regard to sports ranking systems, and we argue for the development of probability-based ranking systems as a way to soften its consequences.},
author = {Newtont, Paul K and Aslamt, Kamran},
doi = {10.1137/050640278},
isbn = {00361445},
issn = {0036-1445},
journal = {SIAM Review SIAM REVIEW Society for Industrial and Applied Mathematics},
keywords = {60J20,65Q05,91A60,91B12,Arrow's theorem AMS subject classifications 65C05,Monte Carlo Method,non-iid effects,probabilistic ranking systems,tennis},
number = {4},
pages = {722--742},
pmid = {25468915},
title = {{Monte Carlo Tennis*}},
url = {http://www.jstor.org/stable/20453873{\%}5Cnhttp://about.jstor.org/terms},
volume = {48},
year = {2006}
}
@article{Lindley2008,
abstract = {Schema theory provides a foundation for the analysis of game play patterns created by players during their interaction with a game. Schema models derived from the analysis of play provide a rich explanatory framework for the cognitive processes underlying game play, as well as detailed hypotheses for the hierarchical structure of pleasures and rewards motivating players. Game engagement is accounted for as a process of schema selection or development, while immersion is explained in terms of levels of attentional demand in schema execution. However, schemas may not only be used to describe play, but might be used actively as cognitive models within a game engine. Predesigned schema models are knowledge representations constituting anticipated or desired learned cognitive outcomes of play. Automated analysis of player schemas and comparison with predesigned target schemas can provide a foundation for a game engine adapting or tuning game mechanics to achieve specific effects of engagement, immersion, and cognitive skill acquisition by players. Hence, schema models may enhance the play experience as well as provide a foundation for achieving explicitly represented pedagogical or therapeutic functions of games.},
author = {Lindley, Craig a. and Sennersten, Charlotte C.},
doi = {10.1155/2008/216784},
isbn = {86905-901-7},
issn = {1687-7047},
journal = {International Journal of Computer Games Technology},
pages = {1--7},
title = {{Game Play Schemas: From Player Analysis to Adaptive Game Mechanics}},
url = {http://www.hindawi.com/journals/ijcgt/2008/216784/},
volume = {2008},
year = {2008}
}
@incollection{Nelson2007,
abstract = {Game generation systems perform automated, intelligent design of games (i.e. videogames, boardgames), reasoning about both the abstract rule system of the game and the visual realization of these rules. Although, as an instance of the problem of creative design, game generation shares some common research themes with other creative AI systems such as story and art generators, game generation extends such work by having to reason about dynamic, playable artifacts. Like AI work on creativity in other domains, work on game generation sheds light on the human game design process, offering opportunities to make explicit the tacit knowledge involved in game design and test game design theories. Finally, game generation enables new game genres which are radically customized to specific players or situations; notable examples are cell phone games customized for particular users and newsgames providing commentary on current events. We describe an approach to formalizing game mechanics and generating games using those mechanics, using WordNet and ConceptNet to assist in performing common-sense reasoning about game verbs and nouns. Finally, we demonstrate and describe in detail a prototype that designs micro-games in the style of Nintendo's WarioWare series.},
address = {Berlin, Heidelberg},
author = {Nelson, Mark J. and Mateas, Michael},
booktitle = {AI*IA 2007: Artificial Intelligence and Human-Oriented Computing},
doi = {10.1007/978-3-540-74782-6_54},
isbn = {978-3-540-74781-9},
issn = {03029743},
pages = {626--637},
publisher = {Springer Berlin Heidelberg},
title = {{Towards Automated Game Design}},
url = {http://www.springerlink.com/content/64921n6401026242/ http://link.springer.com/10.1007/978-3-540-74782-6{\_}54},
year = {2007}
}
@inproceedings{Zook2014,
abstract = {Game designs often center on the game mechanicsâ€”rules governing the logical evolution of the game. We seek to de- velop an intelligent system that generates computer games and assists humans in designing games. As first steps to- wards this goal we present a composable and cross-domain representation for game mechanics that draws from AI plan- ning action representations. We use a constraint solver to generate mechanics subject to design requirements on the form of those mechanicsâ€”what they do in the game. A planner takes a set of generated mechanics and tests whether those mechanics meet playability requirementsâ€”controlling how mechanics function in a game to affect player behav- ior. We demonstrate our system by modeling and generat- ing mechanics in a role-playing game, platformer game, and combined role-playing-platformer game.},
author = {Zook, Alexander and Riedl, Mo},
booktitle = {Proceedings of the 2014 Foundations of Digital Games Workshop on Procedural Content Generation in Games},
keywords = {game design,game mechanics,procedural content generation},
title = {{Generating and Adapting Game Mechanics}},
url = {http://www.cc.gatech.edu/{~}riedl/pubs/pcg14.pdf},
year = {2014}
}
@article{Sicart2008,
abstract = {This article defines game mechanics in relation to rules and challenges. Game mechanics are methods invoked by agents for interacting with the game world. I apply this definition to a comparative analysis of the games Rez, Every Extend Extra and Shadow of the Colossus that will show the relevance of a formal definition of game mechanics.},
author = {Sicart, Miguel},
doi = {1604-7982},
isbn = {1604-7982},
issn = {16047982},
journal = {Game Studies},
keywords = {Challenges,Game design,Game mechanics,Game research,Rules},
number = {2},
title = {{Defining game mechanics}},
volume = {8},
year = {2008}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
