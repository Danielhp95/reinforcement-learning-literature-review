@article{Owen1982,
abstract = {This advanced text introduces the principles of noncooperative game theory in a direct and uncomplicated style that will acquaint students with the broad spectrum of the field while highlighting and explaining what they need to know at any given point.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Owen, G and Owen, G},
doi = {10.4135/9781412984317},
eprint = {arXiv:1011.1669v3},
isbn = {9780803920507},
issn = {0-262-06141-4},
journal = {Collection},
keywords = {DSA 2009 BS-2 E-Democracy},
pmid = {3364812506127435976},
title = {{Game Theory}},
year = {1982}
}
@article{Bansal2017,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archivePrefix = {arXiv},
arxivId = {1710.03748},
author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
eprint = {1710.03748},
file = {:home/sarios/papers/emergent-complexity-behaviour-via-multi-agent-competition.pdf:pdf},
pages = {1--12},
title = {{Emergent Complexity via Multi-Agent Competition}},
url = {http://arxiv.org/abs/1710.03748},
volume = {2},
year = {2017}
}
@article{Piunovskiy2012,
abstract = {In this paper, we show that a discounted continuous-time Markov decision process in Borel spaces with randomized history-dependent policies, arbitrarily unbounded transition rates and a non-negative reward rate is equivalent to a discrete-time Markov decision process. Based on a completely new proof, which does not involve Kolmogorov's forward equation, it is shown that the value function for both models is given by the minimal non-negative solution to the same Bellman equation. A verifiable necessary and sufficient condition for the finiteness of this value function is given, which induces a new condition for the non-explosion of the underlying controlled process. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
author = {Piunovskiy, Alexey and Zhang, Yi},
doi = {10.1007/s10957-012-0015-8},
file = {:home/sarios/papers/The-Transformation-Method-for-Continuous-Time-mdps.pdf:pdf},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Continuous-time Markov decision process,Discrete-time Markov decision process,History-dependent policies,Transformation method,Unbounded transition rates},
number = {2},
pages = {691--712},
title = {{The Transformation Method for Continuous-Time Markov Decision Processes}},
volume = {154},
year = {2012}
}
@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G},
doi = {10.1.1.4.6238-1},
file = {:home/sarios/papers/recent-advancements-in-hierarchical-rl.pdf:pdf},
isbn = {0924-6703},
issn = {0924-6703},
journal = {Most},
number = {5},
pages = {1--28},
title = {{Recent Advances in Hierarchical Reinforcement Learning Markov and Semi-Markov Decision Processes}},
volume = {13},
year = {2003}
}
@article{Jaderberg,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.01281v1},
author = {Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
doi = {arXiv:1807.01281},
eprint = {arXiv:1807.01281v1},
file = {:home/sarios/papers/for-the-win.pdf:pdf},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}}
}
@article{Sutton2010,
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other arti cial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a sin- gle predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the sys- tem's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, re- ward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever ac- tions are taken by the system as a whole. Gradient-based temporal-di erence learning methods are used to learn ef- ciently and reliably with function approximation in this o -policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real- time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from o - policy experience. Horde is a signi cant incremental step towards a real-time architecture for ecient learning of gen-},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam},
doi = {10.1037/a0023964},
eprint = {NIHMS150003},
file = {:home/sarios/papers/horde.pdf:pdf},
isbn = {0-9826571-6-1, 978-0-9826571-6-4},
issn = {0982657161},
journal = {In Practice},
keywords = {artificial intelligence,difference learning,inforcement learning,knowledge representation,off policy learning,re,real time,robotics,temporal,value function approximation},
number = {1972},
pages = {761--768},
pmid = {21604833},
title = {{Horde : A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction Categories and Subject Descriptors}},
url = {http://webdocs.cs.ualberta.ca/{~}sutton/papers/horde-aamas-11.pdf},
volume = {2},
year = {2011}
}
@article{Schaul2015,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
archivePrefix = {arXiv},
arxivId = {1511.05952},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
doi = {10.1038/nature14236},
eprint = {1511.05952},
file = {:home/sarios/papers/prioritized-experience-replay.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {0028-0836},
pages = {1--21},
pmid = {25719670},
title = {{Prioritized Experience Replay}},
url = {http://arxiv.org/abs/1511.05952},
year = {2015}
}
@article{Andrychowicz2017,
abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {1707.01495},
file = {:home/sarios/papers/hindsight-experience-replay.pdf:pdf},
issn = {10495258},
number = {Nips},
title = {{Hindsight Experience Replay}},
url = {http://arxiv.org/abs/1707.01495},
year = {2017}
}
@article{Littman1994,
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly...},
author = {Littman, Michael L.},
doi = {10.1016/B978-1-55860-335-6.50027-1},
file = {:home/sarios/papers/markov-games-as-gramework-for-multi-agent-rl.pdf:pdf},
isbn = {1-55860-335-2},
issn = {00493848},
journal = {Machine Learning Proceedings 1994},
keywords = {Environments},
mendeley-tags = {Environments},
pages = {157--163},
pmid = {17034835},
title = {{Markov games as a framework for multi-agent reinforcement learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271},
year = {1994}
}
@article{Hessel2017,
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
eprint = {1710.02298},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcement Learning.pdf:pdf},
keywords = {dqn},
mendeley-tags = {dqn},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1710.02298},
year = {2017}
}
@article{Firoiu2017,
abstract = {There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.},
archivePrefix = {arXiv},
arxivId = {1702.06230},
author = {Firoiu, Vlad and Whitney, William F. and Tenenbaum, Joshua B.},
eprint = {1702.06230},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Firoiu, Whitney, Tenenbaum - 2017 - Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning.pdf:pdf},
title = {{Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.06230},
year = {2017}
}
@article{Li2018,
archivePrefix = {arXiv},
arxivId = {1805.02070},
author = {Li, Yu-Jhe and Chang, Hsin-Yu and Lin, Yu-Jing and Wu, Po-Wei and Wang, Yu-Chiang Frank},
eprint = {1805.02070},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2018 - Deep Reinforcement Learning for Playing 2.5D Fighting Games.pdf:pdf},
title = {{Deep Reinforcement Learning for Playing 2.5D Fighting Games}},
url = {http://arxiv.org/abs/1805.02070},
year = {2018}
}
@article{Graepel2004,
abstract = {We apply reinforcement learning to the problem of finding good policies for a fighting agent in a commercial computer game. The learning agent is trained using the SARSA algorithm for on-policy learning of an action-value function represented by linear and neural network function approximators. We discuss the selection and construction of features, actions, and rewards as well as other design choices necessary to integrate the learning process into the game. The learning agent is trained against the built-in AI of the game with different rewards encouraging aggressive or defensive behaviour. We show that the learning agent finds interesting (and partly near optimal) policies in accordance with the reward functions provided. We also discuss the particular challenges arising in the application of reinforcement learning to the domain of computer games.},
author = {Graepel, Thore and Herbrich, Ralf and Gold, Julian},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graepel, Herbrich, Gold - 2004 - Learning to Fight.pdf:pdf},
issn = {0885-842X},
journal = {Proceedings of the International Conference on Computer Games: Artificial Intelligence, Design and Education},
keywords = {fighting games,markov decision process,q-learning,reinforcement learning,sarsa},
pages = {193--200},
pmid = {3347383},
title = {{Learning to Fight}},
url = {http://www.herbrich.me/papers/graehergol04.pdf},
year = {2004}
}
@article{Silver2014,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value func- tion. This simple form means that the deter- ministic policy gradient can be estimated much more efficiently than the usual stochastic pol- icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter- parts in high-dimensional action spaces.},
annote = {Deterministic policy gradient is an off-policy, model free policy gradient algorithm. The agent moves in the parameter space of the policy to approximate a target deterministic policy from an exploratory behavioural policy.

100{\%} Talk how this is a corner case of stochastic policy gradient algorithms in your literature review.},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2014 - Deterministic Policy Gradient Algorithms.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
pages = {387--395},
title = {{Deterministic Policy Gradient Algorithms}},
year = {2014}
}
@article{Andersen2017,
author = {Andersen, Per-arne},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andersen - 2017 - Deep RTS A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games.pdf:pdf},
title = {{Deep RTS : A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games}},
year = {2017}
}
@article{Ontanon2013,
abstract = {Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Na¨ ıve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS al- gorithm based on Na¨ ıve Sampling called Na¨ ıveMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, Na¨ ıveMCTS performs significantly better than other algorithms. Introduction},
author = {Onta{\~{n}}{\'{o}}n, Santiago},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Onta{\~{n}}{\'{o}}n - 2013 - The combinatorial multi-armed bandit problem and its application to real-time strategy games.pdf:pdf},
journal = {Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
pages = {58--64},
title = {{The combinatorial multi-armed bandit problem and its application to real-time strategy games}},
url = {http://www.aaai.org/ocs/index.php/AIIDE/AIIDE13/paper/viewPaper/7377},
year = {2013}
}
@article{Lin1993,
abstract = {Reinforcement learning agents are adaptive, reactive, and self-supervised. The aim of this dissertation is to extend the state of the art of reinforcement learning and enable its applications to complex robot-learning problems. In particular, it focuses on two issues. First, learning from sparse and delayed reinforcement signals is hard and in general a slow process. Techniques for reducing learning time must be devised. Second, most existing reinforcement learning methods assume that the world is a Markov decision process. This assumption is too strong for many robot tasks of interest.$\backslash$r$\backslash$nThis dissertation demonstrates how we can possibly overcome the slow learning problem and tackle non-Markovian environments, making reinforcement learning more practical for realistic robot tasks: (1) Reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization, resulting in a significant learning speedup. Neural networks are used in this dissertation, and they generalize effectively even in the presence of noise and a large of binary and real-valued inputs. (2) Reinforcement learning agents can save many learning trials by using an action model, which can be learned on-line. With a model, an agent can mentally experience the effects of its actions without actually executing them. Experience replay is a simple technique that implements this idea, and is shown to be effective in reducing the number of action executions required. (3) Reinforcement learning agents can take advantage of instructive training instances provided by human teachers, resulting in a significant learning speedup. Teaching can also help learning agents avoid local optima during the search for optimal control. Simulation experiments indicate that even a small amount of teaching can save agents many learning trials. (4) Reinforcement learning agents can significantly reduce learning time by hierarchical learning--they first solve elementary learning problems and then combine solutions to the elementary problems to solve a complex problem. Simulation experiments indicate that a robot with hierarchical learning can solve a complex problem, which otherwise is hardly solvable within a reasonable time. (5) Reinforcement learning agents can deal with a wide range of non-Markovian environments by having a memory of their past. Three memory architectures are discussed. They work reasonably well for a variety of simple problems. One of them is also successfully applied to a nontrivial non-Markovian robot task.$\backslash$r$\backslash$n$\backslash$r$\backslash$nThe results of this dissertation rely on computer simulation, including (1) an agent operating in a dynamic and hostile environment and (2) a mobile robot operating in a noisy and non-Markovian environment. The robot simulator is physically realistic. This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning.},
author = {Lin, Long-ji},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin - 1993 - Reinforcement Learning for Robots Using Neural Networks.pdf:pdf},
journal = {Report, CMU},
keywords = {off policy,replay buffer},
mendeley-tags = {off policy,replay buffer},
pages = {1--155},
title = {{Reinforcement Learning for Robots Using Neural Networks}},
year = {1993}
}
@article{Bhatnagar2009,
abstract = {We present four new reinforcement learning algorithms based on actor-critic, natural-gradient and function-approximation ideas, and we provide their convergence proofs. Actor-critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function-approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor-critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor-critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms. {\textcopyright} 2009 Elsevier Ltd.},
author = {Bhatnagar, Shalabh and Sutton, Richard S. and Ghavamzadeh, Mohammad and Lee, Mark},
doi = {10.1016/j.automatica.2009.07.008},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatnagar et al. - 2009 - Natural actor-critic algorithms.pdf:pdf},
isbn = {0005-1098},
issn = {00051098},
journal = {Automatica},
keywords = {Actor-critic reinforcement learning algorithms,Approximate dynamic programming,Function approximation,Natural gradient,Policy-gradient methods,Temporal difference learning,Two-timescale stochastic approximation,actor critic},
mendeley-tags = {actor critic},
number = {11},
pages = {2471--2482},
publisher = {Elsevier Ltd},
title = {{Natural actor-critic algorithms}},
url = {http://dx.doi.org/10.1016/j.automatica.2009.07.008},
volume = {45},
year = {2009}
}
@article{Degris2012,
annote = {First off policy algorithm, using a behavioural policy for exploration and importance sampling techniques to weigh the update to actor parameters. Eligibility traces are used to calculate updates.},
archivePrefix = {arXiv},
arxivId = {arXiv:1205.4839v5},
author = {Degris, Thomas and White, Martha and Sutton, Richard S},
doi = {10.1.1.385.4471},
eprint = {arXiv:1205.4839v5},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Degris, White, Sutton - 2012 - Off-Policy Actor-Critic.pdf:pdf},
isbn = {978-1-4503-1285-1},
issn = {{\textless}null{\textgreater}},
keywords = {actor critic},
mendeley-tags = {actor critic},
title = {{Off-Policy Actor-Critic}},
year = {2012}
}
@article{Konda2000,
abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information pro-vided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
archivePrefix = {arXiv},
arxivId = {1607.07086},
author = {Konda, Vijay R and Tsitsiklis, John N},
doi = {10.1137/S0363012901385691},
eprint = {1607.07086},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konda, Tsitsiklis - 2000 - Actor-Critic Algorithms.pdf:pdf},
isbn = {0363-0129},
issn = {0363-0129},
journal = {Nips},
keywords = {actor critic,actor-critic algorithms,markov decision processes,reinforcement learning,stochas-},
mendeley-tags = {actor critic},
number = {4},
pages = {1143--1166},
pmid = {21222527},
title = {{Actor-Critic Algorithms}},
volume = {42},
year = {2000}
}
@article{Baxter2001,
abstract = {Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes POMDPs controlled by parameterized stochastic policies. A similar algorithm was proposed by (Kimura et al. 1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free beta (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of GPOMDP, and show how the correct choice of the parameter beta is related to the mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter et al., this volume) we show how the gradient estimates generated by GPOMDP can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward.},
archivePrefix = {arXiv},
arxivId = {1106.0665},
author = {Baxter, Jonathan and Bartlett, Peter L.},
doi = {10.1613/jair.806},
eprint = {1106.0665},
isbn = {10769757 (ISSN)},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {319--350},
title = {{Infinite-horizon policy-gradient estimation}},
volume = {15},
year = {2001}
}
@article{Song2018,
abstract = {In this technical report, we consider an approach that combines the PPO objective and K-FAC natural gradient optimization, for which we call PPOKFAC. We perform a range of empirical analysis on various aspects of the algorithm, such as sample complexity, training speed, and sensitivity to batch size and training epochs. We observe that PPOKFAC is able to outperform PPO in terms of sample complexity and speed in a range of MuJoCo environments, while being scalable in terms of batch size. In spite of this, it seems that adding more epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to be worse than its A2C counterpart, ACKTR.},
annote = {Useful overview of policy gradient and trust region famous methods.},
archivePrefix = {arXiv},
arxivId = {1801.05566},
author = {Song, Jiaming and Wu, Yuhuai},
eprint = {1801.05566},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Wu - 2018 - An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients.pdf:pdf},
pages = {1--8},
title = {{An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients}},
url = {http://arxiv.org/abs/1801.05566},
year = {2018}
}
@article{UnityMLAgents,
author = {Unity},
title = {{Unity Machine Learning Agents}},
url = {https://unity3d.com/machine-learning/},
year = {2017}
}
@misc{Broodwar,
author = {Heinermann, Adam},
title = {{BWAPI: Brood war api, an api for interacting with starcraft: Broodwar}},
url = {https://bwapi.github.io/},
year = {2009}
}
@article{Synnaeve2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.00625v2},
author = {Synnaeve, Gabriel and Nardelli, Nantas and Auvolat, Alex and Chintala, Soumith and Lacroix, Timoth{\'{e}}e and Lin, Zeming and Richoux, Florian and Usunier, Nicolas},
eprint = {arXiv:1611.00625v2},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Synnaeve et al. - 2016 - TorchCraft a Library for Machine Learning Research on Real-Time Strategy Games arXiv 1611 . 00625v2 cs . LG.pdf:pdf},
pages = {1--6},
title = {{TorchCraft : a Library for Machine Learning Research on Real-Time Strategy Games arXiv : 1611 . 00625v2 [ cs . LG ] 3 Nov 2016}},
year = {2016}
}
@inproceedings{Kempka2017,
abstract = {The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.},
archivePrefix = {arXiv},
arxivId = {1605.02097},
author = {Kempka, Michal and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Jaskowski, Wojciech},
booktitle = {IEEE Conference on Computatonal Intelligence and Games, CIG},
doi = {10.1109/CIG.2016.7860433},
eprint = {1605.02097},
isbn = {9781509018833},
issn = {23254289},
keywords = {FPS,deep reinforcement learning,first-person perspective games,neural networks,video games,visual learning,visual-based reinforcement learning},
title = {{ViZDoom: A Doom-based AI research platform for visual reinforcement learning}},
year = {2017}
}
@article{Brockman2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brockman et al. - 2016 - OpenAI Gym.pdf:pdf},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Vinyals2017,
abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
archivePrefix = {arXiv},
arxivId = {1708.04782},
author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"{u}}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and van Hasselt, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
doi = {https://deepmind.com/documents/110/sc2le.pdf},
eprint = {1708.04782},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinyals et al. - 2017 - StarCraft II A New Challenge for Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1708.04782},
title = {{StarCraft II: A New Challenge for Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.04782},
year = {2017}
}
@inproceedings{Bellemare2015,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
archivePrefix = {arXiv},
arxivId = {1207.4708},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.1613/jair.3912},
eprint = {1207.4708},
isbn = {9781577357384},
issn = {10450823},
pages = {4148--4152},
title = {{The arcade learning environment: An evaluation platform for general agents}},
volume = {2015-Janua},
year = {2015}
}
@article{Lanctot2017,
abstract = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
archivePrefix = {arXiv},
arxivId = {1711.00832},
author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
eprint = {1711.00832},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanctot et al. - 2017 - A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.pdf:pdf},
number = {Nips},
title = {{A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning}},
url = {http://arxiv.org/abs/1711.00832},
year = {2017}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
doi = {10.1561/2200000006},
eprint = {1509.02971},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learning.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
pmid = {24966830},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Deisenroth2011,
abstract = {In this paper, we introduce pilco, a practi-cal, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforce-ment learning, in a principled way. By learn-ing a probabilistic dynamics model and ex-plicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprece-dented learning efficiency on challenging and high-dimensional control tasks.},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deisenroth, Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach to Policy Search.pdf:pdf},
title = {{PILCO: A Model-Based and Data-Efficient Approach to Policy Search}},
url = {http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf},
year = {2011}
}
@article{Tapas1999,
author = {Das, Tapas K. and Gosavi, Abhijit and Mahadevan, Sridhar and Marchalleck, Nicholas},
doi = {10.1287/mnsc.45.4.560},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Das et al. - 1999 - Solving Semi-Markov Decision Problems Using Average Reward Reinforcement Learning.pdf:pdf},
issn = {0025-1909},
journal = {Management Science},
month = {apr},
number = {4},
pages = {560--574},
title = {{Solving Semi-Markov Decision Problems Using Average Reward Reinforcement Learning}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.45.4.560},
volume = {45},
year = {1999}
}
@article{Wu2017,
abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
archivePrefix = {arXiv},
arxivId = {1708.05144},
author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
eprint = {1708.05144},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.pdf:pdf},
pages = {1--14},
title = {{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}},
url = {http://arxiv.org/abs/1708.05144},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
doi = {10.1177/0956797613514093},
eprint = {1602.01783},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
keywords = {actor critic},
mendeley-tags = {actor critic},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Sutton1999,
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the ¯rst time that a version of policy iteration with arbitrary di{\textregistered}erentiable function approximation is convergent to a locally optimal policy.},
annote = {This paper introduces a (somewhat dated) introduction to policy gradient methods which can prove valuable in lit review. It offers a digsted version of policy gradient therom as compared with many other papers.},
author = {Sutton, Richard S and Mcallester, David and Singh, Satinder and Mansour, Yishay},
doi = {10.1.1.37.9714},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf:pdf},
isbn = {0-262-19450-3},
issn = {0047-2875},
journal = {In Advances in Neural Information Processing Systems 12},
pages = {1057--1063},
title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
year = {1999}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
annote = {This is the article where introducing a baseline in reinforce is shown to keep the estimation unbiased.},
author = {Williams, Ronald J.},
doi = {10.1023/A:1022672621406},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - 1992 - Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning(2).pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis,policy-gradient},
mendeley-tags = {policy-gradient},
number = {3},
pages = {229--256},
pmid = {903},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
volume = {8},
year = {1992}
}
@article{Abdallah2016,
abstract = {Q-learning (QL) is a popular reinforcement learning algorithm that is guaranteed to converge to op-timal policies in Markov decision processes. However, QL exhibits an artifact: in expectation, the effective rate of updating the value of an action depends on the probability of choosing that action. In other words, there is a tight coupling between the learning dynamics and underlying execution policy. This coupling can cause performance degradation in noisy non-stationary environments. Here, we introduce Repeated Update Q-learning (RUQL), a learning algorithm that resolves the undesirable artifact of Q-learning while maintaining simplicity. We analyze the similarities and differences between RUQL, QL, and the closest state-of-the-art algorithms theoretically. Our analysis shows that RUQL maintains the convergence guarantee of QL in stationary environments, while relaxing the coupling between the execution policy and the learning dynamics. Experimental results confirm the theoretical insights and show how RUQL outperforms both QL and the closest state-of-the-art algorithms in noisy non-stationary environments.},
author = {Abdallah, Sherief and Org, Shario@ieee and Kaisers, Michael and Nl, Kaisers@cwi},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdallah et al. - 2016 - Addressing Environment Non-Stationarity by Repeating Q-learning Updates.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {multi-agent learning,non-stationary environ-,q-learning,qlearning,reinforcement learning},
mendeley-tags = {qlearning},
pages = {1--31},
title = {{Addressing Environment Non-Stationarity by Repeating Q-learning Updates}},
url = {http://www.jmlr.org/papers/volume17/14-037/14-037.pdf},
volume = {17},
year = {2016}
}
@article{browne2012survey,
author = {Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browne et al. - 2012 - A survey of {\{}Monte Carlo Tree Search{\}} methods.pdf:pdf},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {mcts},
mendeley-tags = {mcts},
number = {1},
pages = {1--43},
publisher = {IEEE},
title = {{A survey of {\{}Monte Carlo Tree Search{\}} methods}},
volume = {4},
year = {2012}
}
@article{Sutton1991,
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S.},
doi = {10.1145/122344.122377},
eprint = {arXiv:1011.1669v3},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton - 1991 - Dyna, an integrated architecture for learning, planning, and reacting.pdf:pdf},
isbn = {1-55860-141-4},
issn = {01635719},
journal = {ACM SIGART Bulletin},
number = {4},
pages = {160--163},
pmid = {15003161},
title = {{Dyna, an integrated architecture for learning, planning, and reacting}},
url = {http://portal.acm.org/citation.cfm?doid=122344.122377},
volume = {2},
year = {1991}
}
@article{Soemers2014,
abstract = {This thesis describes how Monte-Carlo Tree Search (MCTS) can be applied to perform tac-tical planning for an intelligent agent playing full games of StarCraft: Brood War. StarCraft is a Real-Time Strategy game, which has a large state-space, is played in real-time, and com-monly features two opposing players, capable of acting simultaneously. Using the MCTS al-gorithm for tactical planning is shown to in-crease the performance of the agent, compared to a scripted approach, when competing on a bot ladder. A combat model, based on Lanch-ester's Square Law, is described, and shown to achieve another gain in performance when used in Monte-Carlo simulations as replacement for a heuristic linear model. Finally, the MAST enhancement to the Playout Policy of MCTS is described, but it is found not to have a signifi-cant impact on the agent's performance.},
author = {Soemers, Dennis},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soemers - 2014 - Tactical Planning Using MCTS in the Game of StarCraft.pdf:pdf},
keywords = {lanchester,monte-carlo tree,s square law,search,starcraft},
pages = {12},
title = {{Tactical Planning Using MCTS in the Game of StarCraft}},
url = {https://project.dke.maastrichtuniversity.nl/games/files/bsc/Soemers{\_}BSc-paper.pdf},
year = {2014}
}
@article{Kaisers2010,
abstract = {Multi-agent learning is a crucial method to control or find solutions for systems, in which more than one entity needs to be adaptive. In todays interconnected world, such sys- tems are ubiquitous in many domains, including auctions in economics, swarm robotics in computer science, and politics in social sciences. Multi-agent learning is inherently more complex than single-agent learning and has a relatively thin theoretical framework supporting it. Recently, multi-agent learning dynamics have been linked to evolutionary game theory, allowing the interpretation of learning as an evolu- tion of competing policies in the mind of the learning agents. The dynamical system from evolutionary game theory that has been linked to Q-learning predicts the expected behav- ior of the learning agents. Closer analysis however allows for two interesting observations: the predicted behavior is not always the same as the actual behavior, and in case of deviation, the predicted behavior is more desirable. This discrepancy is elucidated in this article, and based on these new insights Frequency Adjusted Q- (FAQ-) learning is pro- posed. This variation of Q-learning perfectly adheres to the predictions of the evolutionary model for an arbitrarily large part of the policy space. In addition to the theoretical dis- cussion, experiments in the three classes of two-agent two- action games illustrate the superiority of FAQ-learning.},
author = {Kaisers, Michael and Tuyls, Karl},
doi = {10.11451838206},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaisers, Tuyls - 2010 - Frequency Adjusted Multi-agent Q-learning.pdf:pdf},
isbn = {978-0-9826571-1-9},
issn = {15582914},
journal = {Learning},
keywords = {dynamics,evolutionary game theory,multi agent learning,q learning,qlearning,replicator},
mendeley-tags = {qlearning},
pages = {309--316},
title = {{Frequency Adjusted Multi-agent Q-learning}},
url = {http://portal.acm.org/citation.cfm?id=1838250},
year = {2010}
}
@inproceedings{Tijsma2017,
abstract = {—Balancing the ratio between exploration and ex-ploitation is an important problem in reinforcement learning. This paper evaluates four different exploration strategies com-bined with Q-learning using random stochastic mazes to inves-tigate their performances. We will compare: UCB-1, softmax, -greedy, and pursuit. For this purpose we adapted the UCB-1 and pursuit strategies to be used in the Q-learning algorithm. The mazes consist of a single optimal goal state and two suboptimal goal states that lie closer to the starting position of the agent, which makes efficient exploration an important part of the learning agent. Furthermore, we evaluate two different kinds of reward functions, a normalized one with rewards between 0 and 1, and an unnormalized reward function that penalizes the agent for each step with a negative reward. We have performed an extensive grid-search to find the best parameters for each method and used the best parameters on novel randomly generated maze problems of different sizes. The results show that softmax exploration outperforms the other strategies, although it is harder to tune its temperature parameter. The worst performing exploration strategy is -greedy.},
author = {Tijsma, Arryon D and Drugan, Madalina M and Wiering, Marco A},
booktitle = {2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016},
doi = {10.1109/SSCI.2016.7849366},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tijsma, Drugan, Wiering - 2017 - Comparing exploration strategies for Q-learning in random stochastic mazes.pdf:pdf},
isbn = {9781509042401},
title = {{Comparing exploration strategies for Q-learning in random stochastic mazes}},
year = {2017}
}
@article{Guzdial2017,
abstract = {Intelligent agents need to be able to make predic-tions about their environment. In this work we present a novel approach to learn a forward simula-tion model via simple search over pixel input. We make use of a video game, Super Mario Bros., as an initial test of our approach as it represents a physics system that is significantly less complex than real-ity. We demonstrate the significant improvement of our approach in predicting future states compared with a baseline CNN and apply the learned model to train a game playing agent. Thus we evaluate the algorithm in terms of the accuracy and value of its output model.},
author = {Guzdial, Matthew and Li, Boyang and Riedl, Mark O},
doi = {10.24963/ijcai.2017/518},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guzdial, Li, Riedl - 2017 - Game Engine Learning from Video.pdf:pdf},
isbn = {9780999241103},
journal = {International Conference on Artificial Intelligence (IJCAI)},
title = {{Game Engine Learning from Video}},
url = {http://www.cc.gatech.edu/{~}riedl/pubs/ijcai17.pdf},
year = {2017}
}
@book{Sutton1998,
abstract = {Norton, B., {\&} Toohey, K. (Eds). (2004). Critical pedagogies and language learning. New York: Cambridge University Press. (362pp).},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
isbn = {0262193981},
issn = {0959-4388},
pages = {331},
pmid = {7888773},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@article{Borkar1988,
abstract = {This paper develops a new framework for the study of Markov decision processes in which the control problem is viewed as an optimization problem on the set of canonically induced measures on the trajectory space of the joint state and control process. This set is shown to be compact convex. One then associates with each of the usual cost criteria (infinite horizon discounted cost, finite horizon, control up to an exit time) a naturally defined occupation measure such that the cost is an integral of some function with respect to this measure. These measures are shown to form a compact convex set whose extreme points are characterized. Classical results about existence of optimal strategies are recovered from this and several applications to multicriteria and constrained optimization problems are briefly indicated.},
author = {Borkar, Vivek S.},
doi = {10.1007/BF00353877},
issn = {01788051},
journal = {Probability Theory and Related Fields},
number = {4},
pages = {583--602},
pmid = {20840904},
title = {{A convex analytic approach to Markov decision processes}},
volume = {78},
year = {1988}
}
@book{Bertsekas2007,
abstract = {A major revision of the second volume of a textbook on the far-ranging algorithmic methododogy of Dynamic Programming, which can be used for optimal control, Markovian decision problems, planning and sequential decision making under uncertainty, and discrete/combinatorial optimization. The second volume is oriented towards mathematical analysis and computation, and treats infinite horizon problems extensively. New features of the 3rd edition are: 1) A major enlargement in size and scope: the length has increased by more than 50{\%}, and most of the old material has been restructured and/or revised. 2) Extensive coverage (more than 100 pages) of recent research on simulation-based approximate dynamic programming (neuro-dynamic programming), which allow the practical application of dynamic programming to large and complex problems. 3) An in-depth development of the average cost problem (more than 100 pages), including a full analysis of multichain problems, and an extensive analysis of infinite-spaces problems. 4) An introduction to infinite state space stochastic shortest path problems. 5) Expansion of the theory and use of contraction mappings in infinite state space problems and in neuro-dynamic programming. 6) A substantive appendix on the mathematical measure-theoretic issues that must be addressed for a rigorous theory of stochastic dynamic programming. Much supplementary material can be found in the book's web page: http://www.athenasc.com/dpbook.html},
author = {Bertsekas, Dimitri P},
booktitle = {Journal of the Operational Research Society},
doi = {10.1057/jors.1996.103},
isbn = {1886529264},
number = {6},
pages = {543},
title = {{Dynamic Programming and Optimal Control, Vol. II}},
url = {http://portal.acm.org/citation.cfm?id=1396348},
volume = {2},
year = {2007}
}
@book{Bellman1957,
abstract = {Dynamic Programming is a recursive method for solving sequential decision problems (hereafter abbre- viated as SDP). Also known as backward induction, it is used to find optimal decision rules in games against nature and subgame perfect equilibria of dynamic multi-agent games, and competitive equilib- ria in dynamic economic models. Dynamic programming has enabled economists to formulate and solve a huge variety of problems involving sequential decision making under uncertainty, and as a result it is now widely regarded as the single most important tool in economics. Section 2 provides a brief history of dynamic programming. Section 3 discusses some of the main theoretical results underlying dynamic programming, and its relation to game theory and optimal control theory. Section 4 provides a brief survey on numerical dynamic programming. Section 5 surveys the experimental and econometric literature that uses dynamic programming to construct empirical models economic behavior.},
annote = {In this book, Bellman presents the notion of Markov Decision Processes (MDP)s and their use in control theory.},
author = {Bellman, R},
booktitle = {Princeton University Press Princeton New Jersey},
doi = {10.1108/eb059970},
isbn = {978-0-691-07951-6},
issn = {00029092},
pages = {342},
title = {{Dynamic Programming}},
url = {http://ajae.oxfordjournals.org/cgi/doi/10.2307/1241949},
volume = {70},
year = {1957}
}
@article{Tamar2017,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {1602.02867},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tamar et al. - 2017 - Value iteration networks.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {value iteration},
mendeley-tags = {value iteration},
number = {Nips},
pages = {4949--4953},
pmid = {172808},
title = {{Value iteration networks}},
year = {2017}
}
@article{Jaderberg2016,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880$\backslash${\%} expert human performance, and a challenging suite of first-person, three-dimensional $\backslash$emph{\{}Labyrinth{\}} tasks leading to a mean speedup in learning of 10{\$}\backslashtimes{\$} and averaging 87$\backslash${\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {1611.05397},
author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.05397},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2016 - Reinforcement Learning with Unsupervised Auxiliary Tasks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--14},
pmid = {23459267},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
url = {http://arxiv.org/abs/1611.05397},
year = {2016}
}
@article{Lau2012,
abstract = {In this paper, we propose to guide reinforcement learning (RL) with expert coordination knowledge for multi-agent problems managed by a central controller. The aim is to learn to use expert coordination knowledge to restrict the joint action space and to direct exploration towards more promising states, thereby improving the overall learning rate. We model such coordination knowledge as constraints and propose a two-level RL system that utilizes these constraints for online applications. Our declarative approach towards specifying coordination in multi-agent learning allows knowl-edge sharing between constraints and features (basis func-tions) for function approximation. Results on a soccer game and a tactical real-time strategy game show that coordi-nation constraints improve the learning rate compared to using only unary constraints. The two-level RL system also outperforms existing single-level approach that utilizes joint action selection via coordination graphs.},
author = {Lau, Qiangfeng Peter and Lee, Mong Li and Hsu, Wynne},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lau, Lee, Hsu - 2012 - Coordination Guided Reinforcement Learning.pdf:pdf},
isbn = {0-9817381-1-7, 978-0-9817381-1-6},
journal = {In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1. International Foundation for Autonomous Agents and Multiagent Systems},
keywords = {Control Methods,Experimentation Keywords Reinforcement learning,I28 [Artificial Intelligence],Learning,Performance,Problem Solving,Search General Terms Algorithms,coordination constraints,factored Markov decision process,guiding exploration},
pages = {215--222},
title = {{Coordination Guided Reinforcement Learning}},
url = {http://www.ifaamas.org/Proceedings/aamas2012/papers/1B{\_}1.pdf},
year = {2012}
}
@phdthesis{Watkins1989,
author = {Watkins, Christopher J.C.H.},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins - 1989 - Learning from delayed rewards.pdf:pdf},
school = {King's College},
title = {{Learning from delayed rewards}},
year = {1989}
}
@article{Thrun1993,
abstract = {Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failures—namely, a systematic overestimation of utility values. Using Watkins' Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings.},
author = {Thrun, Sebastian and Schwartz, Anton},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun, Schwartz - 1993 - Issues in Using Function Approximation for Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 4th Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum},
pages = {1--9},
title = {{Issues in Using Function Approximation for Reinforcement Learning}},
year = {1993}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Watkins, Christopher J.C.H. and Dayan, Peter},
doi = {10.1023/A:1022676722315},
eprint = {1412.3409},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Technical Note Q-Learning.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {15730565},
journal = {Machine Learning},
keywords = {(Formula presented.)-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
pmid = {7761831},
title = {{Technical Note: Q-Learning}},
volume = {8},
year = {1992}
}
@article{Greensmith2004,
abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
annote = {This article proposes various baselines to reduce varience in policy gradient methods without introducing variance.},
author = {Greensmith, Evan and Bartlett, Pl and Baxter, J},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greensmith, Bartlett, Baxter - 2004 - Variance reduction techniques for gradient estimates in reinforcement learning.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {The Journal of Machine Learning {\ldots}},
keywords = {actor-critic,baseline,gpomdp,policy gradient,reinforcement learning},
pages = {1471--1530},
title = {{Variance reduction techniques for gradient estimates in reinforcement learning}},
url = {http://dl.acm.org/citation.cfm?id=1044710},
volume = {5},
year = {2004}
}
@article{Henderson2017,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:pdf},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Vodopivec2017,
author = {Vodopivec, Tom},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vodopivec - 2017 - On Monte Carlo Tree Search and Reinforcement Learning.pdf:pdf},
title = {{On Monte Carlo Tree Search and Reinforcement Learning}},
volume = {60},
year = {2017}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf:pdf},
isbn = {9781424469178},
issn = {1701.07274},
pages = {1--16},
pmid = {25719670},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240},
year = {2017}
}
@article{Lowe2017,
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
archivePrefix = {arXiv},
arxivId = {1706.02275},
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
eprint = {1706.02275},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe et al. - 2017 - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf:pdf},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}},
url = {http://arxiv.org/abs/1706.02275},
year = {2017}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Pathak2017,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Prediction.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
volume = {2017-July},
year = {2017}
}
@inproceedings{Pathak2017a,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
isbn = {9781538607336},
issn = {21607516},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
volume = {2017-July},
year = {2017}
}
@article{Wender2012,
author = {Wender, Stefan and Watson, Ian},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wender, Watson - 2012 - Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft Broodwar.pdf:pdf},
isbn = {9781467311946},
keywords = {broodwar,qlearning,sarsa,starcraft},
mendeley-tags = {broodwar,qlearning,sarsa,starcraft},
pages = {402--408},
title = {{Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft : Broodwar}},
year = {2012}
}
@article{Deasis2017,
abstract = {Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD($\lambda$) elegantly unifies one-step TD prediction with Monte Carlo meth-ods through the use of eligibility traces and the trace-decay parameter $\lambda$. Currently, there are a multitude of algorithms that can be used to per-form TD control, including Sarsa, Q-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be ex-tended across multiple time steps to achieve bet-ter performance. Each of these algorithms is seemingly distinct, and no one dominates the oth-ers for all problems. In this paper, we study a new multi-step action-value algorithm called Q($\sigma$) which unifies and generalizes these exist-ing algorithms, while subsuming them as special cases. A new parameter, $\sigma$, is introduced to al-low the degree of sampling performed by the al-gorithm at each step during its backup to be con-tinuously varied, with Sarsa existing at one ex-treme (full sampling), and Expected Sarsa exist-ing at the other (pure expectation). Q($\sigma$) is gen-erally applicable to both on-and off-policy learn-ing, but in this work we focus on experiments in the on-policy case. Our results show that an in-termediate value of $\sigma$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater per-formance.},
author = {{De Asis}, Kristopher and Ca, Kldeasis@ualberta and Hernandez-Garcia, J Fernando and Ca, Jfhernan@ualberta and Holland, G Zacharias and Sutton, Richard S},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Asis et al. - 2017 - Multi-step Reinforcement Learning A Unifying Algorithm.pdf:pdf},
keywords = {Unifying,multi-step},
mendeley-tags = {Unifying,multi-step},
title = {{Multi-step Reinforcement Learning: A Unifying Algorithm}},
url = {https://arxiv.org/pdf/1703.01327.pdf},
year = {2017}
}
