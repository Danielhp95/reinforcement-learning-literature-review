@article{Hansen2004,
author = {Hansen, Eric and Bernstein, Daniel and Zilberstein, Shlomo},
file = {:home/sarios/papers/dynamic-programming-for-partially-observable-stochastic-games.pdf:pdf},
journal = {Nineteenth Conference on Artificial Conference (AAAI)},
pages = {709--715},
title = {{Dynamic Programming for Partially Observable Stochastic Games}},
year = {2004}
}
@article{Oliehoek2014,
abstract = {Bayesian methods for reinforcement learning are promising because they allow model uncertainty to be considered ex-plicitly and offer a principled way of dealing with the explo-ration/exploitation tradeoff. However, for multiagent sys-tems there have been few such approaches, and none of them apply to problems with state uncertainty. In this paper we fill this gap by proposing two frameworks for Bayesian RL for multiagent systems with state uncertainty. This includes a multiagent POMDP model where a team of agents oper-ates in a centralized fashion, but has uncertainty about the model of the environment. We also consider a best response model in which each agent also has uncertainty over the policies of the other agents. In each case, we seek to learn the appropriate models while acting in an online fashion. We transform the resulting problem into a planning problem and prove bounds on the solution quality in different situations. We demonstrate our methods using sample-based planning in several domains with varying levels of uncertainty about the model and the other agents' policies. Experimental re-sults show that overall, the approach is able to significantly decrease uncertainty and increase value when compared to initial models and policies.},
author = {Oliehoek, Frans A and Amato, Christopher},
file = {:home/sarios/papers/bayesian-rl-for-multiagent-systems-with-state-uncertainty.pdf:pdf},
journal = {AAMAS Workshop on Multiagent Sequential Decision Making Under Uncertainty, MSDM 2014},
number = {May},
title = {{Best Response Bayesian Reinforcement Learning for Multiagent Systems with State Uncertainty}},
year = {2014}
}
@article{Bansal2017,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archivePrefix = {arXiv},
arxivId = {1710.03748},
author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
eprint = {1710.03748},
file = {:home/sarios/papers/emergent-complexity-behaviour-via-multi-agent-competition.pdf:pdf},
pages = {1--12},
title = {{Emergent Complexity via Multi-Agent Competition}},
url = {http://arxiv.org/abs/1710.03748},
volume = {2},
year = {2017}
}
@article{Piunovskiy2012,
abstract = {In this paper, we show that a discounted continuous-time Markov decision process in Borel spaces with randomized history-dependent policies, arbitrarily unbounded transition rates and a non-negative reward rate is equivalent to a discrete-time Markov decision process. Based on a completely new proof, which does not involve Kolmogorov's forward equation, it is shown that the value function for both models is given by the minimal non-negative solution to the same Bellman equation. A verifiable necessary and sufficient condition for the finiteness of this value function is given, which induces a new condition for the non-explosion of the underlying controlled process. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
author = {Piunovskiy, Alexey and Zhang, Yi},
doi = {10.1007/s10957-012-0015-8},
file = {:home/sarios/papers/The-Transformation-Method-for-Continuous-Time-mdps.pdf:pdf},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Continuous-time Markov decision process,Discrete-time Markov decision process,History-dependent policies,Transformation method,Unbounded transition rates},
number = {2},
pages = {691--712},
title = {{The Transformation Method for Continuous-Time Markov Decision Processes}},
volume = {154},
year = {2012}
}
@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G},
doi = {10.1.1.4.6238-1},
file = {:home/sarios/papers/recent-advancements-in-hierarchical-rl.pdf:pdf},
isbn = {0924-6703},
issn = {0924-6703},
journal = {Most},
number = {5},
pages = {1--28},
title = {{Recent Advances in Hierarchical Reinforcement Learning Markov and Semi-Markov Decision Processes}},
volume = {13},
year = {2003}
}
@article{Jaderberg,
archivePrefix = {arXiv},
arxivId = {arXiv:1807.01281v1},
author = {Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
doi = {arXiv:1807.01281},
eprint = {arXiv:1807.01281v1},
file = {:home/sarios/papers/for-the-win.pdf:pdf},
title = {{Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}}
}
@article{Sutton2010,
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other arti cial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a sin- gle predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the sys- tem's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, re- ward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever ac- tions are taken by the system as a whole. Gradient-based temporal-di erence learning methods are used to learn ef- ciently and reliably with function approximation in this o -policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real- time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from o - policy experience. Horde is a signi cant incremental step towards a real-time architecture for ecient learning of gen-},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam},
doi = {10.1037/a0023964},
eprint = {NIHMS150003},
file = {:home/sarios/papers/horde.pdf:pdf},
isbn = {0-9826571-6-1, 978-0-9826571-6-4},
issn = {0982657161},
journal = {In Practice},
keywords = {artificial intelligence,difference learning,inforcement learning,knowledge representation,off policy learning,re,real time,robotics,temporal,value function approximation},
number = {1972},
pages = {761--768},
pmid = {21604833},
title = {{Horde : A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction Categories and Subject Descriptors}},
url = {http://webdocs.cs.ualberta.ca/{~}sutton/papers/horde-aamas-11.pdf},
volume = {2},
year = {2011}
}
@article{Schaul2015,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
archivePrefix = {arXiv},
arxivId = {1511.05952},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
doi = {10.1038/nature14236},
eprint = {1511.05952},
file = {:home/sarios/papers/prioritized-experience-replay.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {0028-0836},
pages = {1--21},
pmid = {25719670},
title = {{Prioritized Experience Replay}},
url = {http://arxiv.org/abs/1511.05952},
year = {2015}
}
@article{Andrychowicz2017,
abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {1707.01495},
file = {:home/sarios/papers/hindsight-experience-replay.pdf:pdf},
issn = {10495258},
number = {Nips},
title = {{Hindsight Experience Replay}},
url = {http://arxiv.org/abs/1707.01495},
year = {2017}
}
@article{Jaderberg2017,
abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present $\backslash$emph{\{}Population Based Training (PBT){\}}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
annote = {The paper presents a method to find a good set of hyperparameters and trainable parameters for a model. The procedure not only learns hyper parameters, but also a hyperparameter schedule (different set of hyper parameters as the training progresses).

It presents itself as an alternative to random / grid search and bayesian optimization. It's main advantage over the latter is that it can be used asynchronously in parallel. Bayesian optimization is (as far as I know), sequential.

Population based training is based on the idea of evaluating the performance of a model's hyper parameters and parameters (network weights), for instance by average reward in the last 10 episodes. If a model is performing worse than average, the hyper parameters and / or parameters of a better model will be copied over and mutated (to further explore the space of (hyper)parameters). 

Because of the greedy nature of the algorithm, they recommend a population of at least 20. Diminishing returns with bigger population sizes.},
archivePrefix = {arXiv},
arxivId = {1711.09846},
author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
eprint = {1711.09846},
file = {:home/sarios/papers/population-based-training.pdf:pdf},
title = {{Population Based Training of Neural Networks}},
url = {http://arxiv.org/abs/1711.09846},
year = {2017}
}
@article{Brown2018,
abstract = {A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.},
archivePrefix = {arXiv},
arxivId = {1805.08195},
author = {Brown, Noam and Sandholm, Tuomas and Amos, Brandon},
eprint = {1805.08195},
file = {:home/sarios/papers/Depth-Limited-Solving-for-imperfect-information-games.pdf:pdf},
title = {{Depth-Limited Solving for Imperfect-Information Games}},
url = {http://arxiv.org/abs/1805.08195},
year = {2018}
}
@article{Lipton2018,
archivePrefix = {arXiv},
arxivId = {1807.03341},
author = {Lipton, Zachary C and Steinhardt, Jacob},
eprint = {1807.03341},
file = {:home/sarios/papers/troubling-trends-in-machine-learning-scholarship.pdf:pdf},
pages = {1--15},
title = {{Troubling Trends in Machine Learning Scholarship}},
year = {2018}
}
@article{Littman1994,
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly...},
author = {Littman, Michael L.},
doi = {10.1016/B978-1-55860-335-6.50027-1},
file = {:home/sarios/papers/markov-games-as-gramework-for-multi-agent-rl.pdf:pdf},
isbn = {1-55860-335-2},
issn = {00493848},
journal = {Machine Learning Proceedings 1994},
keywords = {Environments},
mendeley-tags = {Environments},
pages = {157--163},
pmid = {17034835},
title = {{Markov games as a framework for multi-agent reinforcement learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271},
year = {1994}
}
@article{Lucas2018,
abstract = {This paper describes the N-Tuple Bandit Evolutionary Algorithm (NTBEA), an optimisation algorithm developed for noisy and expensive discrete (combinatorial) optimisation problems. The algorithm is applied to two game-based hyper-parameter optimisation problems. The N-Tuple system directly models the statistics, approximating the fitness and number of evaluations of each modelled combination of parameters. The model is simple, efficient and informative. Results show that the NTBEA significantly outperforms grid search and an estimation of distribution algorithm.},
archivePrefix = {arXiv},
arxivId = {1802.05991},
author = {Lucas, Simon M and Liu, Jialin and Perez-Liebana, Diego},
eprint = {1802.05991},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lucas, Liu, Perez-Liebana - 2018 - The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation.pdf:pdf},
title = {{The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation}},
url = {http://arxiv.org/abs/1802.05991},
year = {2018}
}
@article{Hessel2017,
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
eprint = {1710.02298},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcement Learning.pdf:pdf},
keywords = {dqn},
mendeley-tags = {dqn},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1710.02298},
year = {2017}
}
@article{Firoiu2017,
abstract = {There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.},
archivePrefix = {arXiv},
arxivId = {1702.06230},
author = {Firoiu, Vlad and Whitney, William F. and Tenenbaum, Joshua B.},
eprint = {1702.06230},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Firoiu, Whitney, Tenenbaum - 2017 - Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning.pdf:pdf},
title = {{Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.06230},
year = {2017}
}
@article{Li2018,
archivePrefix = {arXiv},
arxivId = {1805.02070},
author = {Li, Yu-Jhe and Chang, Hsin-Yu and Lin, Yu-Jing and Wu, Po-Wei and Wang, Yu-Chiang Frank},
eprint = {1805.02070},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2018 - Deep Reinforcement Learning for Playing 2.5D Fighting Games.pdf:pdf},
title = {{Deep Reinforcement Learning for Playing 2.5D Fighting Games}},
url = {http://arxiv.org/abs/1805.02070},
year = {2018}
}
@article{Graepel2004,
abstract = {We apply reinforcement learning to the problem of finding good policies for a fighting agent in a commercial computer game. The learning agent is trained using the SARSA algorithm for on-policy learning of an action-value function represented by linear and neural network function approximators. We discuss the selection and construction of features, actions, and rewards as well as other design choices necessary to integrate the learning process into the game. The learning agent is trained against the built-in AI of the game with different rewards encouraging aggressive or defensive behaviour. We show that the learning agent finds interesting (and partly near optimal) policies in accordance with the reward functions provided. We also discuss the particular challenges arising in the application of reinforcement learning to the domain of computer games.},
author = {Graepel, Thore and Herbrich, Ralf and Gold, Julian},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graepel, Herbrich, Gold - 2004 - Learning to Fight.pdf:pdf},
issn = {0885-842X},
journal = {Proceedings of the International Conference on Computer Games: Artificial Intelligence, Design and Education},
keywords = {fighting games,markov decision process,q-learning,reinforcement learning,sarsa},
pages = {193--200},
pmid = {3347383},
title = {{Learning to Fight}},
url = {http://www.herbrich.me/papers/graehergol04.pdf},
year = {2004}
}
@inproceedings{Bown2011,
abstract = {One of the goals of artificial life in the arts is to develop systems that exhibit creativity. We argue that creativity per se is a confusing goal for artificial life systems because of the complexity of the relationship between the system, its designers and users, and the creative domain. We analyse this confusion in terms of factors affecting individual human motivation in the arts, and the methods used to measure the success of artificial creative systems. We argue that an attempt to understand creative agency as a common thread in nature, human culture, human individuals and computational systems is a necessary step towards a better understanding of computational creativity. We define creative agency with respect to existing theories of creativity and consider human creative agency in terms of human social behaviour. We then propose how creative agency can be used to analyse the creativity of computational systems in artistic domains.},
address = {Berlin, Heidelberg},
author = {Bown, Oliver and McCormack, Jon},
booktitle = {Advances in Artificial Life. Darwin Meets von Neumann},
editor = {Kampis, George and Karsai, Istv{\'{a}}n and Szathm{\'{a}}ry, E{\"{o}}rs},
isbn = {978-3-642-21314-4},
pages = {254--261},
publisher = {Springer Berlin Heidelberg},
title = {{Creative Agency: A Clearer Goal for Artificial Life in the Arts}},
year = {2011}
}
@inproceedings{DInverno2015,
annote = {General comments. There are many questions posed in this paper. Some of which are not answered.


Summary. The paper explores, solely at a theoretical level, the role of AI system in the process of art creation from three different perspectives (as artists, AI researchers and as audience). (add a bit on some of the questions?)

The paper introduces the concept of Heroic AI and Collaborative AI in the framework of art creation. The former encapsualtes any AI system that autonomously produces a piece of art, without intervention or collaboration with outside entities. The latter encompases more abstract systems that communicate with external systems (humans or other AI systems), recieving and communicating feedback on the process of art creation in order to collaboratively generate art.

There are some insightful comments on the trajectory that AI research has been taking over the decades, focusing mainly on systems that could by themself compare or surpass human performance in various fields. Instead, the main body of the paper pays more attention to AI systems that have the potential to aid and support humans in their own work, in this case being the creation of art. From this, the authors propose to move aside from generating heroic AI systems in favour of researching systems of collaborative nature.

Major points:
-Most of the phrased questions are never answered. It is mentioned that questions will be studied in further detail, but most questions are only presented, not discussed.
- The last paragraph of section 2 talks about how the intended outcome of automation is to save time, and how increased automation has not yielded an increase in a human's average free time. The statement is well referenced and the view presented is well balanced. However, I failed to see the relevance of this statement in the broader context of the paper. This paper dwells into AI systems and their relevance in the process of creating art, not on the time efficiency that could come from automating more parts of the artistic process, or the unexpected increase of "time famine" that could come from it. 

Minor points:
-Interesting small comentary on the relevancy of art created by other specices.
- The comment "This tradition [bringing new form of creativity] into the world is renewed... with a greater ironic twist". You indicate that the irony lies in the fact that a heroic AI is restricted to its programmed behaviour, which is based on the author's already existing notion of art. The tone of this note in the paper is dismissal, however this is a systematical restriction that modern "heroic AI" systems suffer from.},
author = {D'Inverno, Mark and McCormack, Jon},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/D'Inverno, McCormack - 2015 - Heroic versus collaborative AI for the arts.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
pages = {2438--2444},
title = {{Heroic versus collaborative AI for the arts}},
volume = {2015-Janua},
year = {2015}
}
@article{Mccormack2016,
author = {Mccormack, Jon and Inverno, Mark},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mccormack, Inverno - 2016 - Designing Improvisational Interfaces.pdf:pdf},
number = {June},
pages = {103--110},
title = {{Designing Improvisational Interfaces}},
year = {2016}
}
@article{Silver2014,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value func- tion. This simple form means that the deter- ministic policy gradient can be estimated much more efficiently than the usual stochastic pol- icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter- parts in high-dimensional action spaces.},
annote = {Deterministic policy gradient is an off-policy, model free policy gradient algorithm. The agent moves in the parameter space of the policy to approximate a target deterministic policy from an exploratory behavioural policy.

100{\%} Talk how this is a corner case of stochastic policy gradient algorithms in your literature review.},
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2014 - Deterministic Policy Gradient Algorithms.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
pages = {387--395},
title = {{Deterministic Policy Gradient Algorithms}},
year = {2014}
}
@article{Andersen2017,
author = {Andersen, Per-arne},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andersen - 2017 - Deep RTS A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games.pdf:pdf},
title = {{Deep RTS : A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games}},
year = {2017}
}
@article{Ontanon2013,
abstract = {Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Na¨ ıve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS al- gorithm based on Na¨ ıve Sampling called Na¨ ıveMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, Na¨ ıveMCTS performs significantly better than other algorithms. Introduction},
author = {Onta{\~{n}}{\'{o}}n, Santiago},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Onta{\~{n}}{\'{o}}n - 2013 - The combinatorial multi-armed bandit problem and its application to real-time strategy games.pdf:pdf},
journal = {Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
pages = {58--64},
title = {{The combinatorial multi-armed bandit problem and its application to real-time strategy games}},
url = {http://www.aaai.org/ocs/index.php/AIIDE/AIIDE13/paper/viewPaper/7377},
year = {2013}
}
@article{Lin1993,
abstract = {Reinforcement learning agents are adaptive, reactive, and self-supervised. The aim of this dissertation is to extend the state of the art of reinforcement learning and enable its applications to complex robot-learning problems. In particular, it focuses on two issues. First, learning from sparse and delayed reinforcement signals is hard and in general a slow process. Techniques for reducing learning time must be devised. Second, most existing reinforcement learning methods assume that the world is a Markov decision process. This assumption is too strong for many robot tasks of interest.$\backslash$r$\backslash$nThis dissertation demonstrates how we can possibly overcome the slow learning problem and tackle non-Markovian environments, making reinforcement learning more practical for realistic robot tasks: (1) Reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization, resulting in a significant learning speedup. Neural networks are used in this dissertation, and they generalize effectively even in the presence of noise and a large of binary and real-valued inputs. (2) Reinforcement learning agents can save many learning trials by using an action model, which can be learned on-line. With a model, an agent can mentally experience the effects of its actions without actually executing them. Experience replay is a simple technique that implements this idea, and is shown to be effective in reducing the number of action executions required. (3) Reinforcement learning agents can take advantage of instructive training instances provided by human teachers, resulting in a significant learning speedup. Teaching can also help learning agents avoid local optima during the search for optimal control. Simulation experiments indicate that even a small amount of teaching can save agents many learning trials. (4) Reinforcement learning agents can significantly reduce learning time by hierarchical learning--they first solve elementary learning problems and then combine solutions to the elementary problems to solve a complex problem. Simulation experiments indicate that a robot with hierarchical learning can solve a complex problem, which otherwise is hardly solvable within a reasonable time. (5) Reinforcement learning agents can deal with a wide range of non-Markovian environments by having a memory of their past. Three memory architectures are discussed. They work reasonably well for a variety of simple problems. One of them is also successfully applied to a nontrivial non-Markovian robot task.$\backslash$r$\backslash$n$\backslash$r$\backslash$nThe results of this dissertation rely on computer simulation, including (1) an agent operating in a dynamic and hostile environment and (2) a mobile robot operating in a noisy and non-Markovian environment. The robot simulator is physically realistic. This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning.},
author = {Lin, Long-ji},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin - 1993 - Reinforcement Learning for Robots Using Neural Networks.pdf:pdf},
journal = {Report, CMU},
keywords = {off policy,replay buffer},
mendeley-tags = {off policy,replay buffer},
pages = {1--155},
title = {{Reinforcement Learning for Robots Using Neural Networks}},
year = {1993}
}
@article{Bhatnagar2009,
abstract = {We present four new reinforcement learning algorithms based on actor-critic, natural-gradient and function-approximation ideas, and we provide their convergence proofs. Actor-critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function-approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor-critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor-critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms. {\textcopyright} 2009 Elsevier Ltd.},
author = {Bhatnagar, Shalabh and Sutton, Richard S. and Ghavamzadeh, Mohammad and Lee, Mark},
doi = {10.1016/j.automatica.2009.07.008},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatnagar et al. - 2009 - Natural actor-critic algorithms.pdf:pdf},
isbn = {0005-1098},
issn = {00051098},
journal = {Automatica},
keywords = {Actor-critic reinforcement learning algorithms,Approximate dynamic programming,Function approximation,Natural gradient,Policy-gradient methods,Temporal difference learning,Two-timescale stochastic approximation,actor critic},
mendeley-tags = {actor critic},
number = {11},
pages = {2471--2482},
publisher = {Elsevier Ltd},
title = {{Natural actor-critic algorithms}},
url = {http://dx.doi.org/10.1016/j.automatica.2009.07.008},
volume = {45},
year = {2009}
}
@article{Degris2012,
annote = {First off policy algorithm, using a behavioural policy for exploration and importance sampling techniques to weigh the update to actor parameters. Eligibility traces are used to calculate updates.},
archivePrefix = {arXiv},
arxivId = {arXiv:1205.4839v5},
author = {Degris, Thomas and White, Martha and Sutton, Richard S},
doi = {10.1.1.385.4471},
eprint = {arXiv:1205.4839v5},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Degris, White, Sutton - 2012 - Off-Policy Actor-Critic.pdf:pdf},
isbn = {978-1-4503-1285-1},
issn = {{\textless}null{\textgreater}},
keywords = {actor critic},
mendeley-tags = {actor critic},
title = {{Off-Policy Actor-Critic}},
year = {2012}
}
@article{Konda2000,
abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information pro-vided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
archivePrefix = {arXiv},
arxivId = {1607.07086},
author = {Konda, Vijay R and Tsitsiklis, John N},
doi = {10.1137/S0363012901385691},
eprint = {1607.07086},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konda, Tsitsiklis - 2000 - Actor-Critic Algorithms.pdf:pdf},
isbn = {0363-0129},
issn = {0363-0129},
journal = {Nips},
keywords = {actor critic,actor-critic algorithms,markov decision processes,reinforcement learning,stochas-},
mendeley-tags = {actor critic},
number = {4},
pages = {1143--1166},
pmid = {21222527},
title = {{Actor-Critic Algorithms}},
volume = {42},
year = {2000}
}
@article{Baxter2001,
abstract = {Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes POMDPs controlled by parameterized stochastic policies. A similar algorithm was proposed by (Kimura et al. 1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free beta (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of GPOMDP, and show how the correct choice of the parameter beta is related to the mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter et al., this volume) we show how the gradient estimates generated by GPOMDP can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward.},
archivePrefix = {arXiv},
arxivId = {1106.0665},
author = {Baxter, Jonathan and Bartlett, Peter L.},
doi = {10.1613/jair.806},
eprint = {1106.0665},
isbn = {10769757 (ISSN)},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {319--350},
title = {{Infinite-horizon policy-gradient estimation}},
volume = {15},
year = {2001}
}
@article{Song2018,
abstract = {In this technical report, we consider an approach that combines the PPO objective and K-FAC natural gradient optimization, for which we call PPOKFAC. We perform a range of empirical analysis on various aspects of the algorithm, such as sample complexity, training speed, and sensitivity to batch size and training epochs. We observe that PPOKFAC is able to outperform PPO in terms of sample complexity and speed in a range of MuJoCo environments, while being scalable in terms of batch size. In spite of this, it seems that adding more epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to be worse than its A2C counterpart, ACKTR.},
annote = {Useful overview of policy gradient and trust region famous methods.},
archivePrefix = {arXiv},
arxivId = {1801.05566},
author = {Song, Jiaming and Wu, Yuhuai},
eprint = {1801.05566},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Wu - 2018 - An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients.pdf:pdf},
pages = {1--8},
title = {{An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients}},
url = {http://arxiv.org/abs/1801.05566},
year = {2018}
}
@article{UnityMLAgents,
author = {Unity},
title = {{Unity Machine Learning Agents}},
url = {https://unity3d.com/machine-learning/},
year = {2017}
}
@misc{Broodwar,
author = {Heinermann, Adam},
title = {{BWAPI: Brood war api, an api for interacting with starcraft: Broodwar}},
url = {https://bwapi.github.io/},
year = {2009}
}
@article{Synnaeve2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.00625v2},
author = {Synnaeve, Gabriel and Nardelli, Nantas and Auvolat, Alex and Chintala, Soumith and Lacroix, Timoth{\'{e}}e and Lin, Zeming and Richoux, Florian and Usunier, Nicolas},
eprint = {arXiv:1611.00625v2},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Synnaeve et al. - 2016 - TorchCraft a Library for Machine Learning Research on Real-Time Strategy Games arXiv 1611 . 00625v2 cs . LG.pdf:pdf},
pages = {1--6},
title = {{TorchCraft : a Library for Machine Learning Research on Real-Time Strategy Games arXiv : 1611 . 00625v2 [ cs . LG ] 3 Nov 2016}},
year = {2016}
}
@inproceedings{Kempka2017,
abstract = {The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.},
archivePrefix = {arXiv},
arxivId = {1605.02097},
author = {Kempka, Michal and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Jaskowski, Wojciech},
booktitle = {IEEE Conference on Computatonal Intelligence and Games, CIG},
doi = {10.1109/CIG.2016.7860433},
eprint = {1605.02097},
isbn = {9781509018833},
issn = {23254289},
keywords = {FPS,deep reinforcement learning,first-person perspective games,neural networks,video games,visual learning,visual-based reinforcement learning},
title = {{ViZDoom: A Doom-based AI research platform for visual reinforcement learning}},
year = {2017}
}
@article{Brockman2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brockman et al. - 2016 - OpenAI Gym.pdf:pdf},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Vinyals2017,
abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
archivePrefix = {arXiv},
arxivId = {1708.04782},
author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"{u}}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and van Hasselt, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
doi = {https://deepmind.com/documents/110/sc2le.pdf},
eprint = {1708.04782},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinyals et al. - 2017 - StarCraft II A New Challenge for Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1708.04782},
title = {{StarCraft II: A New Challenge for Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.04782},
year = {2017}
}
@inproceedings{Perot2017,
abstract = {The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.},
archivePrefix = {arXiv},
arxivId = {1605.02097},
author = {Perot, Etienne and Jaritz, Maximilian and Toromanoff, Marin and Charette, Raoul De},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.64},
eprint = {1605.02097},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perot et al. - 2017 - End-to-End Driving in a Realistic Racing Game with Deep Reinforcement Learning.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
keywords = {deep reinforcement learning,first-person perspective games,fps,neural networks,video games,visual learning,visual-based reinforcement learning},
pages = {474--475},
title = {{End-to-End Driving in a Realistic Racing Game with Deep Reinforcement Learning}},
volume = {2017-July},
year = {2017}
}
@inproceedings{Bellemare2015,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
archivePrefix = {arXiv},
arxivId = {1207.4708},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.1613/jair.3912},
eprint = {1207.4708},
isbn = {9781577357384},
issn = {10450823},
pages = {4148--4152},
title = {{The arcade learning environment: An evaluation platform for general agents}},
volume = {2015-Janua},
year = {2015}
}
@article{Lanctot2017,
abstract = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
archivePrefix = {arXiv},
arxivId = {1711.00832},
author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
eprint = {1711.00832},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanctot et al. - 2017 - A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.pdf:pdf},
number = {Nips},
title = {{A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning}},
url = {http://arxiv.org/abs/1711.00832},
year = {2017}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
doi = {10.1561/2200000006},
eprint = {1509.02971},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learning.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
pmid = {24966830},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Deisenroth2011,
abstract = {In this paper, we introduce pilco, a practi-cal, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforce-ment learning, in a principled way. By learn-ing a probabilistic dynamics model and ex-plicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprece-dented learning efficiency on challenging and high-dimensional control tasks.},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deisenroth, Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach to Policy Search.pdf:pdf},
title = {{PILCO: A Model-Based and Data-Efficient Approach to Policy Search}},
url = {http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf},
year = {2011}
}
@article{Pascanu2017,
abstract = {Conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the "Imagination-based Planner", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a "plan context" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex "imagination tree" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.},
archivePrefix = {arXiv},
arxivId = {1707.06170},
author = {Pascanu, Razvan and Li, Yujia and Vinyals, Oriol and Heess, Nicolas and Buesing, Lars and Racani{\`{e}}re, Sebastien and Reichert, David and Weber, Th{\'{e}}ophane and Wierstra, Daan and Battaglia, Peter},
doi = {10.1287/mnsc.45.4.560},
eprint = {1707.06170},
number = {November},
title = {{Learning model-based planning from scratch}},
url = {http://arxiv.org/abs/1707.06170},
year = {2017}
}
@article{Tapas1999,
author = {Das, Tapas K. and Gosavi, Abhijit and Mahadevan, Sridhar and Marchalleck, Nicholas},
doi = {10.1287/mnsc.45.4.560},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Das et al. - 1999 - Solving Semi-Markov Decision Problems Using Average Reward Reinforcement Learning.pdf:pdf},
issn = {0025-1909},
journal = {Management Science},
month = {apr},
number = {4},
pages = {560--574},
title = {{Solving Semi-Markov Decision Problems Using Average Reward Reinforcement Learning}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.45.4.560},
volume = {45},
year = {1999}
}
@article{Neto2005,
abstract = {Interest in robotic and software agents has increased a lot in the last decades. They allow us to do tasks that we would hardly accomplish otherwise. Par-ticularly, multi-agent systems motivate distributed solutions that can be cheaper and more efficient than centralized single-agent ones. In this context, reinforcement learning provides a way for agents to com-pute optimal ways of performing the required tasks, with just a small in-struction indicating if the task was or was not accomplished. Learning in multi-agent systems, however, poses the problem of non-stationarity due to interactions with other agents. In fact, the RL methods for the single agent domain assume stationarity of the environment and cannot be applied directly. This work is divided in two main parts. In the first one, the reinforcement learning framework for single-agent domains is analyzed and some classical solutions presented, based on Markov decision processes. In the second part, the multi-agent domain is analyzed, borrowing tools from game theory, namely stochastic games, and the most significant work on learning optimal decisions for this type of systems is presented. ii Contents Abstract 1},
author = {Neto, Gon{\c{c}}alo},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neto - 2005 - From Single-Agent to Multi-Agent Reinforcement Learning Foundational Concepts and Methods Learning Theory Course.pdf:pdf},
number = {May},
title = {{From Single-Agent to Multi-Agent Reinforcement Learning: Foundational Concepts and Methods Learning Theory Course}},
year = {2005}
}
@article{Wu2017,
abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
archivePrefix = {arXiv},
arxivId = {1708.05144},
author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
eprint = {1708.05144},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.pdf:pdf},
pages = {1--14},
title = {{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}},
url = {http://arxiv.org/abs/1708.05144},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
doi = {10.1177/0956797613514093},
eprint = {1602.01783},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
keywords = {actor critic},
mendeley-tags = {actor critic},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Sutton1999,
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the ¯rst time that a version of policy iteration with arbitrary di{\textregistered}erentiable function approximation is convergent to a locally optimal policy.},
annote = {This paper introduces a (somewhat dated) introduction to policy gradient methods which can prove valuable in lit review. It offers a digsted version of policy gradient therom as compared with many other papers.},
author = {Sutton, Richard S and Mcallester, David and Singh, Satinder and Mansour, Yishay},
doi = {10.1.1.37.9714},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf:pdf},
isbn = {0-262-19450-3},
issn = {0047-2875},
journal = {In Advances in Neural Information Processing Systems 12},
pages = {1057--1063},
title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
year = {1999}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
annote = {This is the article where introducing a baseline in reinforce is shown to keep the estimation unbiased.},
author = {Williams, Ronald J.},
doi = {10.1023/A:1022672621406},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - 1992 - Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning(2).pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis,policy-gradient},
mendeley-tags = {policy-gradient},
number = {3},
pages = {229--256},
pmid = {903},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
volume = {8},
year = {1992}
}
@article{Abdallah2016,
abstract = {Q-learning (QL) is a popular reinforcement learning algorithm that is guaranteed to converge to op-timal policies in Markov decision processes. However, QL exhibits an artifact: in expectation, the effective rate of updating the value of an action depends on the probability of choosing that action. In other words, there is a tight coupling between the learning dynamics and underlying execution policy. This coupling can cause performance degradation in noisy non-stationary environments. Here, we introduce Repeated Update Q-learning (RUQL), a learning algorithm that resolves the undesirable artifact of Q-learning while maintaining simplicity. We analyze the similarities and differences between RUQL, QL, and the closest state-of-the-art algorithms theoretically. Our analysis shows that RUQL maintains the convergence guarantee of QL in stationary environments, while relaxing the coupling between the execution policy and the learning dynamics. Experimental results confirm the theoretical insights and show how RUQL outperforms both QL and the closest state-of-the-art algorithms in noisy non-stationary environments.},
author = {Abdallah, Sherief and Org, Shario@ieee and Kaisers, Michael and Nl, Kaisers@cwi},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdallah et al. - 2016 - Addressing Environment Non-Stationarity by Repeating Q-learning Updates.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {multi-agent learning,non-stationary environ-,q-learning,qlearning,reinforcement learning},
mendeley-tags = {qlearning},
pages = {1--31},
title = {{Addressing Environment Non-Stationarity by Repeating Q-learning Updates}},
url = {http://www.jmlr.org/papers/volume17/14-037/14-037.pdf},
volume = {17},
year = {2016}
}
@article{browne2012survey,
author = {Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browne et al. - 2012 - A survey of {\{}Monte Carlo Tree Search{\}} methods.pdf:pdf},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {mcts},
mendeley-tags = {mcts},
number = {1},
pages = {1--43},
publisher = {IEEE},
title = {{A survey of {\{}Monte Carlo Tree Search{\}} methods}},
volume = {4},
year = {2012}
}
@article{Sutton1991,
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S.},
doi = {10.1145/122344.122377},
eprint = {arXiv:1011.1669v3},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton - 1991 - Dyna, an integrated architecture for learning, planning, and reacting.pdf:pdf},
isbn = {1-55860-141-4},
issn = {01635719},
journal = {ACM SIGART Bulletin},
number = {4},
pages = {160--163},
pmid = {15003161},
title = {{Dyna, an integrated architecture for learning, planning, and reacting}},
url = {http://portal.acm.org/citation.cfm?doid=122344.122377},
volume = {2},
year = {1991}
}
@article{Soemers2014,
abstract = {This thesis describes how Monte-Carlo Tree Search (MCTS) can be applied to perform tac-tical planning for an intelligent agent playing full games of StarCraft: Brood War. StarCraft is a Real-Time Strategy game, which has a large state-space, is played in real-time, and com-monly features two opposing players, capable of acting simultaneously. Using the MCTS al-gorithm for tactical planning is shown to in-crease the performance of the agent, compared to a scripted approach, when competing on a bot ladder. A combat model, based on Lanch-ester's Square Law, is described, and shown to achieve another gain in performance when used in Monte-Carlo simulations as replacement for a heuristic linear model. Finally, the MAST enhancement to the Playout Policy of MCTS is described, but it is found not to have a signifi-cant impact on the agent's performance.},
author = {Soemers, Dennis},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soemers - 2014 - Tactical Planning Using MCTS in the Game of StarCraft.pdf:pdf},
keywords = {lanchester,monte-carlo tree,s square law,search,starcraft},
pages = {12},
title = {{Tactical Planning Using MCTS in the Game of StarCraft}},
url = {https://project.dke.maastrichtuniversity.nl/games/files/bsc/Soemers{\_}BSc-paper.pdf},
year = {2014}
}
@inproceedings{Tijsma2017,
abstract = {—Balancing the ratio between exploration and ex-ploitation is an important problem in reinforcement learning. This paper evaluates four different exploration strategies com-bined with Q-learning using random stochastic mazes to inves-tigate their performances. We will compare: UCB-1, softmax, -greedy, and pursuit. For this purpose we adapted the UCB-1 and pursuit strategies to be used in the Q-learning algorithm. The mazes consist of a single optimal goal state and two suboptimal goal states that lie closer to the starting position of the agent, which makes efficient exploration an important part of the learning agent. Furthermore, we evaluate two different kinds of reward functions, a normalized one with rewards between 0 and 1, and an unnormalized reward function that penalizes the agent for each step with a negative reward. We have performed an extensive grid-search to find the best parameters for each method and used the best parameters on novel randomly generated maze problems of different sizes. The results show that softmax exploration outperforms the other strategies, although it is harder to tune its temperature parameter. The worst performing exploration strategy is -greedy.},
author = {Tijsma, Arryon D and Drugan, Madalina M and Wiering, Marco A},
booktitle = {2016 IEEE Symposium Series on Computational Intelligence, SSCI 2016},
doi = {10.1109/SSCI.2016.7849366},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tijsma, Drugan, Wiering - 2017 - Comparing exploration strategies for Q-learning in random stochastic mazes.pdf:pdf},
isbn = {9781509042401},
title = {{Comparing exploration strategies for Q-learning in random stochastic mazes}},
year = {2017}
}
@article{Kaisers2010,
abstract = {Multi-agent learning is a crucial method to control or find solutions for systems, in which more than one entity needs to be adaptive. In todays interconnected world, such sys- tems are ubiquitous in many domains, including auctions in economics, swarm robotics in computer science, and politics in social sciences. Multi-agent learning is inherently more complex than single-agent learning and has a relatively thin theoretical framework supporting it. Recently, multi-agent learning dynamics have been linked to evolutionary game theory, allowing the interpretation of learning as an evolu- tion of competing policies in the mind of the learning agents. The dynamical system from evolutionary game theory that has been linked to Q-learning predicts the expected behav- ior of the learning agents. Closer analysis however allows for two interesting observations: the predicted behavior is not always the same as the actual behavior, and in case of deviation, the predicted behavior is more desirable. This discrepancy is elucidated in this article, and based on these new insights Frequency Adjusted Q- (FAQ-) learning is pro- posed. This variation of Q-learning perfectly adheres to the predictions of the evolutionary model for an arbitrarily large part of the policy space. In addition to the theoretical dis- cussion, experiments in the three classes of two-agent two- action games illustrate the superiority of FAQ-learning.},
author = {Kaisers, Michael and Tuyls, Karl},
doi = {10.11451838206},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaisers, Tuyls - 2010 - Frequency Adjusted Multi-agent Q-learning.pdf:pdf},
isbn = {978-0-9826571-1-9},
issn = {15582914},
journal = {Learning},
keywords = {dynamics,evolutionary game theory,multi agent learning,q learning,qlearning,replicator},
mendeley-tags = {qlearning},
pages = {309--316},
title = {{Frequency Adjusted Multi-agent Q-learning}},
url = {http://portal.acm.org/citation.cfm?id=1838250},
year = {2010}
}
@article{Guzdial2017,
abstract = {Intelligent agents need to be able to make predic-tions about their environment. In this work we present a novel approach to learn a forward simula-tion model via simple search over pixel input. We make use of a video game, Super Mario Bros., as an initial test of our approach as it represents a physics system that is significantly less complex than real-ity. We demonstrate the significant improvement of our approach in predicting future states compared with a baseline CNN and apply the learned model to train a game playing agent. Thus we evaluate the algorithm in terms of the accuracy and value of its output model.},
author = {Guzdial, Matthew and Li, Boyang and Riedl, Mark O},
doi = {10.24963/ijcai.2017/518},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guzdial, Li, Riedl - 2017 - Game Engine Learning from Video.pdf:pdf},
isbn = {9780999241103},
journal = {International Conference on Artificial Intelligence (IJCAI)},
title = {{Game Engine Learning from Video}},
url = {http://www.cc.gatech.edu/{~}riedl/pubs/ijcai17.pdf},
year = {2017}
}
@book{Sutton1998,
abstract = {Norton, B., {\&} Toohey, K. (Eds). (2004). Critical pedagogies and language learning. New York: Cambridge University Press. (362pp).},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
isbn = {0262193981},
issn = {0959-4388},
pages = {331},
pmid = {7888773},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@article{Montague1999,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Montague, P.Read},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}
@article{Montague1999a,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Montague, P.Read},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}
@article{Borkar1988,
abstract = {This paper develops a new framework for the study of Markov decision processes in which the control problem is viewed as an optimization problem on the set of canonically induced measures on the trajectory space of the joint state and control process. This set is shown to be compact convex. One then associates with each of the usual cost criteria (infinite horizon discounted cost, finite horizon, control up to an exit time) a naturally defined occupation measure such that the cost is an integral of some function with respect to this measure. These measures are shown to form a compact convex set whose extreme points are characterized. Classical results about existence of optimal strategies are recovered from this and several applications to multicriteria and constrained optimization problems are briefly indicated.},
author = {Borkar, Vivek S.},
doi = {10.1007/BF00353877},
issn = {01788051},
journal = {Probability Theory and Related Fields},
number = {4},
pages = {583--602},
pmid = {20840904},
title = {{A convex analytic approach to Markov decision processes}},
volume = {78},
year = {1988}
}
@book{Bertsekas2007,
abstract = {A major revision of the second volume of a textbook on the far-ranging algorithmic methododogy of Dynamic Programming, which can be used for optimal control, Markovian decision problems, planning and sequential decision making under uncertainty, and discrete/combinatorial optimization. The second volume is oriented towards mathematical analysis and computation, and treats infinite horizon problems extensively. New features of the 3rd edition are: 1) A major enlargement in size and scope: the length has increased by more than 50{\%}, and most of the old material has been restructured and/or revised. 2) Extensive coverage (more than 100 pages) of recent research on simulation-based approximate dynamic programming (neuro-dynamic programming), which allow the practical application of dynamic programming to large and complex problems. 3) An in-depth development of the average cost problem (more than 100 pages), including a full analysis of multichain problems, and an extensive analysis of infinite-spaces problems. 4) An introduction to infinite state space stochastic shortest path problems. 5) Expansion of the theory and use of contraction mappings in infinite state space problems and in neuro-dynamic programming. 6) A substantive appendix on the mathematical measure-theoretic issues that must be addressed for a rigorous theory of stochastic dynamic programming. Much supplementary material can be found in the book's web page: http://www.athenasc.com/dpbook.html},
author = {Bertsekas, Dimitri P},
booktitle = {Journal of the Operational Research Society},
doi = {10.1057/jors.1996.103},
isbn = {1886529264},
number = {6},
pages = {543},
title = {{Dynamic Programming and Optimal Control, Vol. II}},
url = {http://portal.acm.org/citation.cfm?id=1396348},
volume = {2},
year = {2007}
}
@book{Bellman1957,
abstract = {Dynamic Programming is a recursive method for solving sequential decision problems (hereafter abbre- viated as SDP). Also known as backward induction, it is used to find optimal decision rules in games against nature and subgame perfect equilibria of dynamic multi-agent games, and competitive equilib- ria in dynamic economic models. Dynamic programming has enabled economists to formulate and solve a huge variety of problems involving sequential decision making under uncertainty, and as a result it is now widely regarded as the single most important tool in economics. Section 2 provides a brief history of dynamic programming. Section 3 discusses some of the main theoretical results underlying dynamic programming, and its relation to game theory and optimal control theory. Section 4 provides a brief survey on numerical dynamic programming. Section 5 surveys the experimental and econometric literature that uses dynamic programming to construct empirical models economic behavior.},
annote = {In this book, Bellman presents the notion of Markov Decision Processes (MDP)s and their use in control theory.},
author = {Bellman, R},
booktitle = {Princeton University Press Princeton New Jersey},
doi = {10.1108/eb059970},
isbn = {978-0-691-07951-6},
issn = {00029092},
pages = {342},
title = {{Dynamic Programming}},
url = {http://ajae.oxfordjournals.org/cgi/doi/10.2307/1241949},
volume = {70},
year = {1957}
}
@article{Tamar2017,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {1602.02867},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tamar et al. - 2017 - Value iteration networks.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {value iteration},
mendeley-tags = {value iteration},
number = {Nips},
pages = {4949--4953},
pmid = {172808},
title = {{Value iteration networks}},
year = {2017}
}
@article{Jaderberg2016,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880$\backslash${\%} expert human performance, and a challenging suite of first-person, three-dimensional $\backslash$emph{\{}Labyrinth{\}} tasks leading to a mean speedup in learning of 10{\$}\backslashtimes{\$} and averaging 87$\backslash${\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {1611.05397},
author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.05397},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2016 - Reinforcement Learning with Unsupervised Auxiliary Tasks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--14},
pmid = {23459267},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
url = {http://arxiv.org/abs/1611.05397},
year = {2016}
}
@article{Lau2012,
abstract = {In this paper, we propose to guide reinforcement learning (RL) with expert coordination knowledge for multi-agent problems managed by a central controller. The aim is to learn to use expert coordination knowledge to restrict the joint action space and to direct exploration towards more promising states, thereby improving the overall learning rate. We model such coordination knowledge as constraints and propose a two-level RL system that utilizes these constraints for online applications. Our declarative approach towards specifying coordination in multi-agent learning allows knowl-edge sharing between constraints and features (basis func-tions) for function approximation. Results on a soccer game and a tactical real-time strategy game show that coordi-nation constraints improve the learning rate compared to using only unary constraints. The two-level RL system also outperforms existing single-level approach that utilizes joint action selection via coordination graphs.},
author = {Lau, Qiangfeng Peter and Lee, Mong Li and Hsu, Wynne},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lau, Lee, Hsu - 2012 - Coordination Guided Reinforcement Learning.pdf:pdf},
isbn = {0-9817381-1-7, 978-0-9817381-1-6},
journal = {In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1. International Foundation for Autonomous Agents and Multiagent Systems},
keywords = {Control Methods,Experimentation Keywords Reinforcement learning,I28 [Artificial Intelligence],Learning,Performance,Problem Solving,Search General Terms Algorithms,coordination constraints,factored Markov decision process,guiding exploration},
pages = {215--222},
title = {{Coordination Guided Reinforcement Learning}},
url = {http://www.ifaamas.org/Proceedings/aamas2012/papers/1B{\_}1.pdf},
year = {2012}
}
@phdthesis{Watkins1989,
author = {Watkins, Christopher J.C.H.},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins - 1989 - Learning from delayed rewards.pdf:pdf},
school = {King's College},
title = {{Learning from delayed rewards}},
year = {1989}
}
@article{Thrun1993,
abstract = {Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failures—namely, a systematic overestimation of utility values. Using Watkins' Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings.},
author = {Thrun, Sebastian and Schwartz, Anton},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun, Schwartz - 1993 - Issues in Using Function Approximation for Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 4th Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum},
pages = {1--9},
title = {{Issues in Using Function Approximation for Reinforcement Learning}},
year = {1993}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Watkins, Christopher J.C.H. and Dayan, Peter},
doi = {10.1023/A:1022676722315},
eprint = {1412.3409},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Technical Note Q-Learning.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {15730565},
journal = {Machine Learning},
keywords = {(Formula presented.)-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
pmid = {7761831},
title = {{Technical Note: Q-Learning}},
volume = {8},
year = {1992}
}
@article{Greensmith2004,
abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
annote = {This article proposes various baselines to reduce varience in policy gradient methods without introducing variance.},
author = {Greensmith, Evan and Bartlett, Pl and Baxter, J},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greensmith, Bartlett, Baxter - 2004 - Variance reduction techniques for gradient estimates in reinforcement learning.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {The Journal of Machine Learning {\ldots}},
keywords = {actor-critic,baseline,gpomdp,policy gradient,reinforcement learning},
pages = {1471--1530},
title = {{Variance reduction techniques for gradient estimates in reinforcement learning}},
url = {http://dl.acm.org/citation.cfm?id=1044710},
volume = {5},
year = {2004}
}
@article{Henderson2017,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:pdf},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Henderson2017a,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
eprint = {1709.06560},
title = {{Deep Reinforcement Learning that Matters}},
url = {http://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Vodopivec2017,
author = {Vodopivec, Tom},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vodopivec - 2017 - On Monte Carlo Tree Search and Reinforcement Learning.pdf:pdf},
title = {{On Monte Carlo Tree Search and Reinforcement Learning}},
volume = {60},
year = {2017}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf:pdf},
isbn = {9781424469178},
issn = {1701.07274},
pages = {1--16},
pmid = {25719670},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240},
year = {2017}
}
@article{Lowe2017,
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
archivePrefix = {arXiv},
arxivId = {1706.02275},
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
eprint = {1706.02275},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe et al. - 2017 - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf:pdf},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}},
url = {http://arxiv.org/abs/1706.02275},
year = {2017}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Vodopivec2017a,
author = {Vodopivec, Tom},
title = {{On Monte Carlo Tree Search and Reinforcement Learning}},
volume = {60},
year = {2017}
}
@article{Jaderberg2016a,
abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local infor- mation. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
archivePrefix = {arXiv},
arxivId = {1608.05343},
author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1608.05343},
issn = {1938-7228},
journal = {arXiv},
number = {Nips},
pages = {1608.05343v1 [cs.LG]},
title = {{Decoupled Neural Interfaces using Synthetic Gradients}},
year = {2016}
}
@article{Pathak2017,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Prediction.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
volume = {2017-July},
year = {2017}
}
@article{Czarnecki2017,
abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
archivePrefix = {arXiv},
arxivId = {1703.00522},
author = {Czarnecki, Wojciech Marian and {\'{S}}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
eprint = {1703.00522},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Czarnecki et al. - 2017 - Understanding Synthetic Gradients and Decoupled Neural Interfaces.pdf:pdf},
issn = {1938-7228},
title = {{Understanding Synthetic Gradients and Decoupled Neural Interfaces}},
url = {http://arxiv.org/abs/1703.00522},
year = {2017}
}
@inproceedings{Pathak2017a,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.70},
eprint = {1705.05363},
isbn = {9781538607336},
issn = {21607516},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
volume = {2017-July},
year = {2017}
}
@article{Wender2012,
author = {Wender, Stefan and Watson, Ian},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wender, Watson - 2012 - Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft Broodwar.pdf:pdf},
isbn = {9781467311946},
keywords = {broodwar,qlearning,sarsa,starcraft},
mendeley-tags = {broodwar,qlearning,sarsa,starcraft},
pages = {402--408},
title = {{Applying Reinforcement Learning to Small Scale Combat in the Real-Time Strategy Game StarCraft : Broodwar}},
year = {2012}
}
@article{Griffiths1999,
abstract = {Technology his always played a role in the development of gambling practices and will continue to play a critical role in the development of increased gambling opportunities (e.g., internet gambling). Although technological advance his long been associated with improved gambling opportunities, there is little written in the literature explicitly pointing out this link and its implications for problem gamblers. This paper therefore reviews this situation and examines the technological implications of situational and structural characteristics paying particular attention to slot machine gambling as there has been more empirical work on this type of gambling than any other technological form. The impact of technology on the sociability of gambling is also examined followed by a more speculative evolution of internet gambling as an area of potential concern.},
author = {Griffiths, Mark},
doi = {http://dx.doi.org/10.1023/A:1023053630588},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths - 1999 - Gambling Technologies Prospects for Problem Gambling.pdf:pdf},
isbn = {1050-5350 (Print)},
issn = {1573-3602},
journal = {Journal of gambling studies},
number = {3},
pages = {265--283},
pmid = {12766464},
title = {{Gambling Technologies: Prospects for Problem Gambling.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12766464},
volume = {15},
year = {1999}
}
@article{Commission2016,
author = {Commission, Gambling},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Commission - 2016 - Virtual currencies , eSports and social gaming – discussion paper(2).pdf:pdf},
keywords = {Virtual currencies eSports and social gaming  disc},
number = {August},
pages = {1--12},
title = {{Virtual currencies , eSports and social gaming – discussion paper}},
year = {2016}
}
@article{Griffiths2000,
abstract = {It has been noted that adolescents may be more susceptible to pathological gambling. Not only is it usually illegal, but it appears to be related to high levels of problem gambling and other delinquent activities such as illicit drug taking and alcohol abuse. This paper examines risk factors not only in adolescent gambling but also in videogame playing (which shares many similarities with gambling). There appear to be three main forms of adolescent gambling that have been widely researched. Adolescent gambling activities and general risk factors in adolescent gambling are provided. As well, the influence of technology on adolescents in the form of both videogames and the Internet are examined. It is argued that technologically advanced forms of gambling may be highly appealing to adolescents.},
author = {Griffiths, Mark D and Wood, R T},
doi = {10.1023/A:1009433014881},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths, Wood - 2000 - Risk factors in adolescence the case of gambling, videogame playing, and the internet.pdf:pdf},
isbn = {1050-5350},
issn = {1050-5350},
journal = {Journal of Gambling Studies},
keywords = {addiction,adolescence,adolescent gambling is a,be related to high,but it appears to,gambling,internet,is it usually illegal,levels of prob-,major problem in society,not only,problem adolescent gambling,today,videogames},
number = {2-3},
pages = {199--225},
pmid = {14634313},
title = {{Risk factors in adolescence: the case of gambling, videogame playing, and the internet.}},
volume = {16},
year = {2000}
}
@article{Commission2016a,
author = {Commission, Gambling},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Commission - 2016 - Virtual currencies , eSports and social gaming – discussion paper.pdf:pdf},
keywords = {Virtual currencies eSports and social gaming  disc},
number = {August},
pages = {1--12},
title = {{Virtual currencies , eSports and social gaming – discussion paper}},
year = {2016}
}
@article{IabInternetAdvertisingBureauUK2011,
author = {{(Iab) Internet Advertising Bureau UK}},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/(Iab) Internet Advertising Bureau UK - 2011 - Gaming Britain A Nation United by Digital Play.pdf:pdf},
title = {{Gaming Britain: A Nation United by Digital Play}},
year = {2011}
}
@article{Deasis2017,
abstract = {Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD($\lambda$) elegantly unifies one-step TD prediction with Monte Carlo meth-ods through the use of eligibility traces and the trace-decay parameter $\lambda$. Currently, there are a multitude of algorithms that can be used to per-form TD control, including Sarsa, Q-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be ex-tended across multiple time steps to achieve bet-ter performance. Each of these algorithms is seemingly distinct, and no one dominates the oth-ers for all problems. In this paper, we study a new multi-step action-value algorithm called Q($\sigma$) which unifies and generalizes these exist-ing algorithms, while subsuming them as special cases. A new parameter, $\sigma$, is introduced to al-low the degree of sampling performed by the al-gorithm at each step during its backup to be con-tinuously varied, with Sarsa existing at one ex-treme (full sampling), and Expected Sarsa exist-ing at the other (pure expectation). Q($\sigma$) is gen-erally applicable to both on-and off-policy learn-ing, but in this work we focus on experiments in the on-policy case. Our results show that an in-termediate value of $\sigma$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater per-formance.},
author = {{De Asis}, Kristopher and Ca, Kldeasis@ualberta and Hernandez-Garcia, J Fernando and Ca, Jfhernan@ualberta and Holland, G Zacharias and Sutton, Richard S},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Asis et al. - 2017 - Multi-step Reinforcement Learning A Unifying Algorithm.pdf:pdf},
keywords = {Unifying,multi-step},
mendeley-tags = {Unifying,multi-step},
title = {{Multi-step Reinforcement Learning: A Unifying Algorithm}},
url = {https://arxiv.org/pdf/1703.01327.pdf},
year = {2017}
}
@article{Hewitt2013,
abstract = {Other topics in this issue include coaching children with autism, teaching styles, serve correction and information on an exciting new tennis app. Since the launch of CSSR in English in 1992, the ITF has published over 560 articles from contributors of more than 35 different nationalities. Today the review is produced 3 times per year in the 3 official ITF languages of English, Spanish and French and made available free of charge on the ITF coaching web at http://www.itftennis.com/coaching/sportsscience. The 2012 launch of, 'Biomechanics for Advanced Tennis' as an e-book has proven to be very successful in the new electronic format. Interested readers can purchase their copy at: http://www.amazon.es/ITF-Biomechanics-Advanced-Tennis-ebook/dp/ B00A79U7MK The ITF Tennis iCoach website remains at the forefront of online coach education, with up to date and current research available to coaches across the world. For just {\$}30 per year you can keep up to date with then most current tennis specific coaching information. Please click on the following link for a tour of the site. www.tennisicoach.com In late 2013, the Tennis iCoach will be re-launched with mobile and tablet PC support on both Android and Apple devices. Version 3.0 of the site will also offer HD quality video, a new navigation and search system, as well as a range of new features that will enhance the user engagement and learning experience for users. The launch is set for autumn 2013 and will be officially released at the Worldwide conference in Mexico.},
author = {Hewitt, Mitchell and Aus, Kenneth Edwards and Pluim, Babette and Smit, Claudia and Driessen, Dorian and Oskam, Sandy and Aus, Geoff Quinlan},
file = {:home/sarios/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hewitt et al. - 2013 - CONTENTS ISSUE 59 The Official Coaching and Sport Science Publication of the International Tennis Federation.pdf:pdf},
issn = {2225-4757},
journal = {ITF Coaching and Sport Science Review},
number = {59},
title = {{CONTENTS ISSUE 59 The Official Coaching and Sport Science Publication of the International Tennis Federation}},
url = {www.itftennis.com/coaching/sportsscience},
year = {2013}
}
@article{Klaassen2001,
abstract = {This article tests whether points in tennis are independent and/or identically distributed (iid). We model the probability of winning a point on service and show that points are neither independent nor identically distributed: winning the previous point has a positive ef- fect on winning the current point, and at ‘important' points it is more difficult for the server to win the point than at less important points. Furthermore, the weaker a player, the stronger are these effects. De- viations from iid are, however, small and hence the iid hypothesis will still provide a good approximation in many cases. The results are based on a large panel of matches played atWimbledon 1992—1995, in total almost 90,000 points. Our panel data model takes account of the binary character of the dependent variable, uses random effects to cap- ture theunobservedpartofaplayer's quality, andincludesdynamic explanatory variables.},
author = {Klaassen, Franc J. G. M. and Magnus, Jan R.},
doi = {10.1198/016214501753168217},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {binary choice,dependence,dynamic panel data,linear probability model,nonidentical distribution,random effects},
number = {454},
pages = {500--509},
title = {{Are Points in Tennis Independent and Identically Distributed? Evidence From a Dynamic Binary Panel Data Model}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753168217},
volume = {96},
year = {2001}
}
@article{Knottenbelt2012,
abstract = {Tennis features among the most popular sports internationally, with professional matches played for 11 months of the year around the globe. The rise of the internet has stimulated a dramatic increase in tennis-related financial activity, much of which depends on quantitative models. This paper presents a hierarchical Markov model which yields a pre-play estimate of the probability of each player winning a professional singles tennis match. Crucially, the model provides a fair basis of comparison between players by analysing match statistics for opponents that both players have encountered in the past. Subsequently the model exploits elements of transitivity to compute the probability of each player winning a point on their serve, and hence the match. When evaluated using a data set of historical match statistics and bookmakers odds, the model yields a 3.8{\%} return on investment over 2173 ATP matches played on a variety of surfaces during 2011. ?? 2012 Elsevier Ltd. All rights reserved.},
author = {Knottenbelt, William J. and Spanias, Demetris and Madurska, Agnieszka M.},
doi = {10.1016/j.camwa.2012.03.005},
issn = {08981221},
journal = {Computers and Mathematics with Applications},
keywords = {Sport,Stochastic modelling,Tennis},
number = {12},
pages = {3820--3827},
title = {{A common-opponent stochastic model for predicting the outcome of professional tennis matches}},
volume = {64},
year = {2012}
}
@article{Barnett2005,
abstract = {Tennis features among the most popular sports internationally, with professional matches played for 11 months of the year around the globe. The rise of the internet has stimulated a dramatic increase in tennis-related financial activity, much of which depends on quantitative models. This paper presents a hierarchical Markov model which yields a pre-play estimate of the probability of each player winning a professional singles tennis match. Crucially, the model provides a fair basis of comparison between players by analysing match statistics for opponents that both players have encountered in the past. Subsequently the model exploits elements of transitivity to compute the probability of each player winning a point on their serve, and hence the match. When evaluated using a data set of historical match statistics and bookmakers odds, the model yields a 3.8{\%} return on investment over 2173 ATP matches played on a variety of surfaces during 2011. ?? 2012 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1011.1761},
author = {Barnett, Tristan and Brown, Alan and Clarke, Stephen R. and Caron, Francois and Doucet, Arnaud and Glickman, Mark E Me Mark and Knottenbelt, William J. and Spanias, Demetris and Madurska, Agnieszka M. and Krajowski-kukiel, Maciej and Madurska, Agnieszka M. and McHale, Ian and Morton, Alex and Ralph, B Y and Bradley, Allan and Terry, Milton E and Brent, R P and Dekker, T J and Rios, Luis Miguel and Sahinidis, Nikolaos V. and Herbrich, Ralf and Minka, Tom and Graepel, Thore and Cattelan, Manuela and Varin, Cristiano},
doi = {10.1093/imaman/dpi001},
eprint = {1011.1761},
isbn = {1049-5258},
issn = {08981221},
journal = {Boston University},
keywords = {Australian Open,Betting,Bradley-Terry model,Derivative-free algorithms,Direct search methods,Excel,Gambling,Logit,Ranking evaluation,Scoring systems,Sport,Stochastic modelling,Surrogate models,Tennis,approximate,average process,bayesian estimation,bayesian learning,bradley,cumulative logit model,dynamic difficulty adjustment,exponentially weighted moving,index betting,markov chain,match-making,paired comparisons,sport,sports tournaments,tennis,terry model},
number = {3},
pages = {1--6},
pmid = {18268290},
title = {{Developing a Model that Reflects Outcomes of Tennis Matches}},
url = {http://research.microsoft.com/apps/pubs/default.aspx?id=67956{\%}5Cnhttp://dx.doi.org/10.1016/j.ijforecast.2010.04.004{\%}5Cnhttp://glicko.net/glicko/glicko2.pdf{\%}5Cnhttp://www.echecsonline.net/joueurs/doc/The{\_}Glicko{\_}system.pdf{\%}5Cnhttp://www.glicko.net/research/},
volume = {48},
year = {2005}
}
@article{Pfeiffer2010,
abstract = {The evaluation of the structure of sports performance is one of the important functions of diagnostics in competitive sport. Especially in game sports, it is important to obtain diagnostic information on competition because of the interactive process between the two teams or players. This interaction cannot be simulated or replicated in training or test situations. When it comes to table tennis, performance diagnostics offers many different techniques and methods to analyze a game. In this context, problems mostly occur in ...},
author = {Pfeiffer, Mark and Zhang, Hui and Hohmann, Andreas},
doi = {10.1260/1747-9541.5.2.205},
issn = {1747-9541},
journal = {International Journal of Sports Science and Coaching},
number = {2},
pages = {205--222},
title = {{A Markov chain model of elite table tennis competition}},
volume = {5},
year = {2010}
}
@article{Newtont2006,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Abstract. The probability of winning a game, set, match, or single elimination tournament in tennis is computed using Monte Carlo simulations based on each player's probability of winning a point on serve, which can be held constant or varied from point to point, game to game, or match to match. The theory, described in Newton and Keller [Stud. Appl. Math., 114 (2005), pp. 241-269], is based on the assumption that points in tennis are independent, identically distributed (i.i.d.) random variables. This is used as a baseline to compare with the simulations, which under similar circumstances are shown to converge quickly to the analytical curves in accordance with the weak law of large numbers. The concept of the importance of a point, game, and set to winning a match is described based on conditional probabilities and is used as a starting point to model non-i.i.d. effects, allowing each player to vary, from point to point, his or her probability of winning on serve. Several non-i.i.d. models are investigated, including the "hot-hand-effect," in which we increase each player's probability of winning a point on serve on the next point after a point is won. The "back to-the-wall" effect is modeled by increasing each player's probability of winning a point on serve on the next point after a point is lost. In all cases, we find that the results provided by the theoretical curves based on the i.i.d. assumption are remarkably robust and accurate, even when relatively strong non-i.i.d. effects are introduced. We end by showing examples of tournament predictions from the 2002 men's and women's U.S. Open draws based on the Monte Carlo simulations. We also describe Arrow's impossibility theorem and discuss its relevance with regard to sports ranking systems, and we argue for the development of probability-based ranking systems as a way to soften its consequences.},
author = {Newtont, Paul K and Aslamt, Kamran},
doi = {10.1137/050640278},
isbn = {00361445},
issn = {0036-1445},
journal = {SIAM Review SIAM REVIEW Society for Industrial and Applied Mathematics},
keywords = {60J20,65Q05,91A60,91B12,Arrow's theorem AMS subject classifications 65C05,Monte Carlo Method,non-iid effects,probabilistic ranking systems,tennis},
number = {4},
pages = {722--742},
pmid = {25468915},
title = {{Monte Carlo Tennis*}},
url = {http://www.jstor.org/stable/20453873{\%}5Cnhttp://about.jstor.org/terms},
volume = {48},
year = {2006}
}
@article{Lindley2008,
abstract = {Schema theory provides a foundation for the analysis of game play patterns created by players during their interaction with a game. Schema models derived from the analysis of play provide a rich explanatory framework for the cognitive processes underlying game play, as well as detailed hypotheses for the hierarchical structure of pleasures and rewards motivating players. Game engagement is accounted for as a process of schema selection or development, while immersion is explained in terms of levels of attentional demand in schema execution. However, schemas may not only be used to describe play, but might be used actively as cognitive models within a game engine. Predesigned schema models are knowledge representations constituting anticipated or desired learned cognitive outcomes of play. Automated analysis of player schemas and comparison with predesigned target schemas can provide a foundation for a game engine adapting or tuning game mechanics to achieve specific effects of engagement, immersion, and cognitive skill acquisition by players. Hence, schema models may enhance the play experience as well as provide a foundation for achieving explicitly represented pedagogical or therapeutic functions of games.},
author = {Lindley, Craig a. and Sennersten, Charlotte C.},
doi = {10.1155/2008/216784},
isbn = {86905-901-7},
issn = {1687-7047},
journal = {International Journal of Computer Games Technology},
pages = {1--7},
title = {{Game Play Schemas: From Player Analysis to Adaptive Game Mechanics}},
url = {http://www.hindawi.com/journals/ijcgt/2008/216784/},
volume = {2008},
year = {2008}
}
@incollection{Nelson2007,
abstract = {Game generation systems perform automated, intelligent design of games (i.e. videogames, boardgames), reasoning about both the abstract rule system of the game and the visual realization of these rules. Although, as an instance of the problem of creative design, game generation shares some common research themes with other creative AI systems such as story and art generators, game generation extends such work by having to reason about dynamic, playable artifacts. Like AI work on creativity in other domains, work on game generation sheds light on the human game design process, offering opportunities to make explicit the tacit knowledge involved in game design and test game design theories. Finally, game generation enables new game genres which are radically customized to specific players or situations; notable examples are cell phone games customized for particular users and newsgames providing commentary on current events. We describe an approach to formalizing game mechanics and generating games using those mechanics, using WordNet and ConceptNet to assist in performing common-sense reasoning about game verbs and nouns. Finally, we demonstrate and describe in detail a prototype that designs micro-games in the style of Nintendo's WarioWare series.},
address = {Berlin, Heidelberg},
author = {Nelson, Mark J. and Mateas, Michael},
booktitle = {AI*IA 2007: Artificial Intelligence and Human-Oriented Computing},
doi = {10.1007/978-3-540-74782-6_54},
isbn = {978-3-540-74781-9},
issn = {03029743},
pages = {626--637},
publisher = {Springer Berlin Heidelberg},
title = {{Towards Automated Game Design}},
url = {http://www.springerlink.com/content/64921n6401026242/ http://link.springer.com/10.1007/978-3-540-74782-6{\_}54},
year = {2007}
}
@inproceedings{Zook2014,
abstract = {Game designs often center on the game mechanics—rules governing the logical evolution of the game. We seek to de- velop an intelligent system that generates computer games and assists humans in designing games. As first steps to- wards this goal we present a composable and cross-domain representation for game mechanics that draws from AI plan- ning action representations. We use a constraint solver to generate mechanics subject to design requirements on the form of those mechanics—what they do in the game. A planner takes a set of generated mechanics and tests whether those mechanics meet playability requirements—controlling how mechanics function in a game to affect player behav- ior. We demonstrate our system by modeling and generat- ing mechanics in a role-playing game, platformer game, and combined role-playing-platformer game.},
author = {Zook, Alexander and Riedl, Mo},
booktitle = {Proceedings of the 2014 Foundations of Digital Games Workshop on Procedural Content Generation in Games},
keywords = {game design,game mechanics,procedural content generation},
title = {{Generating and Adapting Game Mechanics}},
url = {http://www.cc.gatech.edu/{~}riedl/pubs/pcg14.pdf},
year = {2014}
}
@article{Sicart2008,
abstract = {This article defines game mechanics in relation to rules and challenges. Game mechanics are methods invoked by agents for interacting with the game world. I apply this definition to a comparative analysis of the games Rez, Every Extend Extra and Shadow of the Colossus that will show the relevance of a formal definition of game mechanics.},
author = {Sicart, Miguel},
doi = {1604-7982},
isbn = {1604-7982},
issn = {16047982},
journal = {Game Studies},
keywords = {Challenges,Game design,Game mechanics,Game research,Rules},
number = {2},
title = {{Defining game mechanics}},
volume = {8},
year = {2008}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Owen1982,
abstract = {This advanced text introduces the principles of noncooperative game theory in a direct and uncomplicated style that will acquaint students with the broad spectrum of the field while highlighting and explaining what they need to know at any given point.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Owen, G and Owen, G},
doi = {10.4135/9781412984317},
eprint = {arXiv:1011.1669v3},
isbn = {9780803920507},
issn = {0-262-06141-4},
journal = {Collection},
keywords = {DSA 2009 BS-2 E-Democracy},
pmid = {3364812506127435976},
title = {{Game Theory}},
year = {1982}
}
