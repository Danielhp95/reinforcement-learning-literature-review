\documentclass{../main.tex}{subfiles}
\begin{document}

Take equation~\ref{equation:utility} from Section~\ref{section:policy-gradient}, representing the utility of a policy $\pi_{\theta}$ parameterized by a D-dimensional real valued parameter vector $\theta \in \mathbb{R}^D$
\begin{equation}
U(\theta) = \sum_{\tau}P(\tau ; \theta) R(\tau)
\end{equation}

The goal is to find the expression $\nabla_{\theta} U(\theta)$ that will allow us to update our policy parameter vector $\theta$ in a direction that improves the estimated value of the utility of the policy $\pi_{\theta}$. Taking the gradient w.r.t $\theta$ gives:

\begin{equation}\label{equation:expectance-gradient}
\begin{aligned}
\nabla_{\theta} U(\theta) & = \nabla_{\theta} \sum_{\tau}P(\tau ; \theta) R(\tau) \\
& =  \sum_{\tau} \nabla_{\theta} P(\tau ; \theta) R(\tau) \quad &\text{(Move gradient operator inside sum)} \\
& =  \sum_{\tau} \nabla_{\theta} \frac{P(\tau; \theta)}{P(\tau ; \theta)} P(\tau ; \theta) R(\tau) \quad & (\text{Multiply by} \frac{P(\tau; \theta)}{P(\tau ; \theta)}  )  \\
& =  \sum_{\tau} P(\tau; \theta) \frac{\nabla_{\theta} P(\tau ; \theta)}{P(\tau ; \theta)} R(\tau) \quad & (\text{Rearrange}) \\
& =  \sum_{\tau} P(\tau; \theta) \nabla_{\theta} \log P(\tau ; \theta) R(\tau) \quad & (\text{Note:} \frac{\nabla_{\theta}P(\tau; \theta)}{P(\tau; \theta)} = \nabla_{\theta} \log P(\tau; \theta) ) \\
\nabla_{\theta} U(\theta) & = \mathbb{E}_{\tau \sim \pi_{\theta}} [\nabla_{\theta} \log P(\tau ; \theta) R(\tau)] & (\mathbb{E}[f(x)] = \sum_{x} xf(x))\\
\end{aligned}
\end{equation}

\begin{equation}
\end{equation}

This leaves us with an expectation for the term $\nabla_{\theta} \log P(\tau ; \theta) R(\tau)$. Note that as of now we have not discussed how to calculate $P(\tau ; \theta)$. Let's define the probability of a trajectory under a policy $\pi_{\theta}$ as: 

\begin{equation}
P (\tau; \theta) = \Pi_{t=0}^{H} \underbrace{P (s_{t+1} | s_t, u_t)}_\textrm{dynamics models} \underbrace{\pi_{\theta} (u_t | s_t)}_\textrm{policy}
\end{equation}

From here we can calculate the term $\nabla_{\theta} \log P(\tau ; \theta)$ present in equation~\ref{equation:expectance-gradient}

\begin{equation}\label{equation:gradient-trajectory}
\begin{aligned}
\nabla_{\theta} \log P(\tau ; \theta) & = \nabla_{\theta} \log [\Pi_{t=0}^{H} P(s_{t+1} | s_t, u_t) \pi_{\theta}(u_t | s_t)] \\
 & = \nabla_{\theta} [(\sum_{t=0}^{H} \log P(s_{t+1} | s_t, u_t)) + (\sum_{t=0}^{H} \log \pi_{\theta}(u_t | s_t))] \\ 
 & = \sum_{t=0}^{H} \underbrace{\nabla_{\theta} \log \pi_{\theta}(u_t | s_t)}_\textrm{no dynamics required!}
\end{aligned}
\end{equation}

Plugging the result of equation~\ref{equation:gradient-trajectory} into equation~\ref{equation:expectance-gradient} we obtain the following equation for the gradient of the utility function w.r.t to parameter vector $\theta$:

\begin{equation}
\begin{aligned}
\nabla_{\theta} U(\theta) & = \mathbb{E}_{\tau \sim \pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(\tau) R(\tau)] \\
\end{aligned}
\end{equation}

We can compute an empirical approximation of that expresion by taking $m$ sample trajectories (or paths) under the policy $\pi_{\theta}$. This works even if the reward function $R$ is unkown and/or discontinuous. This works in discrete state spaces. The likelihood ratio changes the probability of experienced paths. That is, the probability of sampling trajectories. Thus we use a Monte Carlo approach to approximate the gradient of the utility of $\pi_{\theta}$: (REPHRASE)
Which, if we plug into the original equation, we get:
\begin{equation}\label{equation:expectance-gradient-vanilla}
\nabla_{\theta}U(\theta) \approx \hat{g} = \frac{1}{m} \sum_{i = 0}^{m} \nabla_{\theta} \log P(\tau^{(i)} ; \theta) R(\tau^{(i)})
\end{equation}

\begin{equation}
\nabla_{\theta}U(\theta) \approx \hat{g} = \frac{1}{m} \sum_{i = 0}^{m} \sum_{t=0}^{H-1} \nabla_{\theta} \log \pi_{\theta}(u_t^{(i)} | s_t^{(i)}) (\sum_{k=0}^{H}R(s_t^{(i)}, u_t^{(i)}))
\end{equation}

\cite{Sutton1999} offers a different approach to this derivation by calculating the gradient for the state value function on an initial state $s_0$, calculating $\nabla_{\theta} V_{\pi_{\theta}}(s_0)$.

This is an unbiased estimate and it works in theory. However it requries an impractical amount of samples, otherwise the approximation is very noisy. In order to overcome this limitation we can do the following tricks:
\begin{itemize}
\item Add a baseline
\item Add temporal structure
\item Use trust region and natural gradient.
\end{itemize}

\subsection{Add a baseline}
Substract a baseline from the equation~\ref{equation:approximate-gradient-vanilla} to reduce variance without introducing varience \citep{ Williams1992}:

\begin{equation}\label{equation:approximate-gradient-vanilla}
\nabla_{\theta}U(\theta) \approx \hat{g} = \frac{1}{m} \sum_{i = 0}^{m} \sum_{t=0}^{H-1} \nabla_{\theta} \log \pi_{\theta}(u_t^{(i)} | s_t^{(i)}) (\sum_{k=0}^{H}R(s_t^{(i)}, u_t^{(i)}) - b)
\end{equation}

The problem with this equation is that each action $u_i$ is being scaled by the whole sum of rewards $R(\tau)$. This means that, for instance, the last action in a trajectory is taking into account rewards that happened much earlier in the episode. A way to compensate for this is to scale the value of an action $u_i$ (clarify what is meant by this) only by rewards that depend on $u_i$. So the equation becomes:

\begin{equation}
\nabla_{\theta}U(\theta) \approx \hat{g} = \frac{1}{m} \sum_{i = 0}^{m} \sum_{t=0}^{H-1} \nabla_{\theta} \log \pi_{\theta}(u_t^{(i)} | s_t^{(i)}) (\sum_{k=t}^{H}R(s_t^{(i)}, u_t^{(i)}))
\end{equation}

There are other proposed variance reduction techniques via tweaking the baseline which some researchers have studied~\citep{Greensmith2004}

\end{document}
