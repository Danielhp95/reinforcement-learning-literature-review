\documentclass{../main.tex}{}
\begin{document}

Take equation~\ref{equation:utility} from Section~\ref{section:policy-gradient-methods}, representing the utility of a policy $\pi_{\theta}$ parameterized by a D-dimensional real valued parameter vector $\theta \in \mathbb{R}^D$:
\begin{equation}
U(\theta) = \mathbb{E}_{s_t \sim \xi, a_t \sim \pi_{\theta}}[\sum_{t=0}^{H}r(s_t, a_t) | \pi_{\theta}] = \sum_{\tau}P(\tau ; \theta)\mathcal{R}(\tau)
\end{equation}

The goal is to find the expression $\nabla_{\theta} U(\theta)$ that will allow us to update our policy parameter vector $\theta$ in a direction that improves the estimated value of the utility of the policy $\pi_{\theta}$. Taking the gradient w.r.t $\theta$ gives:

\begin{equation}\label{equation:expectance-gradient}
\begin{aligned}
\nabla_{\theta} U(\theta) & = \nabla_{\theta} \sum_{\tau}P(\tau ; \theta) R(\tau) \\
& =  \sum_{\tau} \nabla_{\theta} P(\tau ; \theta) R(\tau) \quad &\text{(Move gradient operator inside sum)} \\
& =  \sum_{\tau} \nabla_{\theta} \frac{P(\tau; \theta)}{P(\tau ; \theta)} P(\tau ; \theta) R(\tau) \quad & (\text{Multiply by} \frac{P(\tau; \theta)}{P(\tau ; \theta)}  )  \\
& =  \sum_{\tau} P(\tau; \theta) \frac{\nabla_{\theta} P(\tau ; \theta)}{P(\tau ; \theta)} R(\tau) \quad & (\text{Rearrange}) \\
& =  \sum_{\tau} P(\tau; \theta) \nabla_{\theta} \log P(\tau ; \theta) R(\tau) \quad & (\text{Note:} \frac{\nabla_{\theta}P(\tau; \theta)}{P(\tau; \theta)} = \nabla_{\theta} \log P(\tau; \theta) ) \\
\nabla_{\theta} U(\theta) & = \mathbb{E}_{\tau \sim \pi_{\theta}} [\nabla_{\theta} \log P(\tau ; \theta) R(\tau)] & (\mathbb{E}[f(x)] = \sum_{x} xf(x))\\
\end{aligned}
\end{equation}

This leaves us with an expectation for the term $\nabla_{\theta} \log P(\tau ; \theta) R(\tau)$. Note that as of now we have not discussed how to calculate $P(\tau ; \theta)$. Let's define the probability of a trajectory under a policy $\pi_{\theta}$ as: 

\begin{equation}
P (\tau; \theta) = \Pi_{t=0}^{H} \underbrace{P (s_{t+1} | s_t, a_t)}_\textrm{dynamics models} \underbrace{\pi_{\theta} (a_t | s_t)}_\textrm{policy}
\end{equation}

From here we can calculate the term $\nabla_{\theta} \log P(\tau ; \theta)$ present in equation~\ref{equation:expectance-gradient}:

\begin{equation}\label{equation:gradient-trajectory}
\begin{aligned}
\nabla_{\theta} \log P(\tau ; \theta) & = \nabla_{\theta} \log [\Pi_{t=0}^{H} P(s_{t+1} | s_t, a_t) \pi_{\theta}(a_t | s_t)] \\
 & = \nabla_{\theta} [(\sum_{t=0}^{H} \log P(s_{t+1} | s_t, a_t)) + (\sum_{t=0}^{H} \log \pi_{\theta}(a_t | s_t))] \\ 
 \nabla_{\theta} \log P(\tau ; \theta)& = \sum_{t=0}^{H} \underbrace{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t)}_\textrm{no dynamics required!}
\end{aligned}
\end{equation}

Plugging the result of equation~\ref{equation:gradient-trajectory} into equation~\ref{equation:expectance-gradient} we obtain the following equation for the gradient of the utility function w.r.t to parameter vector $\theta$:

\begin{equation}
\begin{aligned}
\nabla_{\theta} U(\theta) & = \mathbb{E}_{\tau \sim \pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(\tau) R(\tau)] \\
\end{aligned}
\end{equation}

\cite{Sutton1999} offers a different approach to this derivation by calculating the gradient for the state value function on an initial state $s_0$, calculating $\nabla_{\theta} V_{\pi_{\theta}}(s_0)$.

\end{document}
