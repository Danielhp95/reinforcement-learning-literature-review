\documentclass{../main.tex}{}
\begin{document}
% Inspiration: https://raw.githubusercontent.com/zkid18/RL-Study-Roadmap/master/header_image.png
% Looks like a decent read: https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287


Every RL algorithm attempts to learn an optimal policy $\pi^*$ for a given environment $\xi$. So far, there is not a single algorithm which is used in every single environment to find an optimal policy. The choice of algorithm depends on many factors, such as the nature of the environment, the availability of the underlaying mechanics of the environment, access to already existing policies and more practical constraints such as the amount of computational power available. More importantly, RL algorithms are not, against common belief, black boxes. They can be modularized and composed together to overcome the weaknesses of some approaches with the strenghts of others. For this, it is important to know what type of algorithms exist. Most RL algorithms can be divided into the following categories, note that not all of them are mutually exclusive:

\subsection{On-policy and off-policy algorithms}

If any RL algorithm can be regarded as a learning function mapping state-action-reward sequences, also known as paths or trajectories, to a policies~\citep{Laurent2011}. Esentially all that RL algorithms do is applying a learning function over paths sampled from the environment and a policy. The key and only difference between on-policy and off-policy algorithms is the following:
\begin{itemize}
    \item On-policy algorithms use the policy that they are learning about to sample actions in the environment. A policy $\pi$ is both being improved overtime\footnote{The notion of improvement overtime is expressed as a monotonic increase in the expected reward of an episode $\mathbb{E}_{a \sim \pi_0}[\sum_{t=0}^{\infty}r_t] < \mathbb{E}_{a \sim \pi_1}[\sum_{t=0}^{\infty}r_t]$} and also used to sample actions $a \sim \pi(s)$ inside of the environment.
    \item Off-policy algorithms use a behavioural policy $\mu$ to sample actions $a \sim \mu(s)$ and paths inside of the environment, and use this information to improve a target policy $\pi$. The learning that takes place in off-policy algorithms can be regarded as learning from somebody else's experience, whilst on policy algorithms focus on learning from an agent's own experience.
\end{itemize}

On-policy algorithms dedicate all computational power on learning and using a single policy $\pi$. These methods can therefore focus on spending the computational resources on applying a learning function for the sole benefit of improving this policy. With off-policy algorithms it is possible to dedicate computational time to modifying both the behavioural policy $\mu$ and the policy $\pi$ being learnt. The motivation behind this being that the paths sampled from the environment using $\pi$ won't necessarily yield the best paths to learn from. However, by spending some of the computational resources to using, or even changing, the behavioural policy $\mu$, it is possible to generate more ``imformative'' trajectories with which we can improve $\pi$ through a learning function.

By freeing computational time from directly improving the target policy $\pi$, it is possible to tackle many other tasks.~\cite{Sutton2010} use sensorimotor interaction with an environment to learn a multitude of pseudoreward that are used in conjunction with the environment's reward signals.~\cite{Jaderberg2016} takes this idea further by using an off-policy algorithms to learn auxiliary extra tasks: immediate reward prediction\footnote{This is different from value function estimation because the value that the off-policy algorithm is trying to predict is expected immediate reward, instead of expected future cummulative reward.} and a separate policy that maximizes the change in the state representation\footnote{Given a matrix of pixels as input, the authors define ``pixel control'' as a separate policy that tries to maximally change the pixels in the following state. The reasoning behind this approach is that big changes in pixel values may correspond to important events inside of the environment.}.

A method to allow algorithms to perform off-policy updates to their policies is to introduce the notion of an \textit{experience replay}~\citep{Lin1993}, which was made famous after the success of~\cite{Mnih2013}. An experience replay is a list of experiences, where each experience is a 5 element tuple $<s_t, a_t, r_t, s_{t+1}, a_{t+1}>$. As an agent acts in an environment the experience replay is filled. At the time of updating the policy an experience (or batch of experiences) is sampled uniformly from the experience replay. This is not the case with algorithms such as Q-learning, where updates to a value function happen always using the latest experiences. Because these sampled experiences may have been generated using a previous policy, experience replay allows for policy updates to happen in an off-policy fashion. 

The idea of the experience replay buffer has been the focus of much recent research~\citep{Schaul2015, Hessel2017}. A basic improvement is to use a \textit{prioritized} experience replay. The difference being that experiences are not sampled uniformly from the replay buffer.

% About DQN: DQN is able to learn value functions using such function approximators in a stable and robust way due to two innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize correlations between samples; 2. the network is trained with a target Q network to give consistent targets during temporal difference backups.

% Notes:
% good post on DDPG: http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html
%(REPHRASE) One challenge when using neural networks for reinforcement learning is that most optimization al- gorithms assume that the samples are independently and identically distributed. Obviously, when the samples are generated from exploring sequentially in an environment this assumption no longer holds. This can be solved by using a mechanism that breaks down the dependency of samples, such as an experience replay.

\textbf{Famous on-policy algorithms:} Sarsa~\citep{Sutton1998}, $Q(\sigma)$~\citep{Deasis2017}, Monte Carlo Tree search (MCTS), REINFORCE~\citep{Williams1992}, Asynchronous Advantage estimation Actor Critic (A3C).

\textbf{Famous off-policy algorithms:} Q-learning, Deep Q-Network (DQN)~\citep{Mnih2013}, Deterministic Policy Gradient (DPG)~\citep{Silver2014}. Deep Deterministic Policy Gradient (DDPG)~\citep{Lillicrap2015}, Importance Weighted Actor-Learner Architecture (IMPALA)~\citep{Espeholt2018}.


% Reinforcement Learning algorithms which are characterized as off-policy generally employ a separate behavior policy that is independent of the policy being improved upon; the behavior policy is used to simulate trajectories. A key benefit of this separation is that the behavior policy can operate by sampling all actions, whereas the estimation policy can be deterministic (e.g., greedy) [1]. Q-learning is an off-policy algorithm, since it updates the Q values without making any assumptions about the actual policy being followed. Rather, the Q-learning algorithm simply states that the Q-value corresponding to state s(t) and action a(t) is updated using the Q-value of the next state s(t+1) and the action a(t+1) that maximizes the Q-value at state s(t+1).
% 
% On-policy algorithms directly use the policy that is being estimated to sample trajectories during training.

\subsection{Value based}
Value based, also known as critic only methods, rely on deriving a policy $\pi$ from a state value function $V(s)$ or a state action value function $Q(s,a)$. These include most if not all of the traditional RL algorithms. There are various methods for extracting a policy from a value function. The simplest form of deriving a policy from a value function is to create a policy that acts greedily w.r.t a value function:

\begin{equation}
    \label{equation:policy-extraction}
\begin{aligned}
    \forall s \in \mathcal{S}: \quad \pi(s) &= \argmax_{a} \sum_{s' \in Succ(s)} P(s' \mid s, a) V(s') \quad & (\text{Deriving policy from } V(s)) \\
    \forall s \in \mathcal{S}: \quad \pi(s) &= \argmax_{a} Q_{\pi}(s,a) \quad & (\text{Deriving policy from } Q(s,a))
\end{aligned}
\end{equation}

Some algorithms such as Q-learning, covered in Section~\ref{section:q-learning}, and SARSA bootstrap a state action value function $Q(s,a)$ towards the value functions of an optimal policy, $Q_{\pi^*}(s,a)$. Temporal Difference (TD) algorithms bootstrap a state value function $V(s)$ towards the value function of an optimal policy, $V_{\pi^*}(s)$. These algorithms have been proved to converge to the optimal value functions, so an optimal policy $\pi^*$ can be derived once the bootstraping process converges.

Other methods, like Value iteration or Policy iteration go through an iterative loop of \textit{policy evaluation} and \textit{policy improvement}. The policy evaluation step computes a value function $V_{\pi}(s)$ or $Q_{\pi}(s,a)$ for a given a policy $\pi$, which is randomized on initialization. The policy improvement step extracts a new policy $\pi'$ from the pre-computed value functions $V_{\pi}(s)$ or $Q_{\pi}(s,a)$ using equation~\ref{equation:policy-extraction}. The next iteration of the loop is computed using $\pi'$. This two step process is proved to converge to both the optimal value function and optimal policy.

\textbf{Famous value based algorithms:} Value iteration, Policy iteration, SARSA, TD(0), TD(1), TD($\lambda$)~\citep{Sutton1998}, Q-learning, DQN and it's many variants: Dueling DQN, Distributional DQN, Prioritized DQN and Double DQN\@. These can be found in~\citep{Hessel2017}

\subsection{Policy based}
Both Policy based and actor-critic methods will be covered in depth in section~\ref{section:policy-gradient-methods}. These algorithms do not extract a policy from a calculated value function. Instead, they represent a policy $\pi_{\theta}$ through a parameter vector $\theta \in \mathbb{R}^D$. By defining both the utility of a policy's parameters $U(\theta)$ and its gradient w.r.t the policy's parameters $\nabla_{\theta}U(\theta)$, it is possible to iteratively update the policy parameters in a direction of utility improvement. 

\textbf{Famous policy based algorithms:} vanilla policy gradient, REINFORCE\citep{Williams1992} and the REINFORCE family of algorithms\@.

\subsection{Actor-critic}

Policy based only (actor only) and value based only (critic only) algorithms feature some crippling weaknesses that make them unsuitable for complex environments.~\citep{Konda2000} states some of their key downsides:
\begin{itemize}
\item \textbf{Critic only algorithms}: The goal of any reinforcement learning problem is to find a policy. Spending all computational resources on calculating a value function (the critic) is an indirect way of approaching the problem. Furthermore, most value based algorithms only converge under strict assumptions.
\item \textbf{Actor only algorithms}: These algorithms rely on estimating the gradient of a policy's performance (the actor) w.r.t the policy's parameters. Gradient estimators tend to have large variance, leading to unstable parameter updates and, potentially, lost of convergence properties. More philosophically, because new gradients are calculated independently from previous gradients, there is no ``learning'' in the sense of understanding and consolidation of experience.
\end{itemize}

Actor-critic methods combine the strong points of both policy based and value based algorithms, and overcome some of their individual weaknesses. The critic has an approximation architecture to compute a value function. This value function is used to update the actor's policy parameters in a direction of improvement. This dynamic leads to faster convergence than actor-only methods because of a significant decrease in variance in the estimation of the gradient of the policy's performance. It also entails better convergence properties than critic-only methods. 

~\cite{Konda2000} make the key observation that in actor-critic algorithms, the actor parameterization and the critic parameterization should \textit{not} be independent. The choice of critic parameters should be directly prescribed by the choice of the actor parameters. That is why all real world applications that implement an actor-critic algorithm use a single parameterized model (e.g.\ a neural network) to represent both the policy (actor) and the value function approximation (critic). This is the most straight forward way of sharing parameters between actor and critic.

\textbf{Famous actor critic algorithms:} A3C~\citep{Mnih2016}, PPO~\citep{Schulman2017}, TRPO~\citep{Schulman2015}, ACKTR~\citep{Wu2017}.

\subsection{Model based and model free approaches}
In RL literature the \textit{model} or the dynamics of an environment is considered to be the transition function $\mathcal{P}$ and reward function $\mathcal{R}$. Model free algorithms aim to approximate an optimal policy $\pi^*$ without explicitly using either $\mathcal{P}$ or $\mathcal{R}$ in their calculations.

Model based algorithms are either given a prior model that they can use for planning \citep{browne2012survey, Soemers2014}, or they learn a representation via their own interaction with the environment \citep{Sutton1991, Guzdial2017, Deisenroth2011}. Note that an advantage of learning a model specifically tailored for an agent, is that you can choose a representation of the environment that is relevant to the agent's decision making process. (Talk about curiosity and modelling only that you care about) \citep{Pathak2017}. Another advantage of having a model is that it allows for forward planning, which is the main method of learning for search-based artificial intelligence.

\end{document}
