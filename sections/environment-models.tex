\documentclass{../main.tex}{subfile}

\begin{document}

Even though Markov Decision Processes are the most famous mathematical structure used to model an environment in reinforcement learning, there are other types of possible models for RL environment which act as extensions to vanilla MDPs. This section concerns itself to defining these extensions, and making links between them. We will use a running example throughout this section to illustrate the differences between the various environment models. This example will be a grid maze featuring a variety of rooms. Featuring a single agent, the MDP's state space $\mathcal{S}$ will be all valid agent positions withing the maze. The action space $\mathcal{A}$ will be a discrete set corresponding to four movement actions: forward, backwards, left and right. \ldots. The reward function will be binary, and return 1 for reaching some goal state $s_g \in \mathcal{S}$ and 0 otherwise. This can be specified as $R_g(s,a,s') = [s' = g]$. (Maybe example won't work). 

\subsection{Matrix Game}
Also known as a normal-form game.
\subsection{Semi Markov Decision Process (SMDP)}

As stated in \citep{Barto2003}, in an MDP, only the sequential nature of the decision process is relevant, not the amount of time that passes between decision points. Semi Markov Decision Processes are a generalization of this idea, where the time elapsed in between decision points or decision stages is taken into account, usually defined as $\tau$. In an SMDP actions have an assigned delay $\tau$, known as \textit{holding time}. The holding time means that if an action with holding time $\tau$ is decided at time $t$, the agent will have to wait for $\tau$ timesteps before the action is executed and the next decision point is reached. At this decision point, the agent recieves state $s`$ and a cumulative reward which includes the individual reward of all elapsed timesteps, $r = \sum_{l=t}^{t+\tau}r_{t+l}$. The time until the next decision point $\tau$ can only depend on the action $a$ and state $t$ and thus $\tau$ is independent of the history of the environment. SMDPs can also be used for real-valued time systems instead of discretely timed environments. This holding time allows for a gap in time between sensor input reaching the agent and the agent's action being executed on the environment.

This type of procees is considered Semi Markovian because as the holding time is elapsing, the agent cannot know how the system is evolving. Thus, in order to determine when the next state (decision point) will be reached, it is necessary to know how much time has elapsed, introducing temporal dependency, breaking the Markov property. To iterate on this point, the probability of reaching state state $s_{t+\tau}$ depends only on $s_t$ and action $a_t$ with associated holding time $\tau$. Once the action $a_t$ has been decided, estimating when the agent will recieve state $s_{t+\tau}$ depends on how much time has elapsed since the action $a_t$ was decided.

A Semi Markov Decision Process is defined by a 5-element tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}(\cdot, \cdot | \cdot, \cdot), \mathcal{R}(\cdot, \cdot, \cdot), \gamma)$:
\begin{itemize}
    \item $\mathcal{S}, \mathcal{A}$ and $\gamma$ express the same concepts as in classical MDPs.
    \item $\mathcal{P}(s', \tau | s, a)$, where $s',s \in \mathcal{S}, \tau \in \mathbb{N}, a \in \mathcal{A}$, is the transition probability function. Which states the probability of transitioning to state $s`$ after $\tau$ timesteps.
    \item $\mathcal{R}(s, a, s', \tau)$, where $s',s \in \mathcal{S}, a \in \mathcal{A}$, is the reward function. It represents the expected reward of deciding on action $a$ on state $s$ with an assigned holding time of $\tau$ timesteps and landing on state $s'$.
\end{itemize}

%TODO Make a graphic comparing an MDP against an SMDP.

A useful properties of SMDPs is that they can be reduced to regular MDPs through the \textit{data-transformation method}\citep{Piunovskiy2012}.

SMDPs have recieved a lot of attention in the field hierarchical learning.
\subsection{Partially Observable Markov Decision Process (POMDP)}
In an MDP, the internal representation of the environment is the same representation that the agent receives at every timestep. POMDPs introduce the idea that what the agent observes at every timestep is only a partial representation of the real environment state. This means that instead of receiving an observation $s_t$, the agent receives an observation $o_t$ such that $o_t \subset s_t$. A Partially Observable Markov Decision process is defined by a 6-element tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}(\cdot | \cdot, \cdot), \mathcal{R}(\cdot, \cdot), \Omega, \mathcal{O}, \gamma)$:

\begin{itemize}
    \item $\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}$ and $\gamma$ express the same concepts as in classical MDPs.
    \item $\Omega$ is the set of all possible agent observations. Notably $\Omega \subset \mathcal{S}$, meaning that some of the state properties may never be available to the agent.
    \item $\mathcal{O}(o_t | s_t, a_t)$, where $o_t \in \mathcal{O}, s_t \in \mathcal{S}, a_t \in \mathcal{A}$, represents the probability of observing observation $o_t$ after executing action $a_t$ in state $s_t$. 
\end{itemize}

        In an POMDP the goal of the environment is not to find the optimal policy $\pi*$ which will maximize the expected cumulative reward. This is because the observations that the agent receives as it acts inside of the environment may not be informative enough to allow the agent to learn an optimal policy. Because of this, the goal of the agent becomes to find an optimal policy that maximizes the cumulative reward \textit{conditioned} on the history of observations that the agent has recieved from the environment. The agent samples actions from its policy, which is no longer conditioned on the state of the environment, as the agent does not have access to it, but rather it is conditioned on the sequence of the observations that the agent has obtained so far $a_t \sim \pi(o_{\leq t})$. This is formalized as: $\pi = \mathbb{E}$

        \begin{equation}
        \pi^{} = \underset{\pi}{\text{max}}\;  \mathbb{E}_{s_0 \sim \rho_0, s \sim \xi, a \sim \pi(\cdot | o_{\leq t})}[\sum_{t=0}^{T} \gamma r_t]
        \end{equation}

A POMDP can be reduced to an MDP if, for all timesteps $t$ the agent's observation $o_t$ and the environment state $s_t$ are equal $o_t = s_t$.

\subsection{Decentralized Markov Decision Process (dec-MDP)}

A major shortcoming of MDPs is that they assume stationary environments, which by definition do not change over time and can only be affected by agent's actions. This property make MDPs unsuitable for modelling multi-agent environments, as agents can be considered part of the environment that change over time as learning progresses.

Dec-MDPs can be easily generalized to dec-POMDP for partially observable environments.
\subsection{Markov Game}

~\cite{Owen1982} first introduced the notion of a Markov Game. Markov Games serve to model multi-agent environments. They came to be as a crossbreed between game theoretic structures such as extended-form games and Markov Decision Processes. 
A Markov Game with $k$ different agents is denoted by a 5-element tuple $(\mathcal{S}, {\mathcal{A}_{1..k}}, \mathcal{P}(\cdot | \cdot, \cdots), {\mathcal{R}_{1..k}(\cdot, \cdots)}, \gamma)$
\begin{itemize}
    \item $\mathcal{S}$ and $\gamma$ express the same concepts as classical MDPs.
    \item ${\mathcal{A}_1 \cdots \mathcal{A}_k}$ is a collection of action sets, on for each agent in the environment, with $\mathcal{A}_i$ corresponding to the action set of the $i$th agent.
    \item $\mathcal{P}(s' | s, a_1, \ldots, a_k)$, where $s \in \mathcal{S}$ and $a_1 \in \mathcal{A}_1, \ldots, a_k \in \mathcal{A}_k$, represents probability transition function. It states the probability of transitioning from state $s$ to state $s'$ after executing the \textit{joint} action ${a_1, \ldots, a_k}$. The joint action represents all of the actions that were executed at timestep $t$.
    \item $\mathcal{R}_i(s, a_1, \ldots, a_k)$, where $s \in \mathcal{S}$ and $a_1 \in \mathcal{A}_1, \ldots, a_k \in \mathcal{A}_k$, represents the reward function associated to the $i$th agent, indicating the reward that the $i$th agent will obtain after the joint action ${a_1, \ldots, a_k}$ is executed in state $s$.
\end{itemize}

Each agent independently tries to maximize its expected discounted cumulative reward, $\mathbb{E}[\sum_{j=0}^{\infty} \gamma^{j} r_{i,t+j}]$, where $r_{i, t+j}$ is the reward obtained by agent $i$ at time $t+j$.

Markov Games have several important properties~\cite{Owen1982, Littman1994}. Like MDP's, Every Markov game features an optimal policy for each agent. Unlike MDPs, these policies may be \textit{stochastic}. The need for stochastic action choice stems from the agent's uncertainty about the opponent's pending moves. On top of this, sthocastic policies make it difficult for opponents to ``second guess'' the agent's action, which makes the policy less exploitable. 

When the number of agents in a Markov Game is exactly 1, the Markov Game can be considered an MDP\@. When $|\mathcal{S}| = 1$, the environment can be considered a normal-form game from game theory literature.
\end{document}
