\documentclass{../main.tex}{subfiles}
\begin{document}

\cite{Bellman1957} introduced the concept of a Markov Decision Process (MDP) as an extension of the famous idea of Markov chains. Markov decision processes are a standard model for sequential decision making and control problems. An MDP is fully defined by the 5-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P(\cdot | \cdot, \cdot)}, \mathcal{R(\cdot, \cdot)}, \gamma)$. Whereby:

\begin{itemize}
\item $\mathcal{S}$ is the set of states $s \in \mathcal{S}$.
\item $\mathcal{A}$ is the set of actions $a \in \mathcal{A}$.
\item $\mathcal{P}(s' | s, a)$ where $s, s' \in \mathcal{S}$, $a \in \mathcal{A}$ is a transition kernel which states the probability of transitioning to state $s'$ from state $s$ after performig action $a$. $\mathcal{P}: \mathcal{S} \times \mathcal{A} \to \mathcal{S}$. If the environment is stochastic, as opposed to deterministic, the function $\mathcal{P}$ maps a state-action pair to a distribution over states in $\mathcal{S}$. 
\item $\mathcal{R}(s, a)$ where $s \in \mathcal{S}$, $a \in \mathcal{A}$; is the reward function, which returns the immediate reward (typically in the range $[-1,1]$) of performing action $a$ in state $s$. $\mathcal{R}: \mathcal{S} \times \mathcal{A} \to \mathbb{\mathcal{R}}$. The reward at time step $t$ can be interchangably written as $r_t$ or $r(s_t, a_t)$.
\item $\gamma \in [0,1]$ is the discount factor, which represent the difference in importance between the current reward and future rewards. If $\gamma = 0$ the agent cares only about the immediate reward, if $\gamma = 1$ all rewards $r_t$ are taken into account, this is only allowed in episodic tasks, as otherwise $t \rightarrow \infty$ \citep{Sutton1999}. $\gamma$ is often used as a variance reduction method, and aids proofs in infinitely running environments.
\end{itemize}

From here we can introduce the notion of an agent. (link to control theory?), An agent is an entity that on every state $s_t \in \mathcal{S}$ it can take an action $a_t \in \mathcal{A}$ in an environment transforming the environment from $s_t$ to $s_{t+1}$. The beheviour of an agent is fully defined by a policy $\pi$. A policy $\pi$ is a mapping from states to actions, $\pi: \mathcal{S} \to \mathcal{A}$. The agent chooses which action $a_t$ to take in every state $s_t$ by querying its policy such that $a_t = \pi(s_t)$. If the policy is stochastic, $\pi$ will map an action to a distribution over action $a_t \sim \pi(s_t)$. The objective for an agent is to find an \textit{optimal} policy, which tries to maximize the cumulative sum of possibly discounted rewards.

There are two functions of special relevance in reinforcement learning, the \textit{state value} function $V^{\pi}(s)$ and the \textit{action value} function $Q^{\pi}(s, a)$:
\begin{itemize}
\item The state value function $V^{\pi}(s)$ under a policy $\pi$, where $s \in \mathcal{S}$, represents the expected sum of rewards obtained by starting in state $s$ and following the policy $\pi$ until termination. Formally defined as $V^{\pi}(s) = \mathbb{E}^{\pi}[\sum^{\infty}_{t=0} r(s_t, a_t) | s_0 = s] $
\item The state-action value function $Q^{\pi}(s, a)$ under a policy $\pi$, where $s \in \mathcal{S}$, $a \in \mathcal{A}$, represents the expected sum of rewards obtained by performing action $a$ in state $s$ and then following policy $\pi$. Formally defined as: $Q^{\pi}(s, a) = \mathbb{E}^{\pi}[r(s_0, a_0) + \sum^{\infty}_{t=1} r(s_t, a_t) | s_0 = s, a_0 = a]$
\end{itemize}

The Bellman equations are the most straight forward, dynamic programming approach at solving MDPs \citep{Bertsekas2007, Bellman1957}.

\subsection{Bellman equations and optimality principle}

Note that in general it is not the case that all actions $a \in \mathcal{A}$ can be taken on every state $s_t \in \mathcal{S}$.

The optimality principle, found in~\cite{Bellman1957}, states the following: An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. The optimality principle, coupled with the proof of the existance of a deterministic optimal policy for any MDP as outlined in \citep{Borkar1988} give rise to the optimal state value function $V^{*}(s) = \operatorname*{argmax}_{\pi} V^{\pi}(s) = V^{\pi^{*}}(s)$ and the optimal action value function $Q^{*}(s,a) = \operatorname*{argmax}_{\pi} Q^{\pi}(s, a) = Q^{\pi^{*}}(s, a)$. The optimal value functions determine the best possible performance in a MDP\@. An MDP is considered \textit{solved} once the optimal value functions are found. 

Most of the field of reinforcement learning research focuses on approximating these two equations \citep{Tamar2017} \citep{Watkins1992} \citep{Mnih2013}. (cite many more)

% Bellman equations
\cite{Bellman1957} outlined two analytical equations for the state value and action value function:
\begin{equation}\label{equation:bellman:state-value-function}
V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s) * (r(s, a) + \sum_{s' \in \mathcal{S}} \mathcal{P}(s' | s, a) * V^{\pi}(s'))
\end{equation}

\begin{equation}\label{equation:bellman:action-value-function}
Q^{\pi}(s, a) = r(s, a) + \sum_{s' \in \mathcal{S}} \mathcal{P}(s' | s, a) * (\sum_{a' \in \mathcal{A}} \pi(a' | s') Q^{\pi}(s', a'))
\end{equation}

% TODO:
% Add value iteration / policy iteration?
% Add discount factor
% Explain notion of environment

Most RL algorithms can be devided into the following categories:
Policy based
Value based
actor critic

A further categorization of algorithms is the notion of \textit{model free} and \textit{model based} algorithms. Consider a \textit{model} of an environment to be the transition function $\mathcal{P}$ and reward function $\mathcal{R}$. Model free algorithms aim to approximate an optimal policy without them. Model based algorithms are either given a prior model that they can use for planning \citep{browne2012survey, Soemers2014}, or they learn a representation via their own interaction with the environment \citep{Sutton1991, Guzdial2017}. Note that an advantage of learning your own model is that you can choose a representation of the environment that is relevant to the agent's actions, which can have the advantage of modelling uninteresting (but perhaps complicated) environment behaviour \citep{Pathak2017}.
\end{document}
