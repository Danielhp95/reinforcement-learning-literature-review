\documentclass{../main.tex}{}

\begin{document}
Classical approaches to training a good performing policy on a environment with multiple agents suffer from the problem of overfitting. This is where learning agents become good at operating with or against themselves, but considerably drop performance when matched against other agents that act differently to those they have previously encountered. This issue is exacerbated when one knows not of any existing good opponent strategies to test against, which is also the main reason why reinforcement learning is the only viable machine learning paradigm for these cases.

Self-play is a training scheme which arises in the context of multi-agent training. Training using self-play means that learning agents learn \textit{purely} by simulating playing with themselves. The leearning algorithm does not have access to a dataset of previously deemed ``good moves'' or existing policies. This constraint only takes effect during training, allowing for prior knowledge to be embedded into the agent before training begins.

The notion of self-play has been present in the reinforcement learning world for over half a century. \citep{Samuel1959} discusses the notion of learning a generalization procedure to evaluate board positions by playing against itself in the game of checkers. This generalization procedure is now referred to as a state value function $V$, and learning meant tuning the coefficients of a linear polynomial function. The TD-Gammon algorithm \citep{TDGammon} used self-play to train an agent using TD($\lambda$)~\citep{Sutton1998} to reach expert level backgammon play. The researchers used a feedforward neural network to represent the state value function.~\citep{Tesauro1992} argues in favour of the relevance of TD-Gammon because of the non-deterministic nature of the game. More recently, AlphaGo~\citep{Silver2017a} used a version of expert iteration~\citep{Anthony2017}, (explain?) which is built on top of self-play.  This approach was capable of beating the world champion of go, and it generalized to the games of shogi and chess \citep{Silver2017b}. %chktex 36 

It is often assumed that a training scheme can be defined as self-play if, and only if, the learning agent plays against the latest version of itself. When the agent's policy is updated, the other agents in the environment mirror this policy update.~\citep{Bansal2017} relaxes this constraint to allow some of the agents to use previous policies discovered during training. By keeping checkpoints of the policy as it changes over time, it is possible to resample these checkpoint policies during training to ensure the robustness\footnote{The definition of robustness in this scenario is specified as: successful against a variety of opponents without hyperparameter tuning.} of the policy being learnt. This is an attempt to solve the problem of catastrophic forgetting\footnote{In a multi-agent reinforcement learning context, catastrophic forgetting refers to the event of a policy dropping in performance against policies for which it used to perform favourably.}. Similar ideas have also been independently discovered in other fields. In psychology~\cite{Treutwein1995} introduces the \textit{adaptive staircase procedure}, where a learning agent is presented with a set of increasingly difficult sets of trials. Upon succeding enough trials inside a difficullty level, the agent is presented with harder trials, and if it fails contunially, it is demoted to an easier set. The link with~\citep{Bansal2017} and self-play is in the cases where the agent is presented with easier trials that don't belong to the agent's current difficulty level,

This procedure was proved to work for the deep reinforcement learning architecture UNREAL \citep{Beattie2016} in virtual visual acuity tests~\citep{Leibo2018}.

Self-play has been empirically verified as a valid training scheme, but the theoretical proofs are lagging behind, as it is the case with many corners of the field of machine learning~\citep{Henderson2017, Lipton2018}. An underlying issue of self-play studies is the fact that multi-agent environments are non-stationary and non-Markovian~\citep{Laurent2011}, resulting in a loss of convergence guarantess for many algorithms. Beyond this, there are many open questions that arise regarding the nature of self-play.~\cite{Tesauro1992} observes that it is not clear a priori that such a learning system would converge to a sensible solution, and no convergence proofs exist to this date. 

(Evolutionary algorithms)
(Self-play as natural curriculum)
(Pioneer othello, SSBM)

\end{document}
