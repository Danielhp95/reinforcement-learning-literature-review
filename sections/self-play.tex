\documentclass{../main.tex}{}

\begin{document}
Classical approaches to training a good performing policy on multi-agent environment suffer from the problem of overfitting. This is occurs when learning agents become good at operating with or against themselves, but considerably drop performance when matched against other agents that act differently to those they have previously encountered. This issue is exacerbated when one knows not of any existing good opponent strategies to test against, which is also the main reason why reinforcement learning is the only viable machine learning paradigm for these cases.

Self-play is a training scheme which arises in the context of multi-agent training. Training using self-play means that learning agents learn \textit{purely} by simulating playing with themselves. The learning algorithm does not have access to any pre-existing dataset during the course of training. Using existing datasets containing expert knowledge for a given environment is a common paradigm in reinforcement learning~\citep{silver2016mastering}. However, a training scheme that relies on these cannot be considered self-play. This does not mean that the learning algorithm needs to start learning \textit{tabula rasa}, as this constraint only takes effect during training, allowing for prior knowledge to be embedded into the agent before training begins.

The notion of self-play has been present in the reinforcement learning world for over half a century. \citep{Samuel1959} discusses the notion of learning a state value function (which they call generalization procedure) to evaluate board positions by playing against itself in the game of checkers. Their training scheme consisted in having a simple tree search algorithm to select moves, this search used the state value function represented as a linear polynomial function. The TD-Gammon algorithm \citep{TDGammon} used self-play to train an agent using TD($\lambda$)~\citep{Sutton1998} to reach expert level backgammon play. The researchers used a feedforward neural network to represent the state value function.~\citep{Tesauro1992} argues in favour of the relevance of TD-Gammon because of the non-deterministic nature of the game. More recently, AlphaGo~\citep{Silver2017a} used a version of expert iteration~\citep{Anthony2017}, which is built on top of self-play. This approach was capable of beating the world champion of go, and it generalized to the games of shogi and chess \citep{Silver2017b}. %chktex 36 

It is often assumed that a training scheme can be defined as self-play if, and only if, the learning agent plays against the latest version of itself. When the agent's policy is updated, the other agents in the environment mirror this policy update.~\citep{Bansal2017} relaxes this constraint. They create a dataset of policies by checkpointing\footnote{If a neural network is used to represent the policy, creating a checkpoint just means storing the weight values of the neural network at a given timestep.} the latest policy every fixed interval during training. They allow some of the agents to sample uniformly from this policy dataset every fixed amount of episodes. Thus, the latest policy is presented with a wider variety of behaviours, increasing its robustness\footnote{The definition of robustness in this scenario is specified as: successful against a variety of opponents without hyperparameter tuning.}. This is an attempt to solve the problem of catastrophic forgetting\footnote{In a multi-agent reinforcement learning context, catastrophic forgetting refers to the event of a policy dropping in performance against policies for which it used to perform favourably.}.

Similar ideas have also been independently discovered in other fields. In psychology~\cite{Treutwein1995} introduces the \textit{adaptive staircase procedure}, where a learning agent is presented with a set of increasingly difficult sets of trials. Upon succeding enough trials inside a difficullty level, the agent is presented with harder trials, and if it fails continually, it is demoted to an easier set. The link with~\citep{Bansal2017} and self-play is that sometimes the agent is presented with trials beloinging to easier levels of difficulty. Such procedure ensures that the agent does not forget how to solve trials outside its current level of difficulty. This procedure was proved to work for the deep reinforcement learning architecture UNREAL \citep{Beattie2016} in virtual visual acuity tests~\citep{Leibo2018}.

Self-play can be viewed as a natural curriculum learning problem, in which lessons are generated as training occurs. A dataset of historical policies which grows over the course of training can be regarded as a curriculum to which lessons are dynamically added. State of the art algorithms sample uniformly from this dataset of historical policies, but it could be interesting to experiment with other opponent sampling distributions. There is no research exploring which opponent sampling distribution works best for different types of environments. It may even be possible to \textit{learn} this opponent sampling distribution during training using meta reinforcement learning.~\cite{Duan2016} use meta reinforcement learning to learn a ``learning algorithm'' to perform well on a set of MDPs which is sampled from a given distribution over MDPs. Let's assume a dataset of policies that has been generated during training. Let's also assume that each of this policies is fixed. In a 2 agent environment, sampling a new opponent using some distribution over the set of avaialable policies is analogous to sampling a new MDP. Therefore it could be possible to translate the ideas of~\citep{Duan2016} into self-play, with the addition that the set of available policies (MDPs) would grow over time.

Self-play has been empirically verified as a valid training scheme, but the theoretical proofs are lagging behind, as it is the case with many corners of the field of machine learning~\citep{Henderson2017, Lipton2018}. An underlying issue of self-play studies is the fact that multi-agent environments are non-stationary and non-Markovian~\citep{Laurent2011}, resulting in a loss of convergence guarantess for many algorithms. Beyond this, there are many open questions that arise regarding the nature of self-play.~\cite{Tesauro1992} observes that it is not clear a priori that such a learning system would converge to a sensible solution, and no convergence proofs exist to this date. 

Although the lack of theoretical guarantess does not mean lack of experimental results.~\citep{VanDerRee2013} experimented with two different training strategies one being learning by classical self-play (only using the latest policy for all agents) and the other training against a fixed opponent. They have empirical evidence that shows that for some algorithms learning by self-play yields a higher quality policy than learning against a fixed opponent, but that this is algorithm dependant. Concretely, TD-learning~\citep{Sutton1998} learnt best from self play, but Q-learning performed better when learning against a fixed opponent. Similarly,~\citep{Firoiu2017} found that Q-learning based algorithms did not perform well when trained against other policies which were themselves being updated simultaneously, but otherwise performed well when training against fixed opponents.

\end{document}
