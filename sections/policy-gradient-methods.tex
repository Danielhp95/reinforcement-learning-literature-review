\documentclass{../main.tex}{subfiles}

\begin{document}

In order to comply with notation used in the field of direct optimization, we shall use $u$ for actions.

Consider a stochastic control policy  $\pi_{\theta}(s)$ parameterized by a parameter vector $\theta$, that is, a distribution over the action set $\mathcal{A}$ given a state $s \in \mathcal{S}$. $\theta$ is a D-dimensional real valued vector,  $\theta \in \mathbb{R}^{D}$, where $D$ is the number of parameters / dimensions and $D << |\mathcal{S}|$. This prameterized policy function will be denoted by $\pi_{\theta}$.\footnote{Some researchers prefer the notation $\pi(\cdot, \theta)$, $\pi(\cdot \mid \theta)$ or $\pi(\cdot; \theta)$. These notations are equivalent.}. 

There are strong motivations for using policy gradient approaches versus the already discussed RL methods:
\begin{enumerate}
\item A more direct way of approaching the problem. Instead of computing the value functions $V$ or $Q$ and from those deriving a policy function, we are calculating the policy function directly.
\item Using stochastic policies smoothes the optimization problem. With a deterministic policy, changing which action to do in a given state can have a dramatic effect on potential future rewards\footnote{An example of this concept are \textit{greedy} or $\epsilon$-\textit{greedy} policies derived thus: $\pi(s) = \argmax_{a \in \mathcal{A}} Q(s,a)$.}. If we assume a stochastic policy, shifting a distribution over actions slightly will only slightly modify the potential future rewards. This is what is commonly reffered as smooth.
\item Often $\pi$ can be simpler than $V$ or $Q$.
\item If we learn $Q$ in a large or continuous actions space, it can be tricky to compute $\underset{\text{u}}{argmax}\; Q(s,u)$.
\end{enumerate}
 
(they are not, though. REINFORCE is, but dqn, ddpg arent!) Policy gradient methods are on-policy. In them, the agent acts with using a policy which is improved gradually over time. This contrasts with off-policy algorithms, such as the Q-learning algorithm introduced in Section~\ref{section:q-learning}, which allows the agent to interact with the environment with a policy while it is simultaneously learning another policy. There is ongoing research looking at off-policy variants of policy gradient methods~\citep{Mnih2013, Mnih2016}. 

Let's assume an stochastic environment $E$ from which to sample states and rewards, and an stochastic policy $\pi_{\theta}$ parameterized by a vector $\theta$ from which to sample actions. The agent acting under policy $\pi_{\theta}$ is to maximize the (possibly discounted)\footnote{\cite{Williams1992, Sutton1999} present proofs of this same derivation using a discount factor, which makes policy gradient methods work for environments with infinite time horizons.} sum of rewards on environment $E$, over a time horizon $H$ (possibly infinitely long). We reach the following optimization problem:
\begin{equation}\label{equation:expected-reward-theta}
\underset{\theta}{\text{max}} = \mathbb{E}_{s_{t} \sim E, u_t \sim \pi_{\theta}}[\sum^{H}_{t=0} r(s_t, u_t) | \pi_{\theta}]
\end{equation}

For an episode of length $H$ let $\tau$ be the trajectory followed by an agent in an episode. This trajectory $\tau$ is a sequence of state-action tuples $\tau = (s_0, a_0, \dots, s_H, a_H)$. We overload the notation of the reward function $\mathcal{R}$ thus: $\mathcal{R}(\tau) = \sum_{t=0}^{H}r(s_t, u_t)$, indicating the total accumulated reward in the trajectory $\tau$. We will also use $r(s_t) \in \mathbb{R}$ to refer to the scalar reward obtained at timestep $t$ in the trajectory. From here, the utility of a policy parameterized by $\theta$ is defined as:

\begin{equation}\label{equation:utility}
U(\theta) = \mathbb{E}_{s_t \sim E, u_t \sim \pi_{\theta}}[\sum_{t=0}^{H}r(s_t, u_t) | \pi_{\theta}] = \sum_{\tau}P(\tau ; \theta)\mathcal{R}(\tau)
\end{equation}

Where $P(\tau ; \theta)$ denotes the probability of trajectory $\tau$ happening when taking actions sampled from a parameterized policy $\pi_{\theta}$. More informally, how likely is this sequence of state-action pairs to happen as a result of an agent following a policy $\pi_{\theta}$. Linking equations~\ref{equation:expected-reward-theta} and~\ref{equation:utility}, our optimization problem becomes:

\begin{equation}\label{equation:utility-optimization}
\underset{\theta}{\text{max}}\; U(\theta) = \underset{\theta}{\text{max}}\; \sum_{\tau}P(\tau ; \theta)\mathcal{R}(\tau)
\end{equation}

The policy gradient theorem \citep{Williams1992, Sutton1999} introduces the notion of updating the policy parameter vector $\theta$ using well-studied gradient based methods such as gradient descent \citep{Schulman2017,Mnih2013}, natural gradiant \citep{Wu2017} and other approacheswhich are discussed later on. The Appendix section shows the derivation from equation~\ref{equation:utility} to equation~\ref{equation:utility-gradient}

\begin{equation}\label{equation:utility-gradient}
\begin{aligned}
\nabla_{\theta} U(\theta) & = \mathbb{E}_{\tau \sim \pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(\tau) R(\tau)] \\
\end{aligned}
\end{equation}

A key advantage of the policy gradient theorem, as inspected by~\cite{Sutton1999} (and formalized in the Appendix Section), is that equation~\ref{equation:utility-gradient} does not contain any term of the form $\nabla_{\theta}\mathcal{P}(\tau ; \theta)$. This means that we don't need to model the effect of policy changes on the distribution of states. \citep{Sutton1999} goes farther and shows proof of local optima convergence for policy iteration methods with function approximators for both the policy and advantage functions.

% TODO: Talk about baselines instead of using R(\ and notation 
%       Talk about advantage function. Definition
%       

% After all baselines. Talk about DQN (perhaps mention that it is not the first DeepRL attempt, but mention that it's the best one at the time). Follow up with DDPG, saying that DQN cannot easily be applied to continuous action spaces. Mention Batch normalization (Ioffe 2015)
% About DQN: DQN is able to learn value functions using such function approximators in a stable and robust way due to two innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize correlations between samples; 2. the network is trained with a target Q network to give consistent targets during temporal difference backups.

% Notes:
% good post on DDPG: http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html
%(REPHRASE) One challenge when using neural networks for reinforcement learning is that most optimization al- gorithms assume that the samples are independently and identically distributed. Obviously, when the samples are generated from exploring sequentially in an environment this assumption no longer holds. Additionally, to make efficient use of hardware optimizations, it is essential to learn in mini- batches, rather than online
\end{document}
