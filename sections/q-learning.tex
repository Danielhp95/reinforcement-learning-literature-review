\documentclass{../main.tex}{subfiles}
\begin{document}

The Q-learning algorithm was first introduced by~\cite{Watkins1989}, and is arguably one of the most famous and widely implemented methods in the entire field. Given an MDP, Q-learning aims to calculate the corresponding optimal action value function $Q^*$, hence the name. From there, a deterministic greedy policy can be calculated via $\pi(s) = \argmax_{a \in \mathcal{A}_s} Q(s, a)$. Q-learning has been proven to converge to the optimal solution for an MDP under some assumptions. These assumptions being that the MDP is episodic,  meaning that a terminal state is eventually reached, and that each state-action pair is visited an infinite number of times. Another necessary condition for convergence is that the sequence of updates of Q-values has to be monotonically increasing  $Q(s_i, a_i) \leq Q(s_{i+1}, a_{i+1})$.

% TODO: explain q-learningn
% Talk about q learning being off policy?

% Technical note: http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf
Q-learning is not a perfect algorithm,~\cite{Watkins1992} tells us that the algorithm converges to optimality with probability 1 if each state-action pair is represented discretely. If there is a function approximator\footnote{With neural networks being the most famous function approximators in reinforcement learning at the time of writing.} in place,~\cite{Thrun1993} shows that if the approximation error is greater than a threshold which depends on the discount factor $\gamma$ and episode length, then a systematic overestimation effect, which happens mainly due to the joint effort of function approximation methods and the max operator. On top of this,~\cite{Kaisers2010} introduces the concept of \textit{Policy bias}, which states that state-action pairs that are favoured by the policy are chosen more often, biasing the updates. Ideally all state-action pairs are updated on every step. However, because agent's actions modify the environment, this is generally not possible in absence of an environment model. Frequency Adjusted Q-learning (FAQL) proposes scaling the update rule of Q-learning inversely proportional to the likelihood of choosing the action taken at that step \citep{Kaisers2010}.~\cite{Abdallah2016} introduces Repeated Update Q-learning (RUQL), a more promising Q-learning spin off that proposes running the update equation \textit{multiple times}, where the number of times is inversely proportional to the probability of the action selected given the policy being followed.

%References for overestimation of q-learning updates read page 19 onwards: https://project-archive.inf.ed.ac.uk/msc/20162091/msc_proj.pdf (paper Thrun and Schwartz (1993) an analysis was presented that uncovered issues in the way Q-Learning estimates the action-value) Overestimation happens mainly due to the joint effort of function approximation methods and the max operator, which always picks the highest value and makes it succeptible to overestimation. Repeated Update Q-Learning (RUQL) (Abdallah and Kaisers, 2013, 2016) is an algorithm based on Q-Learning and designed with the intention of addressing its overestimation issues. RUQL proposes that an action value must be updated inversely proportional to the probability of the action selected given the policy that is being followed
\end{document}
